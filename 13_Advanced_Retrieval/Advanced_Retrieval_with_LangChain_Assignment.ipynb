{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ðŸ¤ Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ðŸ¤ Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank)).\n",
        "\n",
        "> You do not need to run the following cells if you are running this notebook locally. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgFAXWVW3wm",
        "outputId": "636db35c-f05a-4038-ec7a-02360bef2dae"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain langchain-openai langchain-cohere rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqYM4Eoxcov"
      },
      "source": [
        "We're also going to be leveraging [Qdrant's](https://qdrant.tech/documentation/frameworks/langchain/) (pronounced \"Quadrant\") VectorDB in \"memory\" mode (so we can leverage it locally in our colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0pDRFEWSXvh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using some reviews from the 4 movies in the John Wick franchise today to explore the different retrieval strategies.\n",
        "\n",
        "These were obtained from IMDB, and are available in the [AIM Data Repository](https://github.com/AI-Maker-Space/DataRepository)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKHcZmKzDwT"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub.\n",
        "\n",
        "You could use any review data you wanted in this step - just be careful to make sure your metadata is aligned with your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "0ce6514e-2479-4001-af24-824f987ce599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-25 18:01:44--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19628 (19K) [text/plain]\n",
            "Saving to: â€˜john_wick_1.csvâ€™\n",
            "\n",
            "john_wick_1.csv     100%[===================>]  19.17K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2025-02-25 18:01:44 (6.20 MB/s) - â€˜john_wick_1.csvâ€™ saved [19628/19628]\n",
            "\n",
            "--2025-02-25 18:01:45--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14747 (14K) [text/plain]\n",
            "Saving to: â€˜john_wick_2.csvâ€™\n",
            "\n",
            "john_wick_2.csv     100%[===================>]  14.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-25 18:01:45 (92.2 MB/s) - â€˜john_wick_2.csvâ€™ saved [14747/14747]\n",
            "\n",
            "--2025-02-25 18:01:45--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13888 (14K) [text/plain]\n",
            "Saving to: â€˜john_wick_3.csvâ€™\n",
            "\n",
            "john_wick_3.csv     100%[===================>]  13.56K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2025-02-25 18:01:45 (4.20 MB/s) - â€˜john_wick_3.csvâ€™ saved [13888/13888]\n",
            "\n",
            "--2025-02-25 18:01:45--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15109 (15K) [text/plain]\n",
            "Saving to: â€˜john_wick_4.csvâ€™\n",
            "\n",
            "john_wick_4.csv     100%[===================>]  14.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-25 18:01:46 (99.0 MB/s) - â€˜john_wick_4.csvâ€™ saved [15109/15109]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2025, 2, 23, 7, 3, 38, 535961)}, page_content=\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error creating Qdrant vectorstore: Storage folder /home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/qdrant_file_store is already accessed by another instance of Qdrant client. If you require concurrent access, use Qdrant server instead.\n",
            "Falling back to in-memory storage\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(\"qdrant_file_store\", exist_ok=True)\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Try with explicit path and error handling\n",
        "try:\n",
        "    vectorstore = Qdrant.from_documents(\n",
        "        documents,\n",
        "        embeddings,\n",
        "        path=os.path.abspath(\"qdrant_file_store\"),\n",
        "        collection_name=\"JohnWick\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error creating Qdrant vectorstore: {e}\")\n",
        "    # Fallback to in-memory if file storage fails\n",
        "    print(\"Falling back to in-memory storage\")\n",
        "    vectorstore = Qdrant.from_documents(\n",
        "        documents,\n",
        "        embeddings,\n",
        "        location=\":memory:\",\n",
        "        collection_name=\"JohnWick\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-3.5-turbo` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#chat_model = ChatOpenAI()\n",
        "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, people generally liked John Wick. Reviews highlight its stylish action, Keanu Reeves' performance, and the unique elements of the film. Many reviewers consider it one of the best action films and recommend it, particularly to fans of the genre. While there are some mixed opinions, the overall reception is positive, with several reviewers expressing strong enthusiasm for the film and the franchise.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review:\\n\\n- [A Masterpiece & Brilliant Sequel](https://www.imdb.com/review/rw4854296/?ref_=tt_urv)'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" an ex-hit-man named John Wick, portrayed by Keanu Reeves, comes out of retirement to seek vengeance after gangsters kill his dog and steal his car. The dog was a gift from his deceased wife, adding an emotional layer to his quest for revenge. The story focuses on Wick\\'s relentless pursuit of those responsible, leading him to confront numerous adversaries in a violent and action-packed narrative. As he battles against the criminal underworld, Wick\\'s formidable skills as an assassin are highlighted, and he becomes a target himself, with a bounty placed on his head. The film is known for its stylish and choreographed action sequences, depicting Wick as a legendary figure in the world of assassins.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People have mixed feelings about the John Wick series. Some reviews are very positive, highlighting the action and the unique plot, like the review that calls the first John Wick \"something special\" and praises its world-building and action sequences. However, there are also negative reviews, such as one for John Wick 3 that criticizes it for being \"mindless\" and \"plotless.\" Additionally, a review of John Wick 4 describes it as the weakest installment, emphasizing a lack of meaningful narrative. Overall, while many seem to enjoy the series, there are also significant criticisms from others, indicating a divide in audience reception.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'There are no reviews with a rating of 10 in the provided context.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" the story revolves around a retired hitman named John Wick, played by Keanu Reeves. After the death of his wife, he lives a quiet life until a group of gangsters break into his home, steal his car, and kill his puppy, which was a final gift from his late wife. This act of violence reignites his old life, and John Wick seeks revenge against those responsible. The film is known for its beautifully choreographed action sequences, emotional depth, and exploration of the hitman underworld, making it a highly regarded action film.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse - but the `I don't know` isn't great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked \"John Wick.\" Reviews highlight it as a highly entertaining action film, with praise for its action sequences, Keanu Reeves\\' performance, and its unique style. The movie is described as \"the best action film of the year\" and is recommended to action buffs and general audiences alike. However, reactions to later installments like \"John Wick 3\" seem to vary, with some suggesting that the magic was gone in that sequel. Overall, the original film received positive feedback.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are reviews with a rating of 10. Here are the URLs to those reviews:\\n\\n1. [Review by ymyuseda](https://www.imdb.com/review/rw4854296/?ref_=tt_urv) - Review Date: 15 May 2019\\n'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" the story centers around a former hitman, John Wick (played by Keanu Reeves), who comes out of retirement to seek revenge after gangsters kill his beloved dog and steal his car. The dog was a last gift from his deceased wife, and its loss sends him on a path of vengeance against those responsible. As he embarks on this mission, he faces numerous challenges and encounters various assassins, all of whom are drawn to the substantial bounty placed on his head due to his reputation as a legendary hitman.\\n\\nIn \"John Wick 2,\" after resolving his past issues with the Russian mafia, John Wick returns home, only to be confronted by mobster Santino D\\'Antonio, who demands his assistance in fulfilling a blood oath. When Wick refuses, Santino retaliates by blowing up his house. Wick is then forced to honor the marker by killing Santino\\'s sister, which leads to further complications and a $7 million contract placed on his life, attracting a multitude of killers after him. The story unfolds with intense action and a deepening conflict as Wick seeks retribution against those who betray him.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, people generally liked John Wick. The reviews indicate that the series has been well received, with many praising the action, Keanu Reeves' performance, and the overall entertainment value. While there are some mixed opinions, especially regarding the later films, most viewers appreciate the franchise's unique style and consistency.\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review:\\n\\n- [A Masterpiece & Brilliant Sequel](https://www.imdb.com/review/rw4854296/?ref_=tt_urv)'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" the story follows John Wick, played by Keanu Reeves, a retired hitman who is drawn back into the criminal underworld after a tragic event. The film begins with the recent death of his wife, which leaves him heartbroken. Soon after, a group of Russian mobsters breaks into his home, steals his car, and tragically kills his dog, which was a final gift from his wife. This act of violence ignites Wick\\'s thirst for vengeance, leading him to unleash a brutal rampage against those responsible.\\n\\nAs he seeks retribution, the film showcases his extraordinary skills as an assassin, drawing other criminals into the conflict. The action is characterized by stylish choreography and intense combat sequences, making it a notable entry in the action genre. Throughout the film, Wick\\'s motivations are rooted in loss and revenge, which drive him to confront the very organizations he once distanced himself from. Ultimately, \"John Wick\" is a tale of relentless vengeance in a world filled with consequences and a dark, intricate criminal underworld.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1755566/3298825889.py:10: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
            "  parent_document_vectorstore = Qdrant(\n"
          ]
        }
      ],
      "source": [
        "#client = QdrantClient(location=\":memory:\")\n",
        "os.makedirs(\"qdrant_parent_store\", exist_ok=True)\n",
        "client = QdrantClient(path=os.path.abspath(\"qdrant_parent_store\"))\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the context provided, people have mixed opinions about the \"John Wick\" series. Some reviews express strong enjoyment and appreciation for the films, highlighting their action and emotional depth, while others criticize specific aspects like the plot and fight scenes. For example, one reviewer described \"John Wick 4\" as \"horrible\" and criticized the plot and fight choreography, while another reviewer praised the series as consistently well-received and particularly enjoyed the fourth installment. Overall, it seems that while many people like the series, there are significant criticisms from certain viewers as well.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review:\\n\\n- [A Masterpiece & Brilliant Sequel](https://www.imdb.com/review/rw4854296/?ref_=tt_urv)'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" a retired assassin named John Wick, played by Keanu Reeves, seeks vengeance after gangsters kill his dog and steal his car. The story revolves around his relentless quest for retribution, forcing him to confront various enemies and assassins due to the immense price placed on his head. The film is characterized by its ultra-violent action sequences and intense shootouts.\\n\\nIn the sequel, \"John Wick: Chapter 2,\" the narrative picks up shortly after the first film. John is drawn back into the world of assassins when he must pay off an old debt. He embarks on a violent journey across various locations, including Italy and Manhattan, leading to him killing numerous assassins as he helps an associate take over the Assassin\\'s Guild. The movie features high-octane chases and fights right from the beginning, maintaining the intense action from the first installment.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, people generally liked John Wick. Many reviews highlight its stylish action sequences, Keanu Reeves' performance, and the film's entertaining and engaging nature. Several reviewers described it as one of the best action films of the year or even the best in a decade, praising its unique and fun take on the genre. While there are some negative opinions, the overall reception appears to be very positive.\""
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review:\\n\\n- [A Masterpiece & Brilliant Sequel](https://www.imdb.com/review/rw4854296/?ref_=tt_urv) \\n\\nThis review is for \"John Wick 3.\"'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" an ex-hit-man named John Wick, played by Keanu Reeves, comes out of retirement after the tragic death of his beloved wife. The film\\'s inciting incident occurs when a group of gangsters, led by an arrogant Russian mob prince, steals his car and kills his dog, which was a final gift from his deceased wife. This act of violence prompts Wick to seek vengeance against those who have wronged him.\\n\\nAs he embarks on his quest for revenge, Wick unleashes a wave of destruction upon the Russian mobsters who crossed him. The film is filled with intense action sequences and showcases Wick\\'s formidable skills as an assassin. It establishes him as a legendary figure in the criminal underworld, known as \"the Boogeyman.\" The story focuses on themes of loss, vengeance, and the consequences of one\\'s actions within a violent and stylized world of assassins. Ultimately, Wick\\'s journey is propelled by his need for retribution and the brutal path he takes to reclaim his peace.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!\n",
        "\n",
        "> NOTE: You do not need to run this cell if you're running this locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dHeB-yGXneL",
        "outputId": "efc59105-518a-4134-9228-d98b8a97e08e"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"qdrant_semantic_store\", exist_ok=True)\n",
        "\n",
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    path=os.path.abspath(\"qdrant_semantic_store\"),  # Use 'path' instead of 'location'\n",
        "    collection_name=\"JohnWickSemantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, people generally liked John Wick. The reviews highlight positive aspects such as exciting action sequences, Keanu Reeves' performance, and the unique style of the films. Many reviewers mention that the franchise has set new standards for action movies and express enthusiasm for the series overall. However, there are some mixed opinions, particularly about the later installments, indicating that while the series has a strong fan base, not everyone feels the same way about every film.\""
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review:\\n\\n- [A Masterpiece & Brilliant Sequel](https://www.imdb.com/review/rw4854296/?ref_=tt_urv)'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" the main character, played by Keanu Reeves, is a retired assassin who seeks vengeance after a group of thugs breaks into his home, beats him, steals his car, and kills his beloved dog. This dog was a final gift from his deceased wife, symbolizing his last connection to her. John Wick\\'s quest for revenge leads to a violent confrontation with the Russian mob responsible for these actions, as he unleashes his formidable skills to take on anyone who stands in his way. The film is noted for its stylish action sequences and the intriguing dynamics within a criminal underworld.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Graph Based Synthetic Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"RAGAS_APP_TOKEN\"] = getpass(\"Please enter your Ragas API key!\")### YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#uv add ragas\n",
        "# uv add nltk\n",
        "# uv add rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /home/pkang/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /home/pkang/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a232a8a81bb24229bb21eeb81526403f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/44 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "956f00a70d524272985c652441eafc16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node c280cf7d-ddb2-4d42-97d1-88da1abfdfb2 does not have a summary. Skipping filtering.\n",
            "Node 97fb62d9-ba50-4a49-aa95-22c7c5a67b57 does not have a summary. Skipping filtering.\n",
            "Node cfdee6b2-f8d1-4368-aaac-36dc05869363 does not have a summary. Skipping filtering.\n",
            "Node 0efc0cd6-57ea-4335-b2a0-55df5dbd939e does not have a summary. Skipping filtering.\n",
            "Node 086e9871-8250-47c0-bac2-a5e7339aff1c does not have a summary. Skipping filtering.\n",
            "Node 1e92b9ed-1064-4885-836e-a11c25191254 does not have a summary. Skipping filtering.\n",
            "Node 347f7685-4223-41cd-b5e6-fefe5ac830d3 does not have a summary. Skipping filtering.\n",
            "Node 0b7ed3c2-f2cc-4c1d-ba96-595820f4ce0d does not have a summary. Skipping filtering.\n",
            "Node 13b2563d-aadd-4333-802d-68d5ee6470fd does not have a summary. Skipping filtering.\n",
            "Node 9761b031-0198-4c4e-9aed-c02ed3432fc9 does not have a summary. Skipping filtering.\n",
            "Node 00d0039c-7f6c-45bc-83c3-2b6f929f09ed does not have a summary. Skipping filtering.\n",
            "Node 1b7f2973-3733-4cf6-9dd5-d52e07b68ec6 does not have a summary. Skipping filtering.\n",
            "Node bcc1c841-03ea-4b4e-8fa9-d4cab22116cf does not have a summary. Skipping filtering.\n",
            "Node 85c117e3-ecb0-45c2-8354-13df6f09f3b2 does not have a summary. Skipping filtering.\n",
            "Node 7bba778d-b258-4611-9325-20e2b9ebf148 does not have a summary. Skipping filtering.\n",
            "Node 34bb62ea-9171-4b89-b197-7ab08d3c1ad1 does not have a summary. Skipping filtering.\n",
            "Node 1edb440a-bff2-4361-9483-645ca5f196de does not have a summary. Skipping filtering.\n",
            "Node 28d159a2-6a1a-444d-aaa4-fb897f7156b6 does not have a summary. Skipping filtering.\n",
            "Node a5d712d1-958c-4992-8ade-5ba0997b7df9 does not have a summary. Skipping filtering.\n",
            "Node 52c61998-ac87-4260-ae9d-e50e05848137 does not have a summary. Skipping filtering.\n",
            "Node 99770409-32ca-4c3b-bcc0-bf3ed4909c9f does not have a summary. Skipping filtering.\n",
            "Node 07c35413-d45b-4f5d-8ecc-eb70ce0b9649 does not have a summary. Skipping filtering.\n",
            "Node a69802e3-48b7-41c8-b696-14b8467887e2 does not have a summary. Skipping filtering.\n",
            "Node 4e0c1e34-bc5e-4830-a3b5-bf46246a4db5 does not have a summary. Skipping filtering.\n",
            "Node 69c83ec5-f1d4-47ec-83e0-40fd583a8047 does not have a summary. Skipping filtering.\n",
            "Node 7cd2c2d1-d0da-44b6-9e99-b451686bd128 does not have a summary. Skipping filtering.\n",
            "Node 8c4110a1-b60d-4318-b53d-78e75efdf290 does not have a summary. Skipping filtering.\n",
            "Node 7405e4d4-796c-4a33-9289-ded5b2652729 does not have a summary. Skipping filtering.\n",
            "Node 44f0f2bb-bc85-4feb-bd1e-b59473a77337 does not have a summary. Skipping filtering.\n",
            "Node 939d6651-6fcc-4d18-a0d5-0dc8ec205b3b does not have a summary. Skipping filtering.\n",
            "Node bbdc1562-be0a-4e03-9896-a8dabac7d710 does not have a summary. Skipping filtering.\n",
            "Node 9f47cd3f-a5d6-47cb-9b04-8db776535abc does not have a summary. Skipping filtering.\n",
            "Node c0231758-c012-49f1-964f-54912c04a827 does not have a summary. Skipping filtering.\n",
            "Node 6b4dd404-1bee-470c-b778-9b3fdada19bd does not have a summary. Skipping filtering.\n",
            "Node f9a2df3b-ae96-4ff3-b832-98b53d94ae2d does not have a summary. Skipping filtering.\n",
            "Node adb0966b-45d4-497d-809d-f2338cbecd4e does not have a summary. Skipping filtering.\n",
            "Node 7381bcea-2530-40c9-a8bc-794aea294d6b does not have a summary. Skipping filtering.\n",
            "Node de601a0c-e7d0-4178-9e86-22dea1c2987e does not have a summary. Skipping filtering.\n",
            "Node 0aa47c26-c68e-405a-9dd9-631c5d4ced79 does not have a summary. Skipping filtering.\n",
            "Node 16fcf8af-c391-4033-8cfd-5d3948804197 does not have a summary. Skipping filtering.\n",
            "Node e2689c8f-8021-41c7-bccf-d26066480d70 does not have a summary. Skipping filtering.\n",
            "Node ea11e05f-4b54-4718-b477-9be7d6c6a430 does not have a summary. Skipping filtering.\n",
            "Node 7e607291-8598-4e56-bde2-c93c41b8db4f does not have a summary. Skipping filtering.\n",
            "Node e9b3d41b-ef6d-47b5-9c91-8aaff6b7be80 does not have a summary. Skipping filtering.\n",
            "Node e68924e4-3b9e-47db-8a71-a52cb2241a98 does not have a summary. Skipping filtering.\n",
            "Node 3ac47a09-6879-49f3-aa7e-19c0ca662f32 does not have a summary. Skipping filtering.\n",
            "Node fb2b81b0-b592-4550-9cb8-c83593b526e1 does not have a summary. Skipping filtering.\n",
            "Node abcd7c5b-6597-4dca-ac93-7b74d0825276 does not have a summary. Skipping filtering.\n",
            "Node 136c63e0-2d5e-4704-b6b8-e23056a90912 does not have a summary. Skipping filtering.\n",
            "Node a786824e-43b7-45da-aff1-d103b61ae64c does not have a summary. Skipping filtering.\n",
            "Node c0bd216e-0b29-496a-8fd2-c46c0944bd73 does not have a summary. Skipping filtering.\n",
            "Node 80d8533a-7bc4-4505-8ea5-be7ca1af9aa4 does not have a summary. Skipping filtering.\n",
            "Node 4f0c6aea-7236-4a0b-a1b9-ad037b2ccaff does not have a summary. Skipping filtering.\n",
            "Node 28ba7769-7c49-4622-b3b8-80fe9c883312 does not have a summary. Skipping filtering.\n",
            "Node 4c9724b5-f3a5-48ab-90c8-ba4220dbbbb3 does not have a summary. Skipping filtering.\n",
            "Node fe984bed-37e8-4750-84ff-1beefc739409 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e6a4ccf6a4d407cb3f7f216597dad5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/244 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c651ecd68944d9282327c972a69f969",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1158114a86b4a9ab9f8fa59d7279ac1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29dbb8d12791413abd416b86f64cc238",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b86c381f8f544c8a06f36489ca71a03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(documents, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the basic plot of John Wick?</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>The basic plot of John Wick involves Keanu Ree...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is John Wick popular among audiences?</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who is Chad Stahelski and what makes his direc...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Chad Stahelski is a director known for his exp...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What them Russian mobsters do in John Wick?</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In John Wick, a group of Russian mobsters vict...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What motivates John Wick to come out of retire...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In the original John Wick (2014), the ex-hit-m...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What John Wick: Chapter 3 - Parabellum about a...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum is about the...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does Parabellum expand the world of John W...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 0\\nReview: It is 5 years since t...</td>\n",
              "      <td>Parabellum expands the world of John Wick by i...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How did the film 'Parabellum' contribute to th...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 4\\nReview: I went to the cinema ...</td>\n",
              "      <td>The film 'Parabellum' contributed to the perce...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Is 'John Wick: Chapter 4' considered the best ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: In a world where mov...</td>\n",
              "      <td>The reviews for 'John Wick: Chapter 4' are mix...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does Ian McShane's character influence Joh...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>Ian McShane plays Winston, the owner of the Co...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0               What is the basic plot of John Wick?   \n",
              "1          Why is John Wick popular among audiences?   \n",
              "2  Who is Chad Stahelski and what makes his direc...   \n",
              "3        What them Russian mobsters do in John Wick?   \n",
              "4  What motivates John Wick to come out of retire...   \n",
              "5  What John Wick: Chapter 3 - Parabellum about a...   \n",
              "6  How does Parabellum expand the world of John W...   \n",
              "7  How did the film 'Parabellum' contribute to th...   \n",
              "8  Is 'John Wick: Chapter 4' considered the best ...   \n",
              "9  How does Ian McShane's character influence Joh...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "6  [<1-hop>\\n\\n: 0\\nReview: It is 5 years since t...   \n",
              "7  [<1-hop>\\n\\n: 4\\nReview: I went to the cinema ...   \n",
              "8  [<1-hop>\\n\\n: 20\\nReview: In a world where mov...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  The basic plot of John Wick involves Keanu Ree...   \n",
              "1  The fourth installment of John Wick is scoring...   \n",
              "2  Chad Stahelski is a director known for his exp...   \n",
              "3  In John Wick, a group of Russian mobsters vict...   \n",
              "4  In the original John Wick (2014), the ex-hit-m...   \n",
              "5  John Wick: Chapter 3 - Parabellum is about the...   \n",
              "6  Parabellum expands the world of John Wick by i...   \n",
              "7  The film 'Parabellum' contributed to the perce...   \n",
              "8  The reviews for 'John Wick: Chapter 4' are mix...   \n",
              "9  Ian McShane plays Winston, the owner of the Co...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangChain RAG  Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Naive Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = naive_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69f6f574da5c4b858c98d49f629d6cc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:ragas.executor:Exception raised in Job[2]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[22]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[26]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[34]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[38]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.8300, 'context_entity_recall': 0.7217, 'noise_sensitivity_relevant': 0.3825, 'context_precision': 0.7558}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### bm25 retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = bm25_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "303c4415488d4dd8a1a2c7613d623117",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.3000, 'context_entity_recall': 0.4983, 'noise_sensitivity_relevant': 0.1823, 'context_precision': 0.1417}"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### contextual compression Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = contextual_compression_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8118d263c65740d98c9a35a94e66c453",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.8300, 'context_entity_recall': 0.6517, 'noise_sensitivity_relevant': 0.3025, 'context_precision': 0.8083}"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multi-Query Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = multi_query_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5792bba8d3e94ae6a349171dbd7dc3c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:ragas.executor:Exception raised in Job[2]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[18]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[22]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[26]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[30]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[34]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[38]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.8500, 'context_entity_recall': 0.6229, 'noise_sensitivity_relevant': 0.4537, 'context_precision': 0.7140}"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parent Document Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = parent_document_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4160bf24924444cf9f37ca6f8398ed54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.4800, 'context_entity_recall': 0.5150, 'noise_sensitivity_relevant': 0.2849, 'context_precision': 0.7000}"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ensemble Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = ensemble_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af51a30a00e04212ba235ec0b6defe0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:ragas.executor:Exception raised in Job[2]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[5]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[6]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[10]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[14]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[18]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[22]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[26]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[30]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[34]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[38]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.9667, 'context_entity_recall': 0.7333, 'noise_sensitivity_relevant': nan, 'context_precision': 0.6974}"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Semantic Chunking Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = semantic_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8215da0055746fe91c37c635618ce7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[10]: TimeoutError()\n",
            "Exception raised in Job[26]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.8667, 'context_entity_recall': 0.7125, 'noise_sensitivity_relevant': 0.2533, 'context_precision': 0.6427}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RAGAS Evaluation Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation Results Summary\n",
        "| Retrieval Method | Context Recall | Context Entity Recall | Noise Sensitivity | Context Precision |\n",
        "|------------------|----------------|----------------------|-------------------|------------------|\n",
        "| Naive | 0.8300 | 0.7217 | 0.3825 | 0.7558 |\n",
        "| BM25 | 0.3000 | 0.4983 | 0.1823 | 0.1417 |\n",
        "| Contextual Compression | 0.8300 | 0.6517 | 0.3025 | 0.8083 |\n",
        "| Multi-Query | 0.8500 | 0.6229 | 0.4537 | 0.7140 |\n",
        "| Parent Document | 0.4800 | 0.5150 | 0.2849 | 0.7000 |\n",
        "| Ensemble | 0.9667 | 0.7333 | N/A | 0.6974 |\n",
        "| Semantic Chunking| 0.8667 | 0.7125 | 0.2533 | 0.6427 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uv add matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAMWCAYAAAAgRDUeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAA0UtJREFUeJzs3XlYVVX//vH7gAwqApqAaCiOKYpzzjkkimQOZTkmSKmVWBrakz7OmkNOaTml5lBqDpn6pDkQpeVQTjklzqBmipo5Jyjs3x/+ON9ODCLiPonv13Vx5dl77b0+e7M5yt1a61gMwzAEAAAAAAAAmMjB3gUAAAAAAADg8UMoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQBAJlgsFg0dOtTeZTywzz//XGXLlpWTk5M8PT3tXU6m/FvufVxcnCwWi+bNm2fvUtK1ceNGWSwWffnllw+9ry5dusjf3/+h95NV/5bnBvbHswAA/16EUgCATDl+/Lhef/11lShRQq6urnJ3d1fdunU1efJk/fXXX/YuD5lw6NAhdenSRSVLltSsWbM0c+bMdNsOHTpUFovF+uXk5CR/f3+9/fbbunz5cpb637p1q4YOHZrl4x8VKcGQxWLRggUL0mxTt25dWSwWVahQIUt9LFq0SJMmTXqAKv89UsK+8ePHp7l//PjxslgsiouLM7ewLEq5npQvBwcHFShQQCEhIdq2bVu6x8XExMhiscjV1TXDn5GEhAR9/PHHqlevnvLnzy9nZ2cVLlxYLVu21BdffKGkpKQsnz85OVmfffaZatasqQIFCihfvnwqU6aMQkND9dNPP93z2lOuuWvXrmnuHzBggLXNxYsX73m+f3pc3kMA4HGSy94FAAD+/dasWaOXX35ZLi4uCg0NVYUKFZSYmKjNmzfr3Xff1a+//pphwJET/PXXX8qV69H+a3Pjxo1KTk7W5MmTVapUqUwdM336dLm5uenGjRuKjo7Wxx9/rN27d2vz5s333f/WrVs1bNgwdenS5b5GaT2q997V1VWLFi3SK6+8YrM9Li5OW7dulaura5bPvWjRIh04cEC9e/d+wCpzLns/Nx06dNBzzz2npKQkHTlyRNOmTVOjRo20Y8cOBQYGpmq/YMECFSpUSH/++ae+/PLLNIOdCxcuKCQkRLt27VJwcLAGDhyoAgUK6Ny5c/r222/VsWNHHTt2TIMGDcrS+d9++21NnTpVrVq1UqdOnZQrVy4dPnxYa9euVYkSJVSrVq17Xrerq6uWL1+uadOmydnZ2WbfF198IVdXV926deue50nL4/YeAgCPA96dAQAZio2NVfv27VWsWDF999138vX1te6LiIjQsWPHtGbNGjtW+PAkJycrMTFRrq6uDxQg/FucP39eku7rl7mXXnpJBQsWlCS9/vrrat++vZYsWaLt27erRo0aD6NMSTnj3j/33HP63//+p4sXL1rvoXQ3UPLx8VHp0qX1559/2rHCnM3ez03VqlVtAslnnnlGISEhmj59uqZNm2bT1jAMLVq0SB07dlRsbKwWLlyYZmjUuXNn/fLLL1q+fLlefPFFm339+/fXzp07dfjw4VTHZeb88fHxmjZtmrp165bqfzJMmjRJFy5cyNR1N2vWTP/73/+0du1atWrVyrp969atio2NVZs2bbR8+fJMnetB5IT3EAB4HDB9DwCQobFjx+r69ev69NNPbQKpFKVKlVKvXr2sr+/cuaMRI0aoZMmScnFxkb+/v/773/8qISHB5jh/f389//zz2rhxo6pXr67cuXMrMDBQGzdulCR99dVXCgwMlKurq6pVq6ZffvnF5vguXbrIzc1NJ06cUHBwsPLmzavChQtr+PDhMgzDpu348eNVp04dPfHEE8qdO7eqVauW5po7FotFPXv21MKFC1W+fHm5uLho3bp11n1/X5Pk2rVr6t27t/z9/eXi4iJvb281adJEu3fvtjnnsmXLVK1aNeXOnVsFCxbUK6+8ojNnzqR5LWfOnFHr1q3l5uYmLy8v9e3bN92pOP80bdo0a82FCxdWRESEzRQXf39/DRkyRJLk5eWV5TVWnnnmGUl3p3P+3c8//6xmzZrJw8NDefLkUYMGDbRlyxbr/qFDh+rdd9+VJBUvXtw6hSdlStb93HtJOnPmjF599VX5+PjIxcVF5cuX15w5c6z74+PjlStXLg0bNizVNRw+fFgWi0VTpkyRJF26dEl9+/ZVYGCg3Nzc5O7urpCQEO3du/e+78/ftWrVSi4uLlq2bJnN9kWLFqlt27ZydHRM87gFCxZYn5kCBQqoffv2On36tHV/w4YNtWbNGp08edJ6H/+5tlNycrJGjhypJ598Uq6urmrcuLGOHTuWqq/MPJ+StHLlSlWoUEGurq6qUKGCVqxYkWbtixcvVrVq1ZQvXz65u7srMDBQkydPvtetum87d+5UcHCwChYsqNy5c6t48eJ69dVXbdr887lJmZJ67Ngx60gbDw8PhYeH6+bNmzbH/vXXX3r77bdVsGBB5cuXTy1bttSZM2ceaG2i9H52JGnLli2Ki4tT+/bt1b59e/3www/67bffbNps27ZN69evV/fu3VMFUimqV6+uTp06Zen8sbGxMgxDdevWTXW8xWKRt7d3pq6zSJEiql+/vhYtWmSzfeHChQoMDEx3yqo93kNee+01FS5cWC4uLipevLjefPNNJSYmSpJu376tYcOGqXTp0nJ1ddUTTzyhevXqKSoqKlP3AQCQOYyUAgBk6Ouvv1aJEiVUp06dTLXv2rWr5s+fr5deekl9+vTRzz//rNGjRysmJibVL7LHjh1Tx44d9frrr+uVV17R+PHj1aJFC82YMUP//e9/1aNHD0nS6NGj1bZtWx0+fFgODv/3/1OSkpLUrFkz1apVS2PHjtW6des0ZMgQ3blzR8OHD7e2mzx5slq2bKlOnTopMTFRixcv1ssvv6zVq1erefPmNjV99913Wrp0qXr27KmCBQumu5DzG2+8oS+//FI9e/ZUQECA/vjjD23evFkxMTGqWrWqJGnevHkKDw/X008/rdGjRys+Pl6TJ0/Wli1b9Msvv9iMWEpKSlJwcLBq1qyp8ePH69tvv9WECRNUsmRJvfnmmxne86FDh2rYsGEKCgrSm2++qcOHD2v69OnasWOHtmzZIicnJ02aNEmfffaZVqxYYZ2SV7FixXt+P/8p5RfA/Pnz29yzkJAQVatWTUOGDJGDg4Pmzp2rZ599Vj/++KNq1KihF198UUeOHNEXX3yhDz/80DpyyMvL677vfXx8vGrVqmX9JdTLy0tr167Va6+9pqtXr6p3797y8fFRgwYNtHTpUmsYl2LJkiVydHTUyy+/LEk6ceKEVq5cqZdfflnFixdXfHy8PvnkEzVo0EAHDx5U4cKF7/s+SVKePHnUqlUrffHFF9bv4d69e/Xrr79q9uzZ2rdvX6pjRo4cqUGDBqlt27bq2rWrLly4oI8//lj169e3PjMDBgzQlStX9Ntvv+nDDz+UJLm5udmcZ8yYMXJwcFDfvn115coVjR07Vp06ddLPP/9sbZPZ53PDhg1q06aNAgICNHr0aP3xxx8KDw/Xk08+adNnVFSUOnTooMaNG+uDDz6QdHcdoy1bttgE1w/q/Pnzatq0qby8vNSvXz95enoqLi5OX331VaaOb9u2rYoXL67Ro0dr9+7dmj17try9va01S3eD4qVLl6pz586qVauWNm3alOq94n6l9bOTYuHChSpZsqSefvppVahQQXny5NEXX3xhDWGku+/FklJNB82MzJy/WLFiku4GlS+//LLy5Mlz3/2k6Nixo3r16qXr16/Lzc1Nd+7c0bJlyxQZGZnm1D2z30N+//131ahRQ5cvX1b37t1VtmxZnTlzRl9++aVu3rwpZ2dnDR06VKNHj1bXrl1Vo0YNXb16VTt37tTu3bvVpEmTLN8bAMA/GAAApOPKlSuGJKNVq1aZar9nzx5DktG1a1eb7X379jUkGd999511W7FixQxJxtatW63b1q9fb0gycufObZw8edK6/ZNPPjEkGd9//711W1hYmCHJeOutt6zbkpOTjebNmxvOzs7GhQsXrNtv3rxpU09iYqJRoUIF49lnn7XZLslwcHAwfv3111TXJskYMmSI9bWHh4cRERGR7r1ITEw0vL29jQoVKhh//fWXdfvq1asNScbgwYNTXcvw4cNtzlGlShWjWrVq6fZhGIZx/vx5w9nZ2WjatKmRlJRk3T5lyhRDkjFnzhzrtiFDhhiSbO5NelLaHj582Lhw4YIRFxdnzJkzx8idO7fh5eVl3LhxwzCMu/e8dOnSRnBwsJGcnGw9/ubNm0bx4sWNJk2aWLeNGzfOkGTExsam6u9+7v1rr71m+Pr6GhcvXrRp1759e8PDw8P6/U55bvbv32/TLiAgwOZ7f+vWLZt7ZxiGERsba7i4uNh8T2JjYw1Jxty5c9O5a3d9//33hiRj2bJlxurVqw2LxWKcOnXKMAzDePfdd40SJUoYhmEYDRo0MMqXL289Li4uznB0dDRGjhxpc779+/cbuXLlstnevHlzo1ixYun2Xa5cOSMhIcG6ffLkyTb34n6ez8qVKxu+vr7G5cuXrds2bNhgSLKpoVevXoa7u7tx586dDO/PP6Xc13HjxqW5/5/PzYoVKwxJxo4dOzI87z+fm5Rn+tVXX7Vp98ILLxhPPPGE9fWuXbsMSUbv3r1t2nXp0iXVOTO6nmHDhhkXLlwwzp07Z/z444/G008/bX0u/i4xMdF44oknjAEDBli3dezY0ahUqVKqOiXZfB8MwzD++usv48KFC9avP//8M0vnNwzDCA0NNSQZ+fPnN1544QVj/PjxRkxMTIbX+3eSjIiICOPSpUuGs7Oz8fnnnxuGYRhr1qwxLBaLERcXl+p9yB7vIaGhoYaDg0Oaz1BKDZUqVTKaN2+e6WsHAGQN0/cAAOm6evWqJClfvnyZav/NN99IkiIjI2229+nTR5JSrT0VEBCg2rVrW1/XrFlTkvTss8+qaNGiqbafOHEiVZ89e/a0/jll5ExiYqK+/fZb6/bcuXNb//znn3/qypUreuaZZ1JNtZOkBg0aKCAg4B5Xenddpp9//lm///57mvt37typ8+fPq0ePHjbrmTRv3lxly5ZNcx2uN954w+b1M888k+Y1/923336rxMRE9e7d22YUWbdu3eTu7v7A63099dRT8vLykr+/v1599VWVKlVKa9eutY6i2LNnj44ePaqOHTvqjz/+0MWLF3Xx4kXduHFDjRs31g8//KDk5ORM9ZWZe28YhpYvX64WLVrIMAxrfxcvXlRwcLCuXLli/b6++OKLypUrl5YsWWI9/sCBAzp48KDatWtn3ebi4mK9d0lJSfrjjz/k5uamp556Ks1n5H40bdpUBQoU0OLFi2UYhhYvXqwOHTqk2farr75ScnKy2rZta3NdhQoVUunSpfX9999nut/w8HCbRaZTpo6lPE+ZfT7Pnj2rPXv2KCwsTB4eHtZ2TZo0SfW98vT01I0bNx769KaUEVyrV6/W7du37/v4tH7O/vjjD+v7XcqUr5SRmineeuut++pnyJAh8vLyUqFChfTMM88oJiZGEyZM0EsvvWTTbu3atfrjjz9snosOHTpYR9WlSKnvn6PiZsyYIS8vL+tXvXr1snR+SZo7d66mTJmi4sWLa8WKFerbt6/KlSunxo0bpzmtMz358+dXs2bN9MUXX0i6O2W1Tp061tFYf2f2e0hycrJWrlypFi1aqHr16qn2WywWSXefs19//VVHjx7NVN8AgKwhlAIApMvd3V3S3fWTMuPkyZNycHBI9cluhQoVkqenp06ePGmz/e/BkyTrL71+fn5pbv/notAODg4qUaKEzbYyZcpIks3Hx69evVq1atWSq6urChQoIC8vL02fPl1XrlxJdQ3Fixe/12VKurvW1oEDB+Tn56caNWpo6NChNgFSyrU+9dRTqY4tW7Zsqnvh6upqMw1FuvuL3b0Wwk6vH2dnZ5UoUSJVP/dr+fLlioqK0qJFi1SrVi2dP3/eJuRL+YUtLCzM5hdjLy8vzZ49WwkJCWne57Rk5t5fuHBBly9f1syZM1P1Fx4eLun/FnQvWLCgGjdurKVLl1qPX7JkiXLlymWzJk9ycrI+/PBDlS5dWi4uLipYsKC8vLy0b9++TNeeHicnJ7388statGiRfvjhB50+fVodO3ZMs+3Ro0dlGIZKly6d6tpiYmKs15UZ//zZSpkylvI8Zfb5TPlv6dKlU7X757E9evRQmTJlFBISoieffFKvvvqqNeDJDilhQYMGDdSmTRsNGzZMBQsWVKtWrTR37txU69alJzP3xsHBIdXzmNlPrEzRvXt3RUVF6euvv9Y777yjv/76K8014hYsWKDixYvLxcVFx44d07Fjx1SyZEnlyZNHCxcutLZL+Z8D169ftzm+TZs2ioqKUlRUVJpTcjN7funue2pERIR27dqlixcvatWqVQoJCdF3332n9u3b39f1d+zYUVFRUTp16pRWrlyZ4XMvmfsecvXq1XTXtkoxfPhwXb58WWXKlFFgYKDefffdNKfcAgAeDGtKAQDS5e7ursKFC+vAgQP3dVzKL4/3kt5Cz+ltN/6xgHlm/Pjjj2rZsqXq16+vadOmydfXV05OTpo7d26qhXgl21FVGWnbtq2eeeYZrVixQhs2bNC4ceP0wQcf6KuvvlJISMh915neNdtb/fr1rWu3tGjRQoGBgerUqZN27dolBwcH6wiGcePGqXLlymme458jO9KTmXuf0t8rr7yisLCwNNv8/Rfz9u3bKzw8XHv27FHlypW1dOlSNW7c2ObT8EaNGqVBgwbp1Vdf1YgRI1SgQAE5ODiod+/emR6hkZGOHTtqxowZGjp0qCpVqpTuSI7k5GRZLBatXbs2zechs/dRyt6foczy9vbWnj17tH79eq1du1Zr167V3LlzFRoaqvnz56d7XMpIrb/++ivN/SmLkKe0s1gs+vLLL/XTTz/p66+/1vr16/Xqq69qwoQJ+umnn+55n8y6N6VLl1ZQUJAk6fnnn5ejo6P69eunRo0aWUfoXL16VV9//bVu3bqVZvC3aNEijRw5UhaLRWXLlpV0d7Tf3xcj9/Pzswb5+fPn18WLF6377uf8//TEE0+oZcuWatmypRo2bKhNmzbp5MmTaY52SkvLli3l4uKisLAwJSQkqG3btmm2M/s9JLPq16+v48ePa9WqVdqwYYNmz56tDz/8UDNmzEjzkxEBAFlDKAUAyNDzzz+vmTNnatu2bTZT7dJSrFgxJScn6+jRoypXrpx1e3x8vC5fvpzpX2YyKzk5WSdOnLCOjpKkI0eOSJJ1gdvly5fL1dVV69evl4uLi7Xd3LlzH7h/X19f9ejRQz169ND58+dVtWpVjRw5UiEhIdZrPXz4sJ599lmb4w4fPpxt9+Lv/fx91FhiYqJiY2OtvxRnBzc3Nw0ZMkTh4eFaunSp2rdvr5IlS0q6G2Deq6/MhpUZ8fLyUr58+ZSUlJSpa2vdurVef/116xS+I0eOqH///jZtvvzySzVq1EiffvqpzfbLly/bhFdZVa9ePRUtWlQbN260WUz7n0qWLCnDMFS8eHGbZzotD3ovM/t8pvw3rSlMhw8fTrXN2dlZLVq0UIsWLZScnKwePXrok08+0aBBg9IdaeTl5aU8efKkeb6UfvLkyZPqe1GrVi3VqlVLI0eO1KJFi9SpUyctXrz4gQODlPex2NhYmyAnrU8vvB8DBgzQrFmzNHDgQOsIsq+++kq3bt3S9OnTU13f4cOHNXDgQG3ZskX16tXT888/rzFjxmjhwoVpfkJeWu7n/BmpXr26Nm3apLNnz2b6vSt37txq3bq1FixYoJCQkHR/luzxHuLu7p6p/9lSoEABhYeHKzw8XNevX1f9+vU1dOhQQikAyEZM3wMAZOg///mP8ubNq65duyo+Pj7V/uPHj1s/8v25556TJE2aNMmmzcSJEyXpgT+9Ki1Tpkyx/tkwDE2ZMkVOTk5q3LixpLujIiwWi820mbi4OK1cuTLLfSYlJaWaTuLt7a3ChQtbpxBVr15d3t7emjFjhs20orVr1yomJibb7kVQUJCcnZ310Ucf2Yz0+PTTT3XlypVsv+edOnXSk08+aQ1XqlWrppIlS2r8+PGpphVJd6fKpMibN6+ku2FPVjk6OqpNmzZavnx5mr9U/r0/6e66MMHBwVq6dKkWL14sZ2dntW7dOtU5/zlKZtmyZfe1hk5GLBaLPvroIw0ZMkSdO3dOt92LL74oR0dHDRs2LFU9hmHojz/+sL7OmzfvA00tzOzz6evrq8qVK2v+/Pk2/UVFRengwYM25/x7fdLdqWApo9Yymlrn6Oiopk2b6uuvv9apU6ds9p06dUpff/21mjZtah3h9Oeff6a6PykjbDI7hS8jwcHBkqRp06bZbP/4448f6Lyenp56/fXXtX79eu3Zs0fS3al1JUqU0BtvvKGXXnrJ5qtv375yc3OzTrGrW7eumjRpopkzZ2rVqlVp9vHP+3I/5z937lyq76l0N+COjo5Oc2r2vfTt21dDhgzRoEGD0m1j9nuIg4ODWrdura+//lo7d+5MtT/lHv7zeXZzc1OpUqWy5RkDAPwfRkoBADJUsmRJLVq0SO3atVO5cuUUGhqqChUqKDExUVu3btWyZcvUpUsXSVKlSpUUFhammTNn6vLly2rQoIG2b9+u+fPnq3Xr1mrUqFG21ubq6qp169YpLCxMNWvW1Nq1a7VmzRr997//ta7P1Lx5c02cOFHNmjVTx44ddf78eU2dOlWlSpXK8vog165d05NPPqmXXnpJlSpVkpubm7799lvt2LFDEyZMkHR3LaEPPvhA4eHhatCggTp06KD4+HhNnjxZ/v7+euedd7LlHnh5eal///4aNmyYmjVrppYtW+rw4cOaNm2ann766Sx9fHxGnJyc1KtXL7377rtat26dmjVrptmzZyskJETly5dXeHi4ihQpojNnzuj777+Xu7u79aPsq1WrJunuiJH27dvLyclJLVq0sP6imVljxozR999/r5o1a6pbt24KCAjQpUuXtHv3bn377be6dOmSTft27drplVde0bRp0xQcHGxdKDvF888/r+HDhys8PFx16tTR/v37tXDhwlTrlT2IVq1aqVWrVhm2KVmypN5//331799fcXFxat26tfLly6fY2FitWLFC3bt3V9++fSXdvZdLlixRZGSknn76abm5ualFixaZrud+ns/Ro0erefPmqlevnl599VVdunRJH3/8scqXL28TInTt2lWXLl3Ss88+qyeffFInT57Uxx9/rMqVK9uMnEzLqFGjVKtWLVWtWlXdu3eXv7+/4uLiNHPmTFksFo0aNcradv78+Zo2bZpeeOEFlSxZUteuXdOsWbPk7u5uDcYfRLVq1dSmTRtNmjRJf/zxh2rVqqVNmzZZR2E+yGidXr16adKkSRozZowmTpyo77//Xm+//XaabV1cXBQcHKxly5bpo48+kpOTkxYsWKBmzZqpdevWCgkJUVBQkPLnz69z587p22+/1Q8//GCdPvz777/f1/l/++031ahRQ88++6waN26sQoUK6fz58/riiy+0d+9e9e7d+75HDlaqVEmVKlXKsI2Dg4Pp7yGjRo3Shg0b1KBBA3Xv3l3lypXT2bNntWzZMm3evFmenp4KCAhQw4YNVa1aNRUoUEA7d+7Ul19+afPhGgCAbGD+B/4BAB5FR44cMbp162b4+/sbzs7ORr58+Yy6desaH3/8sXHr1i1ru9u3bxvDhg0zihcvbjg5ORl+fn5G//79bdoYhmEUK1YszY/b1v//SPG/S+sj48PCwoy8efMax48fN5o2bWrkyZPH8PHxMYYMGWIkJSXZHP/pp58apUuXNlxcXIyyZcsac+fOtX4s+b36/vu+lI8UT0hIMN59912jUqVKRr58+Yy8efMalSpVMqZNm5bquCVLlhhVqlQxXFxcjAIFChidOnUyfvvtN5s2KdfyT2nVmJ4pU6YYZcuWNZycnAwfHx/jzTffTPXR8P/8KPaMZNT2ypUrhoeHh9GgQQPrtl9++cV48cUXjSeeeMJwcXExihUrZrRt29aIjo62OXbEiBFGkSJFDAcHB5uPds/svU8RHx9vREREGH5+foaTk5NRqFAho3HjxsbMmTNTHX/16lUjd+7chiRjwYIFqfbfunXL6NOnj+Hr62vkzp3bqFu3rrFt2zajQYMGNteY8hzOnTs37Zv2/33//feGJGPZsmUZtmvQoIFRvnz5VNuXL19u1KtXz8ibN6+RN29eo2zZskZERIRx+PBha5vr168bHTt2NDw9PQ1JRrFixTLsO73aM/N8ptRUrlw5w8XFxQgICDC++uorIywszNqvYRjGl19+aTRt2tTw9vY2nJ2djaJFixqvv/66cfbs2QzvQ4qYmBijXbt2hre3t5ErVy7D29vbaN++vRETE2PTbvfu3UaHDh2MokWLGi4uLoa3t7fx/PPPGzt37rRp98/nJr1neu7cuTbPomEYxo0bN4yIiAijQIEChpubm9G6dWvj8OHDhiRjzJgxGV5HWu9Xf9elSxfD0dHRGD9+vCEp1c/I382bN8+QZKxatcq67a+//jImTZpk1K5d23B3dzdy5cplFCpUyHj++eeNhQsXGnfu3DEMwzAmTJhwX+e/evWqMXnyZCM4ONh48sknDScnJyNfvnxG7dq1jVmzZhnJyckZXrdhZPxznCK974PZ7yEnT540QkNDDS8vL8PFxcUoUaKEERERYSQkJBiGYRjvv/++UaNGDcPT09PInTu3UbZsWWPkyJFGYmLiPe8DACDzLIbxEFe8BADgIenSpYu+/PLLNKd7AEB227Nnj6pUqaIFCxaoU6dO9i4HAIAcgTWlAAAAgL9J65MAJ02aJAcHB9WvX98OFQEAkDOxphQAAADwN2PHjtWuXbvUqFEj5cqVS2vXrtXatWvVvXt3+fn52bs8AAByDEIpAAAA4G/q1KmjqKgojRgxQtevX1fRokU1dOhQDRgwwN6lAQCQo7CmFAAAAAAAAEzHmlIAAAAAAAAwHaEUAAAAAAAATPfYrSmVnJys33//Xfny5ZPFYrF3OQAAAAAAADmKYRi6du2aChcuLAeH9MdDPXah1O+//86npgAAAAAAADxkp0+f1pNPPpnu/sculMqXL5+kuzfG3d3dztUAAAAAAADkLFevXpWfn581g0nPYxdKpUzZc3d3J5QCAAAAAAB4SO61bBILnQMAAAAAAMB0hFIAAAAAAAAwnV1DqR9++EEtWrRQ4cKFZbFYtHLlynses3HjRlWtWlUuLi4qVaqU5s2b99DrBAAAAAAAQPay65pSN27cUKVKlfTqq6/qxRdfvGf72NhYNW/eXG+88YYWLlyo6Ohode3aVb6+vgoODjahYgAAAAAApOTkZCUmJtq7DMAunJyc5Ojo+MDnsWsoFRISopCQkEy3nzFjhooXL64JEyZIksqVK6fNmzfrww8/JJQCAAAAAJgiMTFRsbGxSk5OtncpgN14enqqUKFC91zMPCOP1Kfvbdu2TUFBQTbbgoOD1bt373SPSUhIUEJCgvX11atXH1Z5AAAAAIAczjAMnT17Vo6OjvLz85ODA0s14/FiGIZu3ryp8+fPS5J8fX2zfK5HKpQ6d+6cfHx8bLb5+Pjo6tWr+uuvv5Q7d+5Ux4wePVrDhg0zq0QAAAAAQA52584d3bx5U4ULF1aePHnsXQ5gFyn5y/nz5+Xt7Z3lqXw5PtLt37+/rly5Yv06ffq0vUsCAAAAADyikpKSJEnOzs52rgSwr5RQ9vbt21k+xyM1UqpQoUKKj4+32RYfHy93d/c0R0lJkouLi1xcXMwoDwAAAADwmHiQdXSAnCA7fgYeqZFStWvXVnR0tM22qKgo1a5d204VAQAAAAAAICvsGkpdv35de/bs0Z49eyRJsbGx2rNnj06dOiXp7tS70NBQa/s33nhDJ06c0H/+8x8dOnRI06ZN09KlS/XOO+/Yo3wAAAAAAIBU4uLiZLFYrHnHxo0bZbFYdPnyZbvW9W9j1+l7O3fuVKNGjayvIyMjJUlhYWGaN2+ezp49aw2oJKl48eJas2aN3nnnHU2ePFlPPvmkZs+ereDgYNNrBwAAAAAghX+/Nab2Fzem+X0fc+7cOY0cOVJr1qzRmTNn5O3trcqVK6t3795q3LhxttXWsGFDVa5cWZMmTcq2c97PeRs2bKhNmzZJurukT9GiRRUeHq5+/fox7fJfxq6hVMOGDWUYRrr7582bl+Yxv/zyy0OsCgAAAACAnCUuLk5169aVp6enxo0bp8DAQN2+fVvr169XRESEDh06ZO8Ss1W3bt00fPhwJSQk6LvvvlP37t3l6empN998096l4W8eqTWlAAAAAADA/evRo4csFou2b9+uNm3aqEyZMipfvrwiIyP1008/WdudOnVKrVq1kpubm9zd3dW2bVubDxwbOnSoKleurM8//1z+/v7y8PBQ+/btde3aNUlSly5dtGnTJk2ePFkWi0UWi0VxcXGSpAMHDigkJERubm7y8fFR586ddfHiRUl3p7c5Ozvrxx9/tPY1duxYeXt7Kz4+PsPzpiVPnjwqVKiQihUrpvDwcFWsWFFRUVHW/QkJCerbt6+KFCmivHnzqmbNmtq4caPNObZs2aKGDRsqT548yp8/v4KDg/Xnn39KktatW6d69erJ09NTTzzxhJ5//nkdP348S9+bxxmhFAAAAAAAOdilS5e0bt06RUREKG/evKn2e3p6SpKSk5PVqlUrXbp0SZs2bVJUVJROnDihdu3a2bQ/fvy4Vq5cqdWrV2v16tXatGmTxowZI0maPHmyateurW7duuns2bM6e/as/Pz8dPnyZT377LOqUqWKdu7cqXXr1ik+Pl5t27aVdHdWVO/evdW5c2dduXJFv/zyiwYNGqTZs2fLx8cn3fPei2EY+vHHH3Xo0CE5Oztbt/fs2VPbtm3T4sWLtW/fPr388stq1qyZjh49Kknas2ePGjdurICAAG3btk2bN29WixYtlJSUJEm6ceOGIiMjtXPnTkVHR8vBwUEvvPCCkpOT7/8b9Biz6/Q9AAAAAADwcB07dkyGYahs2bIZtouOjtb+/fsVGxtrDXw+++wzlS9fXjt27NDTTz8t6W54NW/ePOXLl0+S1LlzZ0VHR2vkyJHy8PCQs7OzdaRSiilTpqhKlSoaNWqUdducOXPk5+enI0eOqEyZMnr//fcVFRWl7t2768CBAwoLC1PLli0lKd3zpmfatGmaPXu2EhMTdfv2bbm6uurtt9+WdHc02Ny5c3Xq1CkVLlxYktS3b1+tW7dOc+fO1ahRozR27FhVr15d06ZNs56zfPny1j+3adPGpr85c+bIy8tLBw8eVIUKFe5ZH+5ipBQAAAAAADlYRms5/11MTIz8/PxsRiAFBATI09NTMTEx1m3+/v7WQEqSfH19df78+QzPvXfvXn3//fdyc3OzfqWEZCnT3pydnbVw4UItX75ct27d0ocffpjpa/ynTp06ac+ePdqyZYtCQkI0YMAA1alTR5K0f/9+JSUlqUyZMjb1bNq0yVpLykip9Bw9elQdOnRQiRIl5O7uLn9/f0my+bA23BsjpQAAAAAAyMFKly4ti8WSbYuZOzk52by2WCz3nLZ2/fp1tWjRQh988EGqfb6+vtY/b926VdLdKYeXLl1Kc7phZnh4eKhUqVKSpKVLl6pUqVKqVauWgoKCdP36dTk6OmrXrl1ydHS0Oc7NzU2SlDt37gzP36JFCxUrVkyzZs1S4cKFlZycrAoVKigxMTFL9T6uGCkFAAAAAEAOVqBAAQUHB2vq1Km6ceNGqv2XL1+WJJUrV06nT5/W6dOnrfsOHjyoy5cvKyAgINP9OTs7W9deSlG1alX9+uuv8vf3V6lSpWy+UoKn48eP65133tGsWbNUs2ZNhYWF2YRdaZ03M9zc3NSrVy/17dtXhmGoSpUqSkpK0vnz51PVkjI1sGLFioqOjk7zfH/88YcOHz6sgQMHqnHjxipXrpx1AXTcH0IpAAAAAAByuKlTpyopKUk1atTQ8uXLdfToUcXExOijjz5S7dq1JUlBQUEKDAxUp06dtHv3bm3fvl2hoaFq0KCBqlevnum+/P399fPPPysuLk4XL15UcnKyIiIidOnSJXXo0EE7duzQ8ePHtX79eoWHhyspKUlJSUl65ZVXFBwcrPDwcM2dO1f79u3ThAkTMjxvZr3++us6cuSIli9frjJlyqhTp04KDQ3VV199pdjYWG3fvl2jR4/WmjVrJEn9+/fXjh071KNHD+3bt0+HDh3S9OnTdfHiReXPn19PPPGEZs6cqWPHjum7775TZGRkpmvB/yGUAgAAAAAghytRooR2796tRo0aqU+fPqpQoYKaNGmi6OhoTZ8+XdLdaXirVq1S/vz5Vb9+fQUFBalEiRJasmTJffXVt29fOTo6KiAgQF5eXtYFxbds2aKkpCQ1bdpUgYGB6t27tzw9PeXg4KCRI0fq5MmT+uSTTyTdndI3c+ZMDRw4UHv37k33vJlVoEABhYaGaujQoUpOTtbcuXMVGhqqPn366KmnnlLr1q21Y8cOFS1aVJJUpkwZbdiwQXv37lWNGjVUu3ZtrVq1Srly5ZKDg4MWL16sXbt2qUKFCnrnnXc0bty4+7pHuMtiZHbFsxzi6tWr8vDw0JUrV+Tu7m7vcgAAAAAAj5Bbt24pNjZWxYsXl6urq73LAewmo5+FzGYvjJQCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJgul70LwIPx77fGLv3GjWlul34BAAAAAEDOwEgpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAIBP8/f01adIke5fxUMTFxclisWjPnj2SpI0bN8pisejy5csPrc9cD+3MAAAAAAA8LoZ6mNzflfs+5Ny5cxo5cqTWrFmjM2fOyNvbW5UrV1bv3r3VuHHjbCutYcOGqly5craHN5k9b8OGDbVp06ZU219//XXNmDEjU33NmzdPvXv3ThXI7NixQ3nz5rW+tlgsWrFihVq3bp2p86bn7zW7uLioaNGiCg8PV79+/WSxWB7o3P9mhFIAAAAAAORwcXFxqlu3rjw9PTVu3DgFBgbq9u3bWr9+vSIiInTo0CF7l5itunXrpuHDh9tsy5MnzwOf18vL64HPkZ6UmhMSEvTdd9+pe/fu8vT01JtvvvnQ+rQ3pu8BAAAAAJDD9ejRQxaLRdu3b1ebNm1UpkwZlS9fXpGRkfrpp5+s7U6dOqVWrVrJzc1N7u7uatu2reLj4637hw4dqsqVK+vzzz+Xv7+/PDw81L59e127dk2S1KVLF23atEmTJ0+WxWKRxWJRXFycJOnAgQMKCQmRm5ubfHx81LlzZ128eFHS3alizs7O+vHHH619jR07Vt7e3oqPj8/wvGnJkyePChUqZPPl7u4u6f+mqX311Vdq1KiR8uTJo0qVKmnbtm3WWsLDw3XlyhVrX0OHDpVkO33P399fkvTCCy/IYrHI399fcXFxcnBw0M6dO23qmTRpkooVK6bk5OR71lysWDGFh4erYsWKioqKsu5PSEhQ3759VaRIEeXNm1c1a9bUxo0bbc6xZcsWNWzYUHny5FH+/PkVHBysP//8U5K0bt061atXT56ennriiSf0/PPP6/jx4+nWYwZCKQAAAAAAcrBLly5p3bp1ioiIsJl6lsLT01OSlJycrFatWunSpUvatGmToqKidOLECbVr186m/fHjx7Vy5UqtXr1aq1ev1qZNmzRmzBhJ0uTJk1W7dm1169ZNZ8+e1dmzZ+Xn56fLly/r2WefVZUqVbRz506tW7dO8fHxatu2raS709d69+6tzp0768qVK/rll180aNAgzZ49Wz4+Pume90EMGDBAffv21Z49e1SmTBl16NBBd+7cUZ06dTRp0iS5u7tb++rbt2+q43fs2CFJmjt3rs6ePasdO3bI399fQUFBmjt3rk3buXPnqkuXLnJwuHcMYxiGfvzxRx06dEjOzs7W7T179tS2bdu0ePFi7du3Ty+//LKaNWumo0ePSpL27Nmjxo0bKyAgQNu2bdPmzZvVokULJSUlSZJu3LihyMhI7dy5U9HR0XJwcNALL7yQYVD2sDF9DwAAAACAHOzYsWMyDENly5bNsF10dLT279+v2NhYa+Dz2WefqXz58tqxY4eefvppSXfDq3nz5ilfvnySpM6dOys6OlojR46Uh4eHnJ2draN+UkyZMkVVqlTRqFGjrNvmzJkjPz8/HTlyRGXKlNH777+vqKgode/eXQcOHFBYWJhatmwpSemeNz3Tpk3T7NmzbbZ98skn6tSpk/V137591bx5c0nSsGHDVL58eR07dkxly5aVh4eHLBZLhn2lTOXz9PS0ade1a1e98cYbmjhxolxcXLR7927t379fq1atylTNiYmJun37tlxdXfX2229LujuCbe7cuTp16pQKFy5srX/dunWaO3euRo0apbFjx6p69eqaNm2a9Zzly5e3/rlNmzY2/c2ZM0deXl46ePCgKlSokGFtDwsjpQAAAAAAyMEMw8hUu5iYGPn5+dmMQAoICJCnp6diYmKs2/z9/a2BlCT5+vrq/PnzGZ577969+v777+Xm5mb9SgnJUqaQOTs7a+HChVq+fLlu3bqlDz/8MNPX+E+dOnXSnj17bL5SAq4UFStWtLkGSfe8jsxo3bq1HB0dtWLFCkl3F01v1KiRdbrfvWresmWLQkJCNGDAANWpU0eStH//fiUlJalMmTI293DTpk3W+5cyUio9R48eVYcOHVSiRAm5u7tb6zl16tQDX3NWMVIKAAAAAIAcrHTp0rJYLNm2mLmTk5PNa4vFcs8pYNevX1eLFi30wQcfpNqXEghJ0tatWyXdnXJ46dKlNKcbZoaHh4dKlSqVYZu/X0fKJ9xlx1Q2Z2dnhYaGau7cuXrxxRe1aNEiTZ48+Z7H/b3mpUuXqlSpUqpVq5aCgoJ0/fp1OTo6ateuXXJ0dLQ5zs3NTZKUO3fuDM/fokULFStWTLNmzVLhwoWVnJysChUqKDExMYtX+uAYKQUAAAAAQA5WoEABBQcHa+rUqbpx40aq/ZcvX5YklStXTqdPn9bp06et+w4ePKjLly8rICAg0/05Oztb1zFKUbVqVf3666/y9/dXqVKlbL5Sgqfjx4/rnXfe0axZs1SzZk2FhYXZhERpnfdhyWxfTk5Oabbr2rWrvv32W02bNk137tzRiy++eF/9u7m5qVevXurbt68Mw1CVKlWUlJSk8+fPp7p/KVMHK1asqOjo6DTP98cff+jw4cMaOHCgGjdurHLlylkXQLcnQikAAAAAAHK4qVOnKikpSTVq1NDy5ct19OhRxcTE6KOPPlLt2rUlSUFBQQoMDFSnTp20e/dubd++XaGhoWrQoIGqV6+e6b78/f31888/Ky4uThcvXlRycrIiIiJ06dIldejQQTt27NDx48e1fv16hYeHKykpSUlJSXrllVcUHBys8PBwzZ07V/v27dOECRMyPG96bt68qXPnztl83U8I4+/vr+vXrys6OloXL17UzZs3020XHR2d6vzlypVTrVq19N5776lDhw73HMWUltdff11HjhzR8uXLVaZMGXXq1EmhoaH66quvFBsbq+3bt2v06NFas2aNJKl///7asWOHevTooX379unQoUOaPn26Ll68qPz58+uJJ57QzJkzdezYMX333XeKjIy875qyG6EUAAAAAAA5XIkSJbR79241atRIffr0UYUKFdSkSRNFR0dr+vTpku5OYVu1apXy58+v+vXrKygoSCVKlNCSJUvuq6++ffvK0dFRAQEB8vLysi7OvWXLFiUlJalp06YKDAxU79695enpKQcHB40cOVInT57UJ598IunulL6ZM2dq4MCB2rt3b7rnTc+sWbPk6+tr89WhQ4dMX0OdOnX0xhtvqF27dvLy8tLYsWPTbDdhwgRFRUXJz89PVapUsdn32muvKTExUa+++mqm+/27AgUKKDQ0VEOHDlVycrLmzp2r0NBQ9enTR0899ZRat26tHTt2qGjRopKkMmXKaMOGDdq7d69q1Kih2rVra9WqVcqVK5ccHBy0ePFi7dq1SxUqVNA777yjcePGZamu7GQxMrviWQ5x9epVeXh46MqVK3J3d7d3OQ/Mv98au/QbN6a5XfoFAAAAAHu6deuWYmNjVbx4cbm6utq7HPyLjRgxQsuWLdO+ffvsXcpDkdHPQmazF0ZKAQAAAAAAZJPr16/rwIEDmjJlit566y17l/OvRigFAAAAAACQTXr27Klq1aqpYcOGWZ6697jIZe8CAAAAAAAAcop58+Zp3rx59i7jkcBIKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAAB4AHFxcbJYLNqzZ0+2ts3pctm7AAAAAAAAHnWB8wNN7W9/2P77PubcuXMaOXKk1qxZozNnzsjb21uVK1dW79691bhx42yrrWHDhqpcubImTZqUbee8n/M2bNhQmzZtkiS5uLioRIkS6tmzp3r06JGt9fydn5+fzp49q4IFC2Zr25yOkVIAAAAAAORwcXFxqlatmr777juNGzdO+/fv17p169SoUSNFRETYu7xs161bN509e1YHDx5U27ZtFRERoS+++CLNtomJiQ/cn6OjowoVKqRcue499ud+2uZ0hFIAAAAAAORwPXr0kMVi0fbt29WmTRuVKVNG5cuXV2RkpH766Sdru1OnTqlVq1Zyc3OTu7u72rZtq/j4eOv+oUOHqnLlyvr888/l7+8vDw8PtW/fXteuXZMkdenSRZs2bdLkyZNlsVhksVgUFxcnSTpw4IBCQkLk5uYmHx8fde7cWRcvXpQkbdy4Uc7Ozvrxxx+tfY0dO1be3t6Kj4/P8LxpyZMnjwoVKqQSJUpo6NChKl26tP73v/9JujuSqmfPnurdu7cKFiyo4ODge9YnScnJyRo7dqxKlSolFxcXFS1aVCNHjpSUekren3/+qU6dOsnLy0u5c+dW6dKlNXfu3DTbStKmTZtUo0YNubi4yNfXV/369dOdO3es+xs2bKi3335b//nPf1SgQAEVKlRIQ4cOzcy3/l+NUAoAAAAAgBzs0qVLWrdunSIiIpQ3b95U+z09PSXdDV1atWqlS5cuadOmTYqKitKJEyfUrl07m/bHjx/XypUrtXr1aq1evVqbNm3SmDFjJEmTJ09W7dq1rSOVzp49Kz8/P12+fFnPPvusqlSpop07d2rdunWKj49X27ZtJd0NXXr37q3OnTvrypUr+uWXXzRo0CDNnj1bPj4+6Z43s3Lnzm0zImr+/PlydnbWli1bNGPGjHvWJ0n9+/fXmDFjNGjQIB08eFCLFi2Sj49Pmv2ltFm7dq1iYmI0ffr0dKfrnTlzRs8995yefvpp7d27V9OnT9enn36q999/36bd/PnzlTdvXv38888aO3ashg8frqioqEzfg38jxooBAAAAAJCDHTt2TIZhqGzZshm2i46O1v79+xUbG2sNfD777DOVL19eO3bs0NNPPy3pbng1b9485cuXT5LUuXNnRUdHa+TIkfLw8JCzs7N1pFKKKVOmqEqVKho1apR125w5c+Tn56cjR46oTJkyev/99xUVFaXu3bvrwIEDCgsLU8uWLSUp3fPeS1JSkr744gvt27dP3bt3t24vXbq0xo4da339/vvvZ1ifr6+vJk+erClTpigsLEySVLJkSdWrVy/Nfk+dOqUqVaqoevXqkiR/f/90a5w2bZr8/Pw0ZcoUWSwWlS1bVr///rvee+89DR48WA4Od8cTVaxYUUOGDLHWP2XKFEVHR6tJkyaZvh//NoRSAAAAAADkYIZhZKpdTEyM/Pz8bEYgBQQEyNPTUzExMdZQyt/f3xpISZKvr6/Onz+f4bn37t2r77//Xm5ubqn2HT9+XGXKlJGzs7MWLlyoihUrqlixYvrwww8zVXdapk2bptmzZysxMVGOjo5655139Oabb1r3V6tW7b7qu3z5shISEjK9IPybb76pNm3aaPfu3WratKlat26tOnXqpNk2JiZGtWvXlsVisW6rW7eurl+/rt9++01FixaVdDeU+rvM3Pd/O0IpAAAAAABysNKlS8tisejQoUPZcj4nJyeb1xaLRcnJyRkec/36dbVo0UIffPBBqn2+vr7WP2/dulXS3SmHly5dSnO6YWZ06tRJAwYMUO7cueXr62sdbZTin+e9V30nTpy4r/5DQkJ08uRJffPNN4qKilLjxo0VERGh8ePH3//F/H9Zue//dqwpBQAAAABADlagQAEFBwdr6tSpunHjRqr9ly9fliSVK1dOp0+f1unTp637Dh48qMuXLysgICDT/Tk7OyspKclmW9WqVfXrr7/K399fpUqVsvlKCYiOHz+ud955R7NmzVLNmjUVFhZmE7qkdd70eHh4qFSpUipSpEiqQCot96qvdOnSyp07t6KjozN9H7y8vBQWFqYFCxZo0qRJmjlzZprtypUrp23bttmMaNuyZYvy5cunJ598MtP9PYoIpQAAAAAAyOGmTp2qpKQk1ahRQ8uXL9fRo0cVExOjjz76SLVr15YkBQUFKTAwUJ06ddLu3bu1fft2hYaGqkGDBta1kTLD399fP//8s+Li4nTx4kUlJycrIiJCly5dUocOHbRjxw4dP35c69evV3h4uJKSkpSUlKRXXnlFwcHBCg8P19y5c7Vv3z5NmDAhw/Nml3vV5+rqqvfee0//+c9/9Nlnn+n48eP66aef9Omnn6Z5vsGDB2vVqlU6duyYfv31V61evVrlypVLs22PHj10+vRpvfXWWzp06JBWrVqlIUOGKDIyMlOB2qMsZ18dAAAAAABQiRIltHv3bjVq1Eh9+vRRhQoV1KRJE0VHR2v69OmS7k4HW7VqlfLnz6/69esrKChIJUqU0JIlS+6rr759+8rR0VEBAQHy8vLSqVOnVLhwYW3ZskVJSUlq2rSpAgMD1bt3b3l6esrBwUEjR47UyZMn9cknn0i6O2Vu5syZGjhwoPbu3ZvuebPLveqT7n6iXp8+fTR48GCVK1dO7dq1S3dNJ2dnZ/Xv318VK1ZU/fr15ejoqMWLF6fZtkiRIvrmm2+0fft2VapUSW+88YZee+01DRw4MNuu79/KYmR2xbMc4urVq/Lw8NCVK1fk7u5u73IemH+/NXbpN25Mc7v0CwAAAAD2dOvWLcXGxqp48eJydXW1dzmA3WT0s5DZ7IWRUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAHioLBaLVq5cme7+uLg4WSwW7dmzx+61wDy57F0AAAAAAACPupiy5Uztr9yhmPs+5ty5cxo5cqTWrFmjM2fOyNvbW5UrV1bv3r3VuHHjh1Bl5vn5+ens2bMqWLCgXeuQpAsXLmjw4MFas2aN4uPjlT9/flWqVEmDBw9W3bp17V1ejkIoBQAAAABADhcXF6e6devK09NT48aNU2BgoG7fvq3169crIiJChw4dsmt9jo6OKlSokF1rSNGmTRslJiZq/vz5KlGihOLj4xUdHa0//vjjofWZmJgoZ2fnh3b+fyum7wEAAAAAkMP16NFDFotF27dvV5s2bVSmTBmVL19ekZGR+umnn6ztTp06pVatWsnNzU3u7u5q27at4uPjrfuHDh2qypUra86cOSpatKjc3NzUo0cPJSUlaezYsSpUqJC8vb01cuTIVDWcPXtWISEhyp07t0qUKKEvv/zSuu+f0/c2btwoi8Wi6OhoVa9eXXny5FGdOnV0+PBhm3OuWrVKVatWlaurq0qUKKFhw4bpzp071v1Hjx5V/fr15erqqoCAAEVFRWV4ny5fvqwff/xRH3zwgRo1aqRixYqpRo0a6t+/v1q2bGnT7vXXX5ePj49cXV1VoUIFrV692rp/+fLlKl++vFxcXOTv768JEybY9OPv768RI0YoNDRU7u7u6t69uyRp8+bNeuaZZ5Q7d275+fnp7bff1o0bNzKs+VFGKAUAAAAAQA526dIlrVu3ThEREcqbN2+q/Z6enpKk5ORktWrVSpcuXdKmTZsUFRWlEydOqF27djbtjx8/rrVr12rdunX64osv9Omnn6p58+b67bfftGnTJn3wwQcaOHCgfv75Z5vjBg0apDZt2mjv3r3q1KmT2rdvr5iYjKchDhgwQBMmTNDOnTuVK1cuvfrqq9Z9P/74o0JDQ9WrVy8dPHhQn3zyiebNm2cNxJKTk/Xiiy/K2dlZP//8s2bMmKH33nsvw/7c3Nzk5uamlStXKiEhIc02ycnJCgkJ0ZYtW7RgwQIdPHhQY8aMkaOjoyRp165datu2rdq3b6/9+/dr6NChGjRokObNm2dznvHjx6tSpUr65ZdfNGjQIB0/flzNmjVTmzZttG/fPi1ZskSbN29Wz549M6z5Ucb0PQAAAAAAcrBjx47JMAyVLVs2w3bR0dHav3+/YmNj5efnJ0n67LPPVL58ee3YsUNPP/20pLuhzJw5c5QvXz4FBASoUaNGOnz4sL755hs5ODjoqaee0gcffKDvv/9eNWvWtJ7/5ZdfVteuXSVJI0aMUFRUlD7++GNNmzYt3ZpGjhypBg0aSJL69eun5s2b69atW3J1ddWwYcPUr18/hYWFSZJKlCihESNG6D//+Y+GDBmib7/9VocOHdL69etVuHBhSdKoUaMUEhKSbn+5cuXSvHnz1K1bN82YMUNVq1ZVgwYN1L59e1WsWFGS9O2332r79u2KiYlRmTJlrH2nmDhxoho3bqxBgwZJksqUKaODBw9q3Lhx6tKli7Xds88+qz59+lhfd+3aVZ06dVLv3r0lSaVLl9ZHH32kBg0aaPr06XJ1dU237kcVI6UAAAAAAMjBDMPIVLuYmBj5+flZAylJCggIkKenp82IJn9/f+XLl8/62sfHRwEBAXJwcLDZdv78eZvz165dO9Xre42USgmCJMnX11eSrOfdu3evhg8fbh3d5Obmpm7duuns2bO6efOm9XpSAqm0akhLmzZt9Pvvv+t///ufmjVrpo0bN6pq1arWkU579uzRk08+aQ2k/ikmJibVguh169bV0aNHlZSUZN1WvXp1mzZ79+7VvHnzbK4nODhYycnJio2NvWfdjyJGSgEAAAAAkIOVLl1aFosl2xYzd3JysnltsVjS3JacnJytfVksFkmynvf69esaNmyYXnzxxVTHPeioIldXVzVp0kRNmjTRoEGD1LVrVw0ZMkRdunRR7ty5H+jcKf45lfL69et6/fXX9fbbb6dqW7Ro0Wzp89+GkVIAAAAAAORgBQoUUHBwsKZOnZrmotmXL1+WJJUrV06nT5/W6dOnrfsOHjyoy5cvKyAg4IHr+PuC6imvy5Url+XzVa1aVYcPH1apUqVSfTk4OFiv5+zZs+nWkFkBAQHWe1exYkX99ttvOnLkSJpty5Urpy1btths27Jli8qUKWNddyq96zl48GCa15NTP5mPUAoAAAAAgBxu6tSpSkpKUo0aNbR8+XIdPXpUMTEx+uijj6xT2oKCghQYGKhOnTpp9+7d2r59u0JDQ9WgQYNUU82yYtmyZZozZ46OHDmiIUOGaPv27Q+0iPfgwYP12WefadiwYfr1118VExOjxYsXa+DAgdbrKVOmjMLCwrR37179+OOPGjBgQIbn/OOPP/Tss89qwYIF2rdvn2JjY7Vs2TKNHTtWrVq1kiQ1aNBA9evXV5s2bRQVFaXY2Fjrwu+S1KdPH0VHR2vEiBE6cuSI5s+frylTpqhv374Z9v3ee+9p69at6tmzp/bs2aOjR49q1apVLHQOAADwb+Pfb41d+o0b09wu/QIA8CBKlCih3bt3a+TIkerTp4/Onj0rLy8vVatWTdOnT5d0d3rcqlWr9NZbb6l+/fpycHBQs2bN9PHHH2dLDcOGDdPixYvVo0cP+fr66osvvnigEVjBwcFavXq1hg8frg8++EBOTk4qW7asdTF1BwcHrVixQq+99ppq1Kghf39/ffTRR2rWrFm653Rzc1PNmjX14Ycf6vjx47p9+7b8/PzUrVs3/fe//7W2W758ufr27asOHTroxo0bKlWqlMaMGSPp7oinpUuXavDgwRoxYoR8fX01fPhwm0XO01KxYkVt2rRJAwYM0DPPPCPDMFSyZMlUn36Yk1iMzK54lkNcvXpVHh4eunLlitzd3e1dzgPjH+QAgMcVfwcCAOzh1q1bio2NVfHixXPkp6EBmZXRz0Jmsxem7wEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAcJ8es88MA1LJjp8BQikAAAAAADLJ0dFRkpSYmGjnSgD7unnzpiTJyckpy+fIlV3FAAAAAACQ0+XKlUt58uTRhQsX5OTkJAcHxnrg8WIYhm7evKnz58/L09PTGtRmBaEUAAAAAACZZLFY5Ovrq9jYWJ08edLe5QB24+npqUKFCj3QOQilAAAAAEn+/dbYpd+4Mc3t0i+ArHN2dlbp0qWZwofHlpOT0wONkEpBKAUAAAAAwH1ycHCQq6urvcsAHmlMfgUAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgulz2LgDAv49/vzV26TduTHO79IuHj2cKAAAAwD8xUgoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYLpc9i5g6tSpGjdunM6dO6dKlSrp448/Vo0aNdJtP2nSJE2fPl2nTp1SwYIF9dJLL2n06NFydXU1sWoAANIXOD/QLv3uD9tvl34BAACArLDrSKklS5YoMjJSQ4YM0e7du1WpUiUFBwfr/PnzabZftGiR+vXrpyFDhigmJkaffvqplixZov/+978mVw4AAAAAAIAHYddQauLEierWrZvCw8MVEBCgGTNmKE+ePJozZ06a7bdu3aq6deuqY8eO8vf3V9OmTdWhQwdt377d5MoBAAAAAADwIOwWSiUmJmrXrl0KCgr6v2IcHBQUFKRt27aleUydOnW0a9cuawh14sQJffPNN3ruuedMqRkAAAAAAADZw25rSl28eFFJSUny8fGx2e7j46NDhw6leUzHjh118eJF1atXT4Zh6M6dO3rjjTcynL6XkJCghIQE6+urV69mzwUAAAAAAAAgy+y+0Pn92Lhxo0aNGqVp06apZs2aOnbsmHr16qURI0Zo0KBBaR4zevRoDRs2zORKkdPElC1nl37LHYqxS78AAAAAADxsdgulChYsKEdHR8XHx9tsj4+PV6FChdI8ZtCgQercubO6du0qSQoMDNSNGzfUvXt3DRgwQA4OqWcj9u/fX5GRkdbXV69elZ+fXzZeCQAAAAAAAO6X3daUcnZ2VrVq1RQdHW3dlpycrOjoaNWuXTvNY27evJkqeHJ0dJQkGYaR5jEuLi5yd3e3+QIAAAAAAIB92XX6XmRkpMLCwlS9enXVqFFDkyZN0o0bNxQeHi5JCg0NVZEiRTR69GhJUosWLTRx4kRVqVLFOn1v0KBBatGihTWcAgAAAAAAwL+fXUOpdu3a6cKFCxo8eLDOnTunypUra926ddbFz0+dOmUzMmrgwIGyWCwaOHCgzpw5Iy8vL7Vo0UIjR4601yUAAAAAAAAgC+y+0HnPnj3Vs2fPNPdt3LjR5nWuXLk0ZMgQDRkyxITKAAAAAAAA8LDYbU0pAAAAAAAAPL4IpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApstl7wIA4HEUU7acXfotdyjGLv0CAAAAwD8xUgoAAAAAAACmI5QCAAAAAACA6Zi+BwBADsG0UAAAADxKGCkFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMx0LnAB57gfMDTe9zqek9AgAAAMC/CyOlAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmy2XvAgAAAPDvFVO2nOl9ljsUY3qfAADAfIyUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApmOhc2TNUA879XvFPv0CAAAAAIBsxUgpAAAAAAAAmI6RUgAAAABME1O2nF36LXcoxi79AgDSx0gpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpWFMKwL+HvT7VsXhR+/QLAAAAAI8xRkoBAAAAAADAdIRSAAAAAAAAMB2hFAAAAAAAAExHKAUAAAAAAADTsdA5AAAA8BgKnB9ol36X2qVXAMC/EaEUAADA/bDXJ4UOvWKffgEAAB4Spu8BAAAAAADAdIRSAAAAAAAAMB2hFAAAAAAAAExHKAUAAAAAAADTEUoBAAAAAADAdIRSAAAAAAAAMF0uexcAAAAAAMgZAucH2qXf/WH77dIvgAdDKIVHir3+kltql14BAAAAAMi5mL4HAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHS57F0AAAAPzVAP+/RbvKh9+gUAAAAeIYyUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApmOhcwAAgEdA4PxAu/S71C69AgCAxwEjpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApstl7wIAAACAx9pQD/v0W7yoffoFAOD/Y6QUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0dg+lpk6dKn9/f7m6uqpmzZravn17hu0vX76siIgI+fr6ysXFRWXKlNE333xjUrUAAAAAAADIDrns2fmSJUsUGRmpGTNmqGbNmpo0aZKCg4N1+PBheXt7p2qfmJioJk2ayNvbW19++aWKFCmikydPytPT0/ziAQAAAAAAkGV2DaUmTpyobt26KTw8XJI0Y8YMrVmzRnPmzFG/fv1StZ8zZ44uXbqkrVu3ysnJSZLk7+9vZskAAAAAAADIBnabvpeYmKhdu3YpKCjo/4pxcFBQUJC2bduW5jH/+9//VLt2bUVERMjHx0cVKlTQqFGjlJSUZFbZAAAAAAAAyAZ2Gyl18eJFJSUlycfHx2a7j4+PDh06lOYxJ06c0HfffadOnTrpm2++0bFjx9SjRw/dvn1bQ4YMSfOYhIQEJSQkWF9fvXo1+y4CAAAAAAAAWWL3hc7vR3Jysry9vTVz5kxVq1ZN7dq104ABAzRjxox0jxk9erQ8PDysX35+fiZWDAAAAAAAgLTYLZQqWLCgHB0dFR8fb7M9Pj5ehQoVSvMYX19flSlTRo6OjtZt5cqV07lz55SYmJjmMf3799eVK1esX6dPn86+iwAAAAAAAECW2C2UcnZ2VrVq1RQdHW3dlpycrOjoaNWuXTvNY+rWratjx44pOTnZuu3IkSPy9fWVs7Nzmse4uLjI3d3d5gsAAAAAAAD2Zdfpe5GRkZo1a5bmz5+vmJgYvfnmm7px44b10/hCQ0PVv39/a/s333xTly5dUq9evXTkyBGtWbNGo0aNUkREhL0uAQAAAAAAAFlgt4XOJaldu3a6cOGCBg8erHPnzqly5cpat26ddfHzU6dOycHh/3IzPz8/rV+/Xu+8844qVqyoIkWKqFevXnrvvffsdQkAAAAAAADIAruGUpLUs2dP9ezZM819GzduTLWtdu3a+umnnx5yVQAAAAAAAHiYHqlP3wMAAAAAAEDOQCgFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0+WydwEAAAAAADyImLLl7NJvuUMxdukXyCkYKQUAAAAAAADTEUoBAAAAAADAdIRSAAAAAAAAMB2hFAAAAAAAAExHKAUAAAAAAADTEUoBAAAAAADAdIRSAAAAAAAAMB2hFAAAAAAAAExHKAUAAAAAAADTEUoBAAAAAADAdIRSAAAAAAAAMB2hFAAAAAAAAExHKAUAAAAAAADTEUoBAAAAAADAdIRSAAAAAAAAMN0DhVKJiYk6fPiw7ty5k131AAAAAAAA4DGQpVDq5s2beu2115QnTx6VL19ep06dkiS99dZbGjNmTLYWCAAAAAAAgJwnS6FU//79tXfvXm3cuFGurq7W7UFBQVqyZEm2FQcAAAAAAICcKVdWDlq5cqWWLFmiWrVqyWKxWLeXL19ex48fz7biAAAAAAAAkDNlaaTUhQsX5O3tnWr7jRs3bEIqAAAAAAAAIC1ZCqWqV6+uNWvWWF+nBFGzZ89W7dq1s6cyAAAAAAAA5FhZmr43atQohYSE6ODBg7pz544mT56sgwcPauvWrdq0aVN21wgAAAAAAIAcJksjperVq6e9e/fqzp07CgwM1IYNG+Tt7a1t27apWrVq2V0jAAAAAAAAcpj7Hil1+/Ztvf766xo0aJBmzZr1MGoCAAAAAABADnffI6WcnJy0fPnyh1ELAAAAAAAAHhNZmr7XunVrrVy5MptLAQAAAAAAwOMiSwudly5dWsOHD9eWLVtUrVo15c2b12b/22+/nS3FAQAAAAAAIGfKUij16aefytPTU7t27dKuXbts9lksFkIpAAAAAAAAZChLoVRsbGx21wEAAAAAyC5DPezTb/Gi9ukXwCMpS2tK/Z1hGDIMIztqAQAAAAAAwGMiy6HUZ599psDAQOXOnVu5c+dWxYoV9fnnn2dnbQAAAAAAAMihsjR9b+LEiRo0aJB69uypunXrSpI2b96sN954QxcvXtQ777yTrUUCAAAAAAAgZ8lSKPXxxx9r+vTpCg0NtW5r2bKlypcvr6FDhxJKAQAAAAAAIENZmr539uxZ1alTJ9X2OnXq6OzZsw9cFAAAAAAAAHK2LIVSpUqV0tKlS1NtX7JkiUqXLv3ARQEAAAAAACBny9L0vWHDhqldu3b64YcfrGtKbdmyRdHR0WmGVQAAAAAAAMDfZWmkVJs2bfTzzz+rYMGCWrlypVauXKmCBQtq+/bteuGFF7K7RgAAAAAAAOQwWRopJUnVqlXTggULsrMWAAAAAAAAPCayFEp98803cnR0VHBwsM329evXKzk5WSEhIdlSHAAAAAAA+BcY6mGnfq/Yp1+YIkvT9/r166ekpKRU2w3DUL9+/R64KAAAAAAAAORsWQqljh49qoCAgFTby5Ytq2PHjj1wUQAAAAAAAMjZshRKeXh46MSJE6m2Hzt2THnz5n3gogAAAAAAAJCzZSmUatWqlXr37q3jx49btx07dkx9+vRRy5Yts604AAAAAAAA5ExZCqXGjh2rvHnzqmzZsipevLiKFy+usmXL6oknntD48eOzu0YAAAAAAADkMFn69D0PDw9t3bpVUVFR2rt3r3Lnzq1KlSrpmWeeye76AAAAAAAAkAPd10ipbdu2afXq1ZIki8Wipk2bytvbW+PHj1ebNm3UvXt3JSQkPJRCAQAAAAAAkHPcVyg1fPhw/frrr9bX+/fvV7du3dSkSRP169dPX3/9tUaPHp3tRQIAAAAAACBnua9Qas+ePWrcuLH19eLFi1WjRg3NmjVLkZGR+uijj7R06dJsLxIAAAAAAAA5y32tKfXnn3/Kx8fH+nrTpk0KCQmxvn766ad1+vTp7KsOAAAAAB5h/v3W2KXfOFe7dAsA9+W+Rkr5+PgoNjZWkpSYmKjdu3erVq1a1v3Xrl2Tk5NT9lYIAAAAAACAHOe+QqnnnntO/fr1048//qj+/fsrT548Np+4t2/fPpUsWTLbiwQAAAAAAEDOcl/T90aMGKEXX3xRDRo0kJubm+bPny9nZ2fr/jlz5qhp06bZXiQAAAAAAABylvsKpQoWLKgffvhBV65ckZubmxwdHW32L1u2TG5ubtlaIAAAAAAAAHKe+wqlUnh4eKS5vUCBAg9UDAAAAAAAAB4P97WmFAAAAAAAAJAdCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKb7V4RSU6dOlb+/v1xdXVWzZk1t3749U8ctXrxYFotFrVu3frgFAgAAAAAAIFvZPZRasmSJIiMjNWTIEO3evVuVKlVScHCwzp8/n+FxcXFx6tu3r5555hmTKgUAAAAAAEB2sXsoNXHiRHXr1k3h4eEKCAjQjBkzlCdPHs2ZMyfdY5KSktSpUycNGzZMJUqUMLFaAAAAAAAAZAe7hlKJiYnatWuXgoKCrNscHBwUFBSkbdu2pXvc8OHD5e3trddee82MMgEAAAAAAJDNctmz84sXLyopKUk+Pj422318fHTo0KE0j9m8ebM+/fRT7dmzJ1N9JCQkKCEhwfr66tWrWa4XAAAAAAAA2cPu0/fux7Vr19S5c2fNmjVLBQsWzNQxo0ePloeHh/XLz8/vIVcJAAAAAACAe7HrSKmCBQvK0dFR8fHxNtvj4+NVqFChVO2PHz+uuLg4tWjRwrotOTlZkpQrVy4dPnxYJUuWtDmmf//+ioyMtL6+evUqwRQAAAAAAICd2TWUcnZ2VrVq1RQdHa3WrVtLuhsyRUdHq2fPnqnaly1bVvv377fZNnDgQF27dk2TJ09OM2xycXGRi4vLQ6kfAAAAAAAAWWPXUEqSIiMjFRYWpurVq6tGjRqaNGmSbty4ofDwcElSaGioihQpotGjR8vV1VUVKlSwOd7T01OSUm0HAAAAAADAv5fdQ6l27drpwoULGjx4sM6dO6fKlStr3bp11sXPT506JQeHR2rpKwAAAAAAANyD3UMpSerZs2ea0/UkaePGjRkeO2/evOwvCAAAAAAAAA8VQ5AAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpctm7AAAAAAAAgH+TmLLl7NJvuUMxdunXXhgpBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwXS57FwAAAAAAADLHv98au/Qb52qXbpHDMVIKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACY7l8RSk2dOlX+/v5ydXVVzZo1tX379nTbzpo1S88884zy58+v/PnzKygoKMP2AAAAAAAA+Pexeyi1ZMkSRUZGasiQIdq9e7cqVaqk4OBgnT9/Ps32GzduVIcOHfT9999r27Zt8vPzU9OmTXXmzBmTKwcAAAAAAEBW2T2Umjhxorp166bw8HAFBARoxowZypMnj+bMmZNm+4ULF6pHjx6qXLmyypYtq9mzZys5OVnR0dEmVw4AAAAAAICssmsolZiYqF27dikoKMi6zcHBQUFBQdq2bVumznHz5k3dvn1bBQoUeFhlAgAAAAAAIJvlsmfnFy9eVFJSknx8fGy2+/j46NChQ5k6x3vvvafChQvbBFt/l5CQoISEBOvrq1evZr1gAAAAAAAAZAu7T997EGPGjNHixYu1YsUKubq6ptlm9OjR8vDwsH75+fmZXCUAAAAAAAD+ya6hVMGCBeXo6Kj4+Hib7fHx8SpUqFCGx44fP15jxozRhg0bVLFixXTb9e/fX1euXLF+nT59OltqBwAAAAAAQNbZNZRydnZWtWrVbBYpT1m0vHbt2ukeN3bsWI0YMULr1q1T9erVM+zDxcVF7u7uNl8AAAAAAACwL7uuKSVJkZGRCgsLU/Xq1VWjRg1NmjRJN27cUHh4uCQpNDRURYoU0ejRoyVJH3zwgQYPHqxFixbJ399f586dkyS5ubnJzc3NbtcBAAAAAACAzLN7KNWuXTtduHBBgwcP1rlz51S5cmWtW7fOuvj5qVOn5ODwfwO6pk+frsTERL300ks25xkyZIiGDh1qZukAAAAAAADIIruHUpLUs2dP9ezZM819GzdutHkdFxf38AsCAAAAAADAQ/VIf/oeAAAAAAAAHk2EUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMl8veBQAAAAAAAKQlcH6gXfpdapdeHz+MlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDp/hWh1NSpU+Xv7y9XV1fVrFlT27dvz7D9smXLVLZsWbm6uiowMFDffPONSZUCAAAAAAAgO9g9lFqyZIkiIyM1ZMgQ7d69W5UqVVJwcLDOnz+fZvutW7eqQ4cOeu211/TLL7+odevWat26tQ4cOGBy5QAAAAAAAMgqu4dSEydOVLdu3RQeHq6AgADNmDFDefLk0Zw5c9JsP3nyZDVr1kzvvvuuypUrpxEjRqhq1aqaMmWKyZUDAAAAAAAgq+waSiUmJmrXrl0KCgqybnNwcFBQUJC2bduW5jHbtm2zaS9JwcHB6bYHAAAAAADAv08ue3Z+8eJFJSUlycfHx2a7j4+PDh06lOYx586dS7P9uXPn0myfkJCghIQE6+srV65Ikq5evfogpf9rJCfctEu/Vy2GXfpN+ivJLv1eT7JPv/Z6TnmuHj6eKXM8Ts+UxHNlFp6rh49nyhyP0zMl8VyZhefq4eOZMsfj9Ew9DCnXYRgZPzd2DaXMMHr0aA0bNizVdj8/PztUk3N42K3nGLv0WsMuvUrysN+dtofH6bnimTLH4/RMSTxXZuG5MgHPlEkeo2dK4rkyDc9VTsUzZZIc9kxdu3ZNHhlck11DqYIFC8rR0VHx8fE22+Pj41WoUKE0jylUqNB9te/fv78iIyOtr5OTk3Xp0iU98cQTslgsD3gFMNPVq1fl5+en06dPy93d3d7lIIfguUJ245nCw8BzhezGM4WHgecK2Y1n6tFlGIauXbumwoULZ9jOrqGUs7OzqlWrpujoaLVu3VrS3dAoOjpaPXv2TPOY2rVrKzo6Wr1797Zui4qKUu3atdNs7+LiIhcXF5ttnp6e2VE+7MTd3Z03JGQ7nitkN54pPAw8V8huPFN4GHiukN14ph5NGY2QSmH36XuRkZEKCwtT9erVVaNGDU2aNEk3btxQeHi4JCk0NFRFihTR6NGjJUm9evVSgwYNNGHCBDVv3lyLFy/Wzp07NXPmTHteBgAAAAAAAO6D3UOpdu3a6cKFCxo8eLDOnTunypUra926ddbFzE+dOiUHh//7kMA6depo0aJFGjhwoP773/+qdOnSWrlypSpUqGCvSwAAAAAAAMB9snsoJUk9e/ZMd7rexo0bU217+eWX9fLLLz/kqvBv4+LioiFDhqSajgk8CJ4rZDeeKTwMPFfIbjxTeBh4rpDdeKZyPotxr8/nAwAAAAAAALKZw72bAAAAAAAAANmLUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAABpMgxDSUlJ9i4DQA5FKAUAwD3wmSB4GHiuAPzbXb58WRaLRY6Ojtq8ebN++OEHe5cEIIchlAIAIAPJycmyWCySpJs3byoxMdG6j1ABDyLlubpx44adK0FOkpycLIn3Jzy4ixcvqnLlypo/f742bNigBg0a6M6dO/YuC0AOQygF4LHAP86RVQ4Od/+qHDVqlJo2barWrVvro48+knQ3VODZwv1KCQ0kacWKFWrUqJEuXLhgx4qQk6S8Z504ccLOleBRd+fOHb322mt666231LJlS3355Zd69tlnbd7DgKz49ddf9b///Y9/Q0ESoRTsKOVN6MyZM/r999919OhRO1eEnCDluTp79qyOHTum3377TdLd8ID1EHA//v6P7g8//FATJ05U48aNVbBgQQ0ZMkR9+vSRRDCF+5OcnGwNDdatW6cNGzZo165d6t69u86fP2/n6pBTrF27VvXq1VNcXJy9S8EjrFChQqpZs6auX78uSbp69aqku8EnwRSyau/evQoMDNT+/futI4bxeMtl7wLweDIMQxaLRatWrdLIkSN17do1JSUl6YUXXtCIESPk7Oxs7xLxCEp5rlasWKFRo0YpPj5epUuXlo+PjxYsWCBHR0d7l4hHSEpw8NNPPylPnjxasGCBmjVrpmvXrqlhw4Z68803JUkTJkywBlP84wr3kvJcRUZGasOGDQoJCVGzZs20a9cuvfLKK/r888/l4+Nj5yrxqMubN6/y58+v3377Tf7+/jZhKJAZKX+nlS1bVitWrNCBAwf01ltv6a+//tIbb7xhDaZ4rnA/9u3bp9q1a+u9997TgAED0mzDv6ceP4RSsAuLxaINGzaoQ4cOmjhxooKCghQVFaWIiAg1bNhQISEh9i4RjyCLxaJvv/1WnTp10rhx49S6dWstXbpUffr0UUhIiDp37mzvEvGI2bp1q+rVqycPDw999dVXkqR8+fKpQ4cOkqSIiAhZLBaNHz+ef0Ah0zZv3qwvvvhCy5YtU7169SRJ8+bN06xZsxQWFqbPP/9cXl5edq4Sj4q0goH69evrqaeeUt++ffXTTz8RHCDTUgKBixcvyjAMFS1aVEWLFlWNGjX0119/6T//+Y8cHBzUvXt3OTg4aNmyZfL19bW+lwHpOXbsmCpXrqw+ffpo9OjRMgxDM2fO1IEDB1S4cGFVrVpVwcHB/HvqMcTfUDDV34f6rlmzRr1799Ybb7whJycnjR8/Xt27dyeQQpYYhqE7d+5ozZo16tWrlyIiIpQrVy59+OGH6tmzJ4EUsqRYsWIaNWqU7ty5Y/OJQ7lz51aHDh00ffp0TZw4UR9//LEdq8Sj5tq1a0pISJCvr6912yuvvKIOHTpo8+bNeu2115jKh0xLCZyuX79uM5W4X79+unPnjtatWyeJtRWROSkjzuvVq6dnnnlGjRo1UmxsrHx9ffX222/r7bffVmRkpPr376/33ntPnTt3VuHChe1dNh4Bp0+flnT3f+7dvHlTDRo00Ny5c7V79259/fXXCg8P1/Tp0+1cJeyBUAoP3dSpU1WlShUlJSVZh/reuXNHP/30kwoVKqSrV6+qbt26CgoKsr4RzZgxQ+vXr7dz5XiUWCyW/9feXcdFmXYNHP8NM4iIilhY2NiNgaJrB+batSp2165iJ3a3WNidayB2gbqiYmBiIIoiBoqI9PX+4cv9gLL1PLsO6vn+s8wdM2fc63PPPWfOdS4MBgOPHj0iffr0PH36FDs7OxwdHZk/fz4Au3btYuvWrUaOVCRXSfXHyJ49O926dcPZ2Zlp06Yxa9YsbZ+5uTmtWrVi37592lQ+IT6VMBEQ/3e2bNnIli0bly5d0sadwWCgU6dO2NjYcP/+fXr27MmbN2+MEbL4SiTsk7h06VJsbW0ZN24c165dA6BYsWKYm5uzY8cOAKk+EH8o/vp09epVevXqRceOHRk+fDjv37+nVq1aeHt7kzlzZgYPHoyLiwu7du3C09OTs2fPkjdvXiNHL5I7pRTVq1dnz549TJ48maxZs2Jtbc3WrVvx8vJi69atdOzYkTlz5uDr62vscMUXJkkp8a8rXbo0L1++pE6dOlpiymAw0Lx5c86ePUuhQoVo1KgRrq6u6HQ6IiIiuHDhAufPnyc6OtrY4YuvwJUrV7h48SIA+fLl47fffsPBwQFHR0eWLVsGfFxy3d3dnfv378u4Ep9JOP1lw4YNTJ48mQEDBnD16lXSpEnDkCFDGDVqFBMnTmT27NnaealSpaJBgwYYDAZZJlt8Ji4uTksEREVFERkZCUDBggXJli0bc+bM4bffftOODw0NpVixYjg5OfHw4UO8vLyMErf4OsT3SZw6dSqWlpa0b9+ea9euYW9vz+DBg7l27RozZ87kwIEDeHp6GjlakdzpdDouXbqEv78/PXv2ZNSoUXTu3JmzZ8+SI0cOWrVqxcWLF8mQIQODBw/m4sWLHDhwgDJlyhg7dPEViP8sbNy4MTt37qREiRL079+fXLlyAWBjY0OzZs14+vQpT58+NWaowhiUEF+At7e3ypMnj6pataqKiYlRSim1a9culSdPHmVnZ6fu3r2rlFIqMjJSjRw5UuXMmVP5+fkZM2TxFYiNjVWvX79WBQoUUNOnT1dKKXX16lVlZWWlcufOrZ49e6aUUiomJkaNHDlS2djYaGNNiKT88ssvKmPGjMrR0VEVLlxYZcuWTU2ePFm9fv1avXv3Tk2aNElZWVmpcePGGTtU8RWZPHmyqlOnjqpZs6batGmTUkqpt2/fqpIlS6qyZcuqUaNGqS1btqjq1aurxo0bq9jYWJUjRw41YsQII0cukqPY2Fjt723btimDwaBu3LihlFIqPDxcbd++Xf34448qT548qlSpUipbtmxqxowZSiml3YMJ8amIiAhVsGBBpdPpVPv27RPti46OVj/88IOytbVVZ8+eTTQGhfg9Dx48UDNnzlQjR45UixcvTrTv4cOH6sOHD0qp/1zT7ty5o0qWLKkuXrz4xWMVxqVTSiaYiy/D29ub1q1bkyNHDk6dOoVOp2PJkiXMnj2bbNmykTFjRpRSeHl5cfjwYUqXLm3skMVXYtKkSSxevBhPT0/y5cvHiRMnaNCgARUqVECv12NlZcXx48c5evSojCvxu9zd3enevTvu7u6UKFECnU7H6NGj2bdvH127dmXAgAEEBQUxf/58Ll26xKFDh2Q6jEhSwsq76dOnM3v2bJycnAgICGDbtm1MmjSJkSNH8u7dO4YOHcqVK1d4+/YtuXPnZteuXZibm1O9enXatWtH9+7djfxuRHK1bds23r17R0xMDD179ky0782bNwQHBzNp0iTOnj1LeHg4V65cIXPmzEaKVnwNAgICaN++PUFBQXh4eJAvXz6t8XlMTAx2dnaYmJhw7tw5UqZMaexwRTJ2/fp16tSpQ7ly5Xj27BkhISHUrl37D3tGDR8+nIMHD3L06FFZ7OM7I0kp8a9QSSzlGRsby+XLl2nZsiU2NjacOXMGgH379nHjxg0uXbpE2bJladq0KQUKFDBG2CKZi42N1aYrAERHR2NqasrDhw/p0qULTZs2ZcCAAQDcvHmTLVu2EBgYSJEiRWjcuDG2trbGCl0kM2PHjqVFixaUKFFC27Zp0yYmTZrE6dOnsbKy0sba4MGD2bFjB3fv3sXc3Jw3b95gaWmJTqeTZYvFH7pz5w4HDx6kWLFi1KpVi5iYGJYtW8bAgQOZMGECo0aNIi4ujsjISMLCwrSb8NGjR7Ny5Uq8vLzIly+fkd+FSI4CAwMpVqwYb9++Zfz48YwdO1brCfTpNcnHx4dBgwbRtGlTBg0aJNctAfznXj0sLAy9Xo9SilSpUhEYGEjNmjVJmzYtO3bsIGfOnNqxsbGxPHnyRJtyJURSAgICqFWrFs2aNWPatGm8ffuW7du3s2LFCrZu3Uru3LkTHX/lyhVWrlzJpk2bOHHiBCVLljRO4MJ4jFKfJb55cXFxSiml/Pz81KVLl9TDhw+1bd7e3ipnzpyqcuXK2jYh/siFCxcSPb569aoKDAxMtK1bt26qZMmSibbJ+BJJ8fLyUu3bt1fR0dGJtq9YsUJlyZJFhYaGKqU+ToNRSqmXL1+qtGnTqoMHDyY6XsaXSGjQoEEqJCREe3z8+HGl0+lUxowZ1dGjRxMdu2jRIqXX69W0adMSbb9586Zq1KiRypEjh7p8+fKXCFt8JT693sTExChPT09VokQJVb58efX+/XulVOKpffF/x8XFqTZt2qh27dp9uYBFshY/nvbv36+aNm2qihYtqpycnNTatWuVUko9efJEFS5cWJUrV04FBAQkOkeIPxIbG6sWLFig6tSpo16+fKltv3PnjkqbNq06e/ZsouP9/f3VmDFjlIODg7p69eqXDlckE9LoXPxjZs2axZ49e4CPv9Lt2rWLChUq0KxZMwoWLEjXrl05ffo0ZcuWZefOnTx+/Fj75ViI37Nq1SrGjh1LSEgI8PHX4d69e1OoUCGWL1/OhQsXAJgxYwbh4eFMnjz5s+dQUhAqEqhUqRLr16/HYDCwe/duTp06BcBPP/2EhYUFrVq1Aj6urgfw4sULrK2tSZ8+faLnkUoDEc/f3x9fX19Sp06tbbOzs2P8+PGEhoZy+/Zt4D/Xor59+7Jw4UJGjBjBxo0btXMKFy5Mhw4dOH78uEw1FpqEDfPfvXtHSEgIer0eBwcHXF1dCQwM5McffwTQVjmO/1v9f4VLxowZCQoKIiIiQj4TBTqdjn379tGiRQsqVKjA6NGjMTExwcnJiUuXLpE9e3YOHz5MREQENWvWJDAwUD7zxF9iYmKizVDIkCED8HGmQ86cOcmQIQMRERGJjs+VKxcdO3bk119/TVS9Lr4zxs2JiW9J69atlZmZmTp48KDy9/dXOXPmVAsXLlR+fn5qy5YtqmrVqqp+/frqzJkzSqmPFVOWlpaqQYMGRo5cJGeBgYHqwYMHSimlNS5/+vSpmjRpkipTpowqWrSoGjx4sLp165YaPHiw6tSpk1bpIsSn4isHYmNj1Z07d5Stra1q0aKF8vT0VEopdezYMZU9e3ZVpUoVdezYMXXkyBHVoEEDVaFCBWkQLP6SjRs3quDgYKXUx2bmI0aMUCYmJlqD84R27tz5WcWeEAklrE6ZNGmSql+/vsqaNavq37+/2r17t1JKqbNnzyobGxtVp06dJM+7evWqKleunPLx8flSYYtkLjQ0VDVq1EjNmjVLKaXUixcvVPbs2VW/fv0SHffo0SNVvnx57T5MiN9z//59tX//fhUTE6MiIyO17QmvRUWLFlXu7u7a423btn3RGEXyJUkp8Y/q2bOnSps2rXJ1dVXdunVLVEZ+/Phx5eDgoHr16qWU+lh6funSJVllT/yuhOPn0qVLqmLFimrVqlXatmvXrqlNmzap7Nmzq4YNG6oSJUoonU732TQrIZRSSa4WtHv3blWxYkXVunVrbZropUuXlL29vcqePbsqWLCgqlmzpoqKilJKycpV4nMJp0i9ePFCmZmZqRo1amjTFt69e6eGDRumTExM1ObNm5N8DklMiT8zatQolSFDBrV9+3a1c+dOValSJWVra6uePHmi4uLi1NmzZ1WuXLlUmTJlkjz/1atXXzhikZy9e/dOFS5cWJ08eVI9ffpUZc+eXXXv3l3bv23bNnXlyhWllHzuiT/35MkTpdPplLm5udq5c6d2zxQvNjZWRUdHqzx58mhJqbFjxyqdTicJT6GUkul74h/m6upK69at6d27NydPnuT169favurVq9OtWzdWr15NYGAger2eMmXKkD9/fiNGLJKz+NWrAKytrUmdOjUbN25k9erVABQvXpy2bdty9epV6tWrp40lacApPpVwNbQFCxYwZswYAH788UdGjRrFw4cPmTVrFhcuXKBMmTKcO3eOo0ePcujQIQ4fPoypqSkxMTGJGu0LAf+5Tvn6+pIxY0YuXbrE/fv3adeuHa9evSJ16tSMHj0aZ2dnOnbsiJub22fPYTAYvnTY4ivi5+eHh4cHO3bsoEWLFqRLlw4fHx+GDx9O9uzZAahYsSJr164lZ86c2vQ9QPv70+nH4vuj/n/aZvx/ixQpwuXLl3FwcKB+/fq4uroC8OzZMw4ePMjt27dRSsnnnvhTVlZWFC1aFL1eT9euXdm5cyexsbHa/ri4OKKiooiNjcXS0pJZs2Yxa9YsvL29yZMnjxEjF8mGkZNi4hsUExOjhgwZokxMTNSGDRsSVSdcvHhR5cuXT925c8eIEYqv1ePHj1WTJk3UDz/8oFavXp3kMfHTZoRIytChQ5WNjY2aOXOmevjwobZ9z549qly5cqp169baFOOEkqqyEiKeq6urqlq1qvb4xo0bKnv27IkavYaFhanevXurypUrGylK8bXy8/NT+fPnV6GhoWrXrl0qderUaunSpUqpjwsyrFu3Tj158iTROXLNEvHip0/FL94Rb8yYMUqn06l69eolqtYcMWKEKliwoPL39/+icYqvU3wlnYuLixo1apT6+eeflampqdq8efNnVXb29vaqVKlSKmXKlMrb29sY4YpkSn6aE/8T9f8NNF+9ekV0dDRZsmRBr9czc+ZM3rx5Q8+ePYmNjaVmzZpkyJCBLVu2EBUVhZWVlbFDF8lY/Lh6/PgxL1++JGvWrKROnZocOXKwYMECBgwYwOrVq9HpdHTq1AmAmJgYDAaDtqS6EJ9yc3Nj7dq1uLu7Y2dnB6AttNCkSRPMzc0ZN24cLi4uzJ49m2LFimnnJqzaE9+35s2bU6tWLXr37q1dq549e4aNjQ3wcUwVKVKEw4cPU6dOHdq3b8/GjRvJkCEDs2fPJmXKlEZ+ByI5ix9TCUVHR5MyZUoWL17M9OnTmT59Or169QI+Vujt2bMHW1tbrWoK5Jol/kOn03HgwAHmzJlDpkyZKFWqFMOHD2fixIkEBwezfv16xowZg16vJygoiO3bt3Pq1CmpOhd/SXwlXcWKFWnRogWenp6kSZMGJycnAFq2bIler+fDhw88ffqUp0+fcvnyZYoXL27EqEVyI59Y4n+i0+nYvXs3tWvXxt7enoEDB3Ljxg0AVqxYQdu2bXFycqJixYr069eP3bt38+uvv0riQPyu+Bvy+HHVuHFjateuzYQJE/D39ydnzpwsWLAAKysr1q1bx7JlywCZ/iJ+n1KK2NhYfH19adasGXZ2dty8eZNly5ZRrlw5SpYsyc6dO6lTpw6DBw8mZ86cFClSxNhhi2QoMjKSnDlz0r9/f9atW6clD54/f46FhQXwn2tRfGLq1q1b1KlTh9DQUMzNzdHpdLL6mfhd8WPKzc0NZ2dn4OOqjJUrV2bkyJH07duXPn36ABAeHs6ECROIiIigfPnyRotZJG/nz5+nbdu2FC1alOjoaFavXk27du2Aj203hg4diq+vL8eOHUOv13P27FlKlSpl3KBFsnbv3j3279+Pn5+ftq1mzZp06NCBJUuWMH78eLp3707nzp3Zvn070dHRmJubM3nyZG7duiUJKfE5I1ZpiW+Ar6+vsrGxURMnTlTz5s1TWbJkUY0bN040/WXYsGFKp9OpLVu2qBcvXhgxWpEcJZxiEF8+7uHhodKmTavmzp2r3r9/r8aMGaMyZ86s2rVrp+7du6eUUiogIEBVr15dNWjQQL1588YosYvkK+G4iv978uTJKkWKFMrFxUWVLl1aNWnSRE2ePFm1atVKZc2a9bNVG2X6i0hKWFiYGjdunNLpdNo0YicnJ23Vqk+bll+5ckU1adJExpP4y8LCwlT//v1V8eLF1bhx45RSSkVFRalmzZqp1KlTqxEjRqghQ4aoGjVqqKJFi2pNhWWMiXgJVzzz8PBQU6dOVUop9ebNG7V582aVI0cO1apVK+2Y0NBQFRMTI4suiD8V39Rcr9erihUrqlGjRqn79++r2NhY5eHhoezs7LT78r59+6o0adKoNWvWKKXkGiV+n5QWiL9F/f+vu/G/5KVIkYIWLVpoTYNr1apFy5YtmTFjBjqdDgcHB6ZNm8b79+8pXbo0GTNmNFrsInkyMTHh0aNH5MyZE4PBQFBQEAsXLmTo0KEMGjSIFy9esG7dOgoUKICvry9jx45l8uTJ5M6dm3Xr1gFgaWlp5HchkpOETc2XLVtGSEgIv/zyC507d+bt27ds2bKFbt26UadOHYoUKYK3tzdPnz4lLCyMNGnSaM8j019EUiwsLBg6dChKKbp06ULGjBnJkycPjx8/5u7du7x584YMGTKQMmVKrl27hqOjI3v27AESj00h4n06LiwsLBgxYgRp0qRh79696HQ6xo0bx86dOxk9ejSXLl3C1NQUOzs7pkyZgsFg0KawC6H+v+L8woULBAUFsXPnTm1qp6WlJY0bNwbA2dmZdu3asWnTpkSffUL8kezZs1OhQgUuXrxIjRo12Lt3L9euXSNNmjTMmTOH6OhoXFxcmDVrFosWLeLDhw8MHz6cpk2bkjZtWmOHL5IpnVJSQy7+uvgPupMnT3Ls2DH8/PywtLTUplABXL9+nVatWlGoUCEGDhxItWrVjBewSPYiIyOpVq0aQUFBPHjwAJ1Ox7Zt2yhSpAhZsmThhx9+oEqVKixbtozBgwezcuVKqlatyvz588mXL5+xwxfJ2NChQ9m8eTPDhw/nxx9/JEeOHACEhoZqN0axsbE0bNgQg8GgffkT4lNJJZPevXvHtGnTmDZtGqamptja2hIREcHLly9JkyYNSily5syJp6enjCvxl/j4+FC6dGntcVBQEPPnz8fDw4NmzZppPwCGhYWROnVq7bjY2FhZIU0ksmfPHtq0aUP27NkJCwujcOHCnDhxQrsWhYeHs3//fjp37kzbtm1ZuXKlkSMWyZ1SiujoaFKkSAGAvb09SilcXFxQSrFmzRpu3brFixcvsLa25vDhw1oxwvPnz7G2tjZm+CK5M1aJlvh6eXh4KJ1Op+rUqaPMzMxUjhw51Pbt2xMdc/36dWVtba3atGnz2WofQiQUFxenzpw5o4oVK6ZKly6dqOR8wYIFiVavWrlypSpWrJhq3br1ZysNCZHQsmXLlLW1tbpw4UKi7fFTE8LDw9XWrVtVjRo1VMmSJbXpLwnHnxBKJZ5ucOvWLXX58mXt8YcPH9T06dOVXq9XEyZMUFFRUerp06cqMDBQ+fv7a+fKuBJ/Zt++fapQoUJqyZIlibYHBgaq7t27q2zZsqkZM2Z8dp6MLREvfiyEhoaqRo0aqXXr1il/f3+1Z88elTlzZtW0adNEx4eFhamdO3equ3fvGiNc8RW5c+eO6tevn2ratKmaMmWKtr1MmTKqcOHC6ty5c0oppU6fPq0mTZqkNmzYoJSS65P466SGXPwt/v7+eHp6snTpUg4dOsTFixextbVl9erV/Prrr9pxxYoV4/jx40yaNAlzc3MjRiySm7i4uESPdTodlSpVYsWKFYSHh1O2bFltmmhQUBDPnj3THt+5c4e2bduydOnSRKsMCfGpS5cu0aJFC8qVK8etW7dwc3PD3t6ecuXKcfLkSV6/fo2fnx+5c+fm4sWLmJqaEhMTIxUt4jPxFVLOzs7Url2batWqUaVKFX777TfMzMzo378/o0aNYsKECWzdupWsWbOSLVs2cuXKhYmJCXFxcTKuxJ/KkycP5cqVY+PGjYmqz7Nly0aPHj2IiIhg5syZrFixItF5MrZEPJ1Ox4kTJ2jSpAnwcTW0XLly0ahRI7Zs2YKXlxdNmzbVjrewsKBZs2bY2toaK2TxFbh69SqVK1fmyZMnmJmZMW7cOKZOnQp8vNdKkyYNrVu35ty5c1SpUoVRo0bRvn17QK5P4q+T6XviL7t+/ToDBw7k+fPnLF26lB9++AH4eLH6+eefMTU1pU+fPtpcdSE+FT8FJigoCH9/f+zt7bV90dHR+Pj48NNPP5EqVSp8fHw4ePAgo0ePJnPmzKRNm5YDBw5w+fJlChYsaMR3IZIb9f/TilWCpdTHjRvHqlWr6N69O+7u7mTJkoUiRYrg5+eHt7c3d+7cITIykrRp06LT6WT6i/hMwil7u3btYvjw4cyaNYsMGTIwYMAAYmJimD17NjVq1CA2NhYXFxcmTZqEu7s79erVM3L0Ijk7e/Yst27d4tKlS2TKlInmzZtTokQJnjx5wtixY7l16xZOTk707NkT+Ditb8aMGdSqVYvOnTtLXzLxu65fv07dunUJDg7m3LlzlCtXDvh4PTt16hQdOnSgYMGCHDt2zMiRiq/BtWvXsLe3Z/DgwUyePJm4uDgGDhyIwWBgwoQJWiuE6tWr8/DhQzZt2oS9vb1co8TfJiNG/C1WVlYEBARw7tw5bVvJkiWZO3cuAFOnTsXd3d1Y4YlkzsTEhMePH1OsWDEqVapE9erVGTlyJMePH+fDhw+UL1+ejRs3otPpsLe3p379+gwYMAArKytiYmL47bffJCElEklYhRIaGkpERAQxMTF07dqVZs2asWHDBlq1asXkyZOZOnUq3bt3J0+ePERGRmJpaaklsyQhJT4Vf1O9Y8cO/Pz86Nu3L40bN8bBwYHz58+TJk0afv75Z44fP45er2f06NEsW7aMWrVqGTlykZytWrWKVq1asXXrVk6dOsXy5cupUKECI0aMIG3atEyYMIHChQvj5ubGiBEjuHLlCiNHjsTS0pIuXbpgYmJCbGyssd+GSGYeP35MSEgIxYsX5+jRo2TLlo1Ro0bx8uVL4OP1rGrVqri5ufH48WOePHli5IhFcvf48WNq1qxJw4YNmTx5MvBxHL148YKTJ09SoUIFatWqxb59+zhx4gS5c+emYcOGeHt7Gzly8VUy4tRBkcwlNQ/4zp07qn379qpUqVLaUtjxLl26pJo0aaIePXr0hSIUXyN/f39VqlQpVbBgQVW2bFnVqVMnlTJlSlWqVCnVoUMHtXXrVrVt2zaVL18+1aBBA+28iIgII0YtkqOEvX5mzJih6tatq8qXL69atmypXYfilyWOP75u3bqqadOm0udA/K74sREbG6s+fPig0qRJo3Q6nRo4cGCi46KiopSDg4MqVaqU2r9/f6IxJcuqi6Rs27ZNpUqVSu3YsUOFhYUppZQKCAhQgwcPVjqdTjk7OyullLp3754aO3assra2Vnnz5lUVK1aUvnciSXFxcerOnTvK0tJSzZkzR71+/Vop9bG3a9asWZWjo6N68eJFouPfv39vrHDFV+Thw4eqXLlyqnHjxsrT01MppdTUqVNVqlSplIuLi1q5cqUqXLiwyp07t3bPVbNmTeXn52fMsMVXSqbviSSp/58Gc+7cOa5evcqDBw9o3749JUuW5MGDB4wbN44HDx7QvXt3nJyctPOioqK0VRmE+D337t3D2dmZuLg4RowYQdasWTl79iyLFi0iOjoaX19f8uXLh6+vL02aNGH37t2JpmYJkdDIkSNZsWIFc+bMIXXq1IwYMYIPHz5w/fp10qZNy/v37zl27Bjz58/n1atXeHt7Y2pqKmNK/KG3b99iaWnJmzdvqFKlCjExMWzatIlSpUpp4yY6OppixYpRoUIF1q1bZ+SIRXKllCI0NJQOHTpQuXJlnJ2diYmJwWAwaMc4Ozszb948Dh48SM2aNfnw4QNv377l6dOnlCpVChMTk8/OESJe//792b59O2PHjqVt27ZYWVnh6+tLnTp1KFu2LKtWrSJTpkzGDlN8Zfz8/BgwYAApUqQgc+bM7N27l/Xr11OnTh0AAgICyJ07NwsWLKBfv35GjlZ8zWT6nkiSTqdj586dNG7cmL1793L9+nUqVqyIi4sLefPmZcSIEeTNm5fVq1fj6uqqnScJKfFX5M+fn6lTpxIREcGYMWN4/vw5bdq0wdPTk0OHDuHq6kqjRo0oVaoUY8eOBaRZokja/fv3OXz4MNu2baNDhw4YDAaeP3/OsGHDtF4HQUFBnD9/npw5c0pTc/GXLFu2jPHjx/PgwQPSpUvHqVOn+PDhA3369OH69evacaampty4cYPVq1cbMVqR3MVPE7506RIZMmQA0JJL8b8Nu7i4UKpUKebOnUtcXBwpU6YkS5YslClTRmuYLwkpAf8ZMwnrChYuXEiHDh0YM2YMmzdv5s2bNxQrVowjR45w6NAh+vbt+9lCM0L8GVtbW+bPn8+HDx/YuHEjzs7O1KlTB6UU0dHR6PV6SpQoQZYsWYDEY1KIv0OSUiJJN27cYNCgQcycORN3d3f27dtHRESEtr9IkSKMHDkSKysr9uzZw9u3b40YrfgaFSxYkIULF2JiYsKYMWM4deoUAOnSpaNDhw5MnjyZCxcuULp0aSNHKpKThDfV0dHRxMTE8OjRIypXrsyBAwdo164dU6dOpU+fPrx//55ly5aRI0cOhg4dipubGwaDgdjYWPlyJ/5QYGAg+/btY/Xq1Tx48ID06dPj4+PDs2fP6NmzZ6LElMFgQK/XS58f8Ydev37Nhw8ftBWJY2JigP8krMzMzLCzs+PVq1dar7yEX/CkcbCIp9PpOHbsGHv27CEqKkrbPnPmTLp06YKzszMbN27k9evXFC1alCtXrjBp0iQZQ+K/UqBAAZYuXUqVKlU4duwYZ86cQafTYWpqyrJlywgNDaVChQqA/IAs/ntydRJJev36Nfnz58fJyYnbt2+TN29eunbtypgxYwB48uQJhQsXZurUqbi5uWFpaWnkiMXXyNbWloULF6LT6Zg6dSpnz55NtF8SB+JT8TfVkyZNYunSpVhYWGBvb8+UKVNo06YNs2fPplevXsDHaaIeHh74+PhgZWUlTc1FkpKqHpg4cSI9evRg+/btrFq1igcPHmBlZYWPjw8vXrzgxx9/5P79+4nOkXEl/kju3LkpU6YMs2bNIjg4GIPBoI29+P9aWlpSvHhx7bNPvuCJT8UnKl1dXWnevDkeHh6fJaYaNWrEtGnTWLt2LSEhIRQuXJgCBQoYK2TxDciXLx+LFi1CKcXkyZO1FUFnzpzJzp07sbGxMXaI4isnSSmRpMePHxMcHExgYCCOjo7Uq1ePZcuWAXDo0CHGjx/Py5cvKVy4MNmyZTNytOJrZmtry4IFCzA1NeWXX37h/Pnzxg5JJEMJEwfbtm1j8eLFVKlShUyZMmlLE/fr148ePXoAEB4ezogRI4iJiaF8+fLaufIlT3wqPtF56dIlXr16pW13dnbGycmJnTt3snLlSh49eoSVlRW//fYbpUuXJnfu3EaKWHyNTExMaNy4MY8fP2bs2LE8f/5cG3t6vZ7o6GhOnTrFpk2baNy4MStXruTNmzfGDVokG/HJqGfPngGwfft22rVrR8eOHXF3d0+UmCpUqBAfPnxg6dKlRolVfJsS3q/Xq1eP0aNH4+npKTMaxD9CklJC+6C7c+eONiWhYcOGpE+fnly5clG9enWWL1+ufZk7fvw4AQEBUgYs/jG2trbMnDmTHDlySJJTJCn+erN79278/PwYMmQIpUuXxszMjA0bNlCiRAkOHjzIwIEDmTZtGg0aNODJkyfs2rVL68cixO85cuQINWvWZMOGDYSEhGjbhw8fTtu2bZk3bx4rV67kzp07ZMiQgR07dsiUPfGXxV9/BgwYQJMmTdi2bRsdO3bkt99+48GDB1y5coUmTZrw6tUrhg0bRvXq1cmdOzfp0qUzbuAiWYhflGPfvn00btxYW1Rhw4YN1K9fHycnJw4cOKAlMSMjI9m+fTteXl5YWVkZMXLxrbG1tWXWrFnY29vj4+ODnZ2dsUMS3whZfe87F/9Bt3v3boYNG0afPn1o1aoVmTJlYvny5bi6ulK2bFnmzp2Lv78/W7duxdXVlTNnzlCsWDFjhy++MbJ6o/gj7969I2PGjERHR9OvXz8WLFig7QsLC2P06NFcu3YNc3Nz7cbJYDDIilXiM0mtvNi7d2+OHTtG//79+emnn7Qvc2FhYRQqVIiIiAimTJlCjx49ZOVGkaSrV69SsmTJz7Z/Ol7GjBnDrl27uHv3LqlSpcLW1pYMGTJw4MABuVaJJO3du5fWrVszdepUHBwcKFeunLavQ4cOHD9+nBIlSmBubs6hQ4e4cuUKtra2RoxYfMuio6MxNTU1dhjiGyJJKYG7uzstW7Zk2rRpdOjQQftlLiIigmXLlrF8+XLu37+Pra0ter2eNWvWUKpUKaPGLIT49iX1xT8oKIgqVapgMBjYtGlTorJxpZTWIDi+skoSUuJTCZPfUVFR6PV6rR9U3759OXjwIIMHD9Y+D+/fv8+8efMoXLgwPXv2lN5RIkm//PIL4eHhLF26VLt2xV+PdDod27dv59SpUyxatAj4uJS6r68vHz58IHfu3JQqVQq9Xi/XLPGZkJAQGjVqpE2ZipfwWjZr1ixu376t/UAjPxwLIb4mkpT6jimlCAsLo1mzZjg4ODB+/Hjev3/Pixcv2L9/P1myZKFFixbExcVx6NAh8uXLh5WVFZkyZTJ26EKIb1xcXJyWWHr79i2pUqVCKUWKFCl48uQJZcuWpUiRIixZsoRChQoBnyexpJpFJHT06FFq1aqlPZ41axZHjhwha9asVKtWDScnJ+BjYuro0aPUqVOHSpUqsWHDBszNzdmxYwcAsbGxkpgSn/Hy8qJChQoYDAYeP36cqPHv9u3b6dSpEzNnzqRv376/+xwJr3tCxHvy5AkVKlRg+fLlNGjQINFn26djRpKaQoivkXzyfcd0Oh0WFhaYm5vz9u1b7t+/z6hRo+jSpQszZsygZ8+eDBw4EBMTExwdHSlQoIAkpIQQ/zqllHaT7eLiQrNmzbCzs2PUqFGcPXuWHDly4O3tzY0bN+jbty+3b98GPm9iLgkpEW/VqlW0bNmSlStXAh9XqJo6dSpFixbl2bNnTJ06lXHjxgGwePFiOnTogI+PD2PGjCEqKorNmzcDyOqN4jPxv+06ODhgMBjYuHEjLVq04PTp0wD4+/szduzYP01IAZKQEonEj60UKVJoVZuAVoUHcOLECa3HFMiqxUKIr5N8+n1nPi2MU0qRP39+zp07R8GCBQkMDKRLly5cuXIFJycngoKCjBSpEOJ7FD/dBWD+/PnMmTOHJk2aUKVKFa5fv06HDh04evQoNjY2XLx4kdu3b9OqVSsePXpk5MhFcmZvb0+nTp2YNWsWixYt4sOHD2zbto05c+awevVqOnbsyOrVqxk7diwAo0ePZseOHZw6dYpDhw5hampKTEyMJDrFHwoPD8fGxoa0adMyffp0vLy8yJ07N7t37/7ThJQQkPg+Pf56ky5dOmxtbVmzZg3e3t7AfxKYhw4dYs2aNYSGhn75YIUQ4h8i0/e+I/HlvhcvXuTevXukTp2ahg0bEhMTw5UrVwgODqZ+/fracZ07dyYuLo5Vq1bJLy9CiC/K19eXWbNm0ahRI5o3bw6Aj48P8+fP5+LFi2zZsoVixYoREBBAv3792L17t1SwiD/05MkTZs6cydGjRwkLC2P//v0UL14c+NirzM3NjWXLltGlSxetaiqeTKsSSdm7dy9ZsmShfPnyDBkyhKCgIDZs2MDBgwdZsGABcXFxjBs3jkqVKgEypVj8sfjxcfr0aQ4fPkxISAgODg60a9eO0NBQKlWqRKpUqWjSpAm5c+fmzJkzbN68GU9PT+1aJoQQXyPJNHxHdDode/bsoXXr1hQuXJhr167Rtm1bJkyYQNmyZbXjnj17xrx58/j11185c+aMJKSEEF/Uvn376NSpEylSpKBFixba9tKlS9OzZ0+uXr3KzZs3KVasGDlz5mTv3r2A9PoRnzt//jzXrl0DoHPnzvTp0wedToerqyvHjx/XvshlyZKFLl26YGJiwtixY8mZMyedO3fWnkcSUuJTsbGxLFmyhPPnz1O/fn3279/PmTNnAHB0dARgwYIFTJgwgdGjR1OlShVJSIk/pNPp2LVrFx06dKBq1arExMTg6uqKu7s78+bN49y5c/Tv359ff/2VN2/eaIkpSUgJIb52cpf1jYqfaw7/KQV+8eIFS5YswdXVFU9PT7y8vDhy5AgjRozA19cXAA8PDwYNGsT+/fs5fvw4RYsWNUr8Qojvx6cFu40aNaJdu3YEBwfj4eFBSEiItq9ixYrodDrOnz//2fNIQkoktGbNGjp16sSZM2fQ6/WYmJhQsGBB+vfvT7du3Vi4cCErVqzQjs+SJQsdO3Zk5cqVdOzY0YiRi+Rs1apVBAUFodfr8fDwIGXKlOzevZulS5dSsmRJYmNjgY+JqQEDBmAwGJg6dSpHjx41cuQiuQsICMDZ2ZnZs2fj7u7O4cOHOX78OCdOnOCXX34hTZo0rFy5ktOnT3P27Fn27NlDiRIljB22EEL8z6QE5hsUP83Az8+Px48fU6NGDQ4dOsSOHTuwsrLC0dGR1KlTU7FiRdzd3WnQoAETJ05k2rRp1KlTh8jISMqUKZNo5RghhPg3/N60qEWLFhEdHc2BAwcoWLAgTk5OpEmThrCwMAAyZ878pUMVX5FNmzbRt29f1q5dS8OGDUmZMqW2L1++fAwYMAC9Xs/s2bPR6XR069YNgGzZsmkJKam8E5/y9vame/fu9O/fn+HDh5MuXTqyZMlC3rx5GTp0KPny5cPe3l6bhuXo6IhOp2P06NEcOHAg0eqPQnwqNjYWpRQlS5bUHletWpWNGzdSs2ZNmjZtyo8//ojBYEh0TRNCiK+d9JT6xsR/wbty5QpVqlRhxowZ9O7dm4MHD9KgQQNSpkyJl5cXpUuXTtRjqmnTphQsWJAVK1aQJ08eY78NIcR3IGF/laVLl3LhwgWKFy9O5cqVKV++PPBxytXBgwcpWbIkdnZ23Lp1i7t373LlyhVMTU2NGb5Iph4/fkzLli1p1qwZzs7O2vZP+/ncvXuXxYsXc+TIEXr06MGgQYOMEK342uzdu5dmzZrRq1cvFi5cqI2phg0bcunSJXbv3o29vb12fFRUFA8ePKBAgQIyDVT8odu3b1O6dGm2bNlCkyZNiI6O1qo87e3tqVevHuPHjzd2mEII8Y+TT8dvSHxC6urVqzg4ONCvXz969+6NUgpHR0e8vLyIiopi4cKFBAUFodPpUEpRtmxZtm/fzuPHj+VLnhDii4n/MjdlyhRGjx7Nu3fvWLhwIePHj2fHjh0ArF69mubNm3PkyBFu3LhB1apVuXHjhrYamhCfCgoKIiAggB9++CHR9vjxFj+9qkCBAjg7O1OuXDkuXLjw2TRSIRKKi4tDKUXjxo3ZuXMnS5YsoX///jx79gyA3bt3U65cOZo3b86pU6cICQmhefPmDBo0iEKFCmFiYpKotYL4fgUHB3Po0CGGDx/O+PHjOXr0KOHh4RQqVIgOHTowcOBALly4gKmpqZbI1Ov1pE2b1siRCyHEv0Om730j4hNS165do1KlSgwaNIjJkycDH2/EDx48SPXq1fHw8KBu3bqkSJGC8ePHkyVLFpRS2Nvbc+3aNczMzIz8ToQQ37pPp+wFBASwe/dufvjhB86dO8esWbNYuHAhSilatmzJ4sWLiYqK4sKFC1hYWBATE4PBYJBFGESSAgICiI6O1qagfzoNT6/X8+zZM3bv3k2fPn2YOHEiNjY22g810oxafEoppV2zoqOjadKkCbt27aJZs2ba9Dxra2t27dpFq1atqFWrFoULFyY6OpotW7ZozyOVUuLmzZv06NEDpRQvX77kzZs3uLq6Ur16dVasWMGYMWN49eoVzZs3Z8aMGVhaWnLmzBlu3bpFo0aNjB2+EEL8K+TT8RthYmLC48ePqVmzJg0bNtQSUgCTJk2ie/fu3L9/n1q1auHu7s6KFSuYNGkST58+1W7AU6RIYazwhRDfiYQJqXPnznHt2jVevHih9YiqWLEiQ4cOJVOmTCxZskSrmFqxYgVly5Zlzpw5LFmyhHfv3hntPYjkLU+ePLx69QoPDw/gYxLq0yqoLVu2cP36dQBy5cqlVbFIQkokJX5crF69mmXLlhEeHs6PP/7Irl27WLx4MZMmTeL58+cYDAZ27drFmjVrGDZsGNevX5eqTqGJn8lQoUIFli9fzq1bt/Dz86Nt27Z4enrSrl07MmXKxOzZs2nWrBm9e/dmyJAhHDp0iGPHjmFra2vstyCEEP8OJb4ZDx8+VOXKlVONGzdWnp6eSimlpk6dqjJmzKgOHjyolFIqJiZGKaWUh4eH0ul0auDAgdo2IYT4Un7++WdlZWWl0qVLp8zMzNTKlSsT7T9//rxq2bKlKlKkiDp69Ki2vWXLlsrOzk6FhIR84YjF1+Ldu3eqUaNGKkeOHOrQoUNKKZXocy4iIkI1b95cjRs3zkgRiq9RTEyMqlu3ripdurRatWqVev/+vVJKqd27dyudTqf69++vAgMDkzxPCF9fX5UyZUo1adKkz/ZFRESoiRMnqsyZMysXFxdt+5MnT9SLFy/U69evv2SoQgjxxUmj82+Mn58fAwYMIEWKFFhbW7Nnzx42bNhAnTp1gP80eg0PD+fevXuYmppSuHBhI0cthPjWqQTTou7evUvTpk1ZuXIlL168YOvWrfz2229MnDiRdu3aaeecOXOGQ4cOMWHChETTr549e0bWrFm/+HsQX4+9e/cyYsQIoqOjmTlzJk2aNCEiIgI/Pz+GDh3Ky5cvOX/+vEwBFb8rqdUXIyIi6Ny5M/fv36dHjx60a9eOVKlSsWfPHlq1akWbNm2YN28e6dOnN1LUIjl6+/Ytjo6OPHv2DF9fXywsLLTxFV89HBMTQ/369QkODsbb21t6vAohviuSlPoG3b17l379+uHp6YmLiwu//PKLNnUhvveBm5sbfn5+WFhYGDlaIcT3ZPbs2Vy/fp0MGTIwe/ZsAHx9fVm0aBGnTp1izJgxiRJT8eKbU3/6JVGIhBImP7dv387cuXM5f/48ZcuW5e3bt1hZWWFqasrx48cxNTVNMvEgREL3798nT5482rTjiIgIOnbsiL+/P7169aJt27aYm5uzefNmFi9ezOnTp6V3lPjMjBkzcHd3J1++fEyZMgVra2stIRUdHY2pqSkHDx6kTZs2eHt7Y2trK9OJhRDfDfnU/AYVKFCApUuXUqVKFY4dO8aZM2fQ6XTodDrGjh3L7Nmz2bt3rySkhBBf1Lt373j06BHbtm3j3r172vZixYrRr18/qlWrxpQpU1i5cuVn5+r1ekkeCM3vrWKm0+m0fS1btmTDhg1s2bIFe3t7WrRowbBhwzh58qTW50fGlPjU4cOHtebkW7ZsoUGDBri7u2vjKmXKlKxduxYrKytcXFzYsmUL79+/1/oCySp7IqH4seDs7EzTpk25ffs2I0aMIDg4WBsr8VVRV65cIWfOnOTJk0cSUkKI74pUSn3D4qfyKaWYOnUqR44cYdy4cXh6emJnZ2fs8IQQ3ziVxEpm9+/fZ9WqVUybNg03NzecnJy0fTdu3MDFxQUTExM2bdr0haMVX4uEzfK3bNmCjY0NZcqUwdzcXDsmqbGXkFRIiaR4eXlRpUoV7OzsGDFiBHXq1KF+/frExcUxcuRI6tWrp409X19fHBwcyJIlCzNnzqRx48ayeqNIUsJr1vz589m2bRuFChXSKqZiY2OJjo6mX79+WFhYMGvWLAwGg4wlIcR3Q5opfMNsbW1ZsGABP//8M/Xq1SMkJIRz585JQkoI8a9LeBP+4sULIiIisLGxIV++fAwZMoTIyEj69++PTqejU6dOABQtWpSpU6eSK1cuY4Yukrn4ceXs7MyGDRsYMWIERYsWTZSU+vTL3KfJAklIiaS8fPkSgFSpUrFq1SrMzc3x8PCgYcOGuLi4oJSiQYMGAISEhNCiRQusra21bZJEEEmJr4gyMTFh4MCBAGzbto0RI0YwdepUrK2tGTt2rLbKnvSTEkJ8b6RS6jtw584dnJ2dmTJlCkWLFjV2OEKIb1zCBMD48ePZu3cvQUFB5MiRg19++YXGjRsTGRnJpEmTWLFiBYsWLaJDhw6JniNhUkuITy1dupTx48fj4eFBkSJFMDMzM3ZI4hvRsWNHAgICyJAhA8HBwYwePZoqVarQsGFDIiMjadmyJTVq1GDUqFEUKVKE6dOnA1J9J/4jJiYmyUUUkqqYKlWqFKampqxYsQJPT09Kly79pcMVQgijk6TUdyK+iaIQQnwpkydPZu7cucyZM4fMmTOzcuVK7t69S6dOnejfvz8hISHMmzeP6dOnc+DAARwdHY0dskiGnj59SrZs2RJt69atG6lSpWLBggXaFz2ZOiX+F5GRkZiZmbFx40ZOnTpF165dmTFjBkFBQYwdO5YffviB3r17c+rUKaKjo8mZMyenTp3C1NRUxp4AIDg4mDRp0mBubs6RI0dImzYtFSpUSHRMwsTUwoULmTdvHkFBQZw5c4YyZcoYI2whhDA6SUoJIYT4R8XFxfHmzRscHR3p2rUrPXr00PYNHjyYAwcOsG7dOuzt7Xn48CEeHh507949yV+WxffNycmJ9+/fs337dm1bZGQk5cuXx8HBgSVLlgD/qc6Ljo7m3r175M+fX36IEX/qxIkTPHjwgK5du2rbnj17Rrly5XBxccHR0ZG+ffvy/PlzRo4cSf369fH39+fZs2dUqFABExOT362KEd+Xly9f0r59e0qWLEnJkiXp0KEDe/fupWHDhp8dmzAxtXLlSqpXr06+fPm+dMhCCJFsyNwIIYQQ/7OQkBAeP34MfOyfYTAYCAsL06oHIiMjAZg7dy5p06Zl8eLFAOTJk4fevXtjMBiIiYkxTvAi2ZozZw4bN24E4M2bNwCYmZnx448/4unpyYULF4D/9PK5d+8ekydP5v79+0aJV3w9Tpw4Qc2aNenevTv16tXD1dUVX19fsmbNyqxZs9i9ezepU6fGxcVFa2bu5uZG7ty5qVixIiYmJsTGxkpCSgAf+5A5ODjw66+/0qVLF5YvX07Dhg2JjY397NiEKzR269ZNElJCiO+eJKWEEEL8T3bs2IGTkxPVq1fnxIkTAKRNm5aMGTOyc+dO4GMiISoqCoAyZcok2S9KvtyJhGJjY0mfPj0pUqRg1apVFC1aVEs2Va1aFQsLCxYsWMDZs2eBjxUuw4cPJyAgAFtbW2OGLr4CNjY2VKlSherVqxMZGcnNmzepVq0a8+fP59mzZ7x//54rV65QpEgRJk6cCICPjw8JJxhIDykBHys1U6VKRd26dQkODiZr1qw8fPiQDx8+oNfrfzcxJYQQ4iOZvieEEOK/5ubmhrOzMzNmzKBAgQJUrlxZm0rl7e2No6MjjRo1YvXq1dqUhUqVKmFvb8+cOXOMHb74CoSGhhIeHk7dunUxMTFh9+7d5M6dmx07drBixQouX75MxowZ0ev1pEiRgt9++w1TU1Npli/+1N27dxkxYgTR0dEMGDCA2NhYli9fzocPH/Dw8KBJkybs2LEDvV6Pv78/OXPmlP5l4nf5+fkRFBTEqVOn2L9/P5UrV2bSpEmkTJlSGuELIcQfkKSUEEKI/8rx48dp27Yts2bNSrR6XvwXtqioKPbu3Uvv3r3Jnj07OXPm5OXLl7x584Zr165JZZRI0rFjx/Dz86NXr1707dsXgMWLF/PixQvq1q1LVFQU+/fvJ3fu3Ny7d4/79+9z9epVcuXKRYsWLdDr9dLnR/xld+7cYdCgQcTFxTF//nxsbW25c+cOc+bMoX///pQsWTJREkqSnSJe/Ljw9/fHYDBgMBjIkiUL7969Y8aMGRw5coQffvgBFxcXzMzMWLVqFSVKlKBcuXLGDl0IIZIVSUoJIYT4W+JvxIcNG8a9e/dYt24dFhYWSR4bFxfH48ePtaqotGnTMm7cOK2HlCQOREKhoaH07duXu3fvkj59ejw9PTl37hzFihUDPq5uVa9ePaKioti3bx958uT57DmkIkH8XX5+fvTr1w+A0aNHU6VKFW2fJKHEH9m1axeDBg3C1NQUS0tLJkyYQKNGjQgLC2P69OkcO3YMa2tr8ubNy9y5c7l16xYFCxY0dthCCJGsSFJKCCHE3xKflCpfvjzly5dn0aJFn01niX8cHBxM5syZP3sOSRyI3/Ps2TPq1avH9evXGTNmDBMmTAD+M2aCg4NxdHQkLi6OrVu3UqBAASNHLL4Ffn5+DBgwAIBRo0ZRuXJlI0ckkqv4z7fAwEDKli3L+PHjsbCwwMvLixUrVrB582ZatmxJWFgYbm5uHD9+nNevX7Nw4UJKlixp7PCFECLZkaSUEEKI/0rdunVRSuHh4ZFkn5V3795Ro0YNJk2aRN26dY0YqfhaxMXFERgYyLBhwwgPD+f169e0a9eOXr16ARAdHY2pqSkvXrzAzs6OqlWrsn79eiNHLb4Vfn5+DB48mOfPn2tTrYRIyokTJ7h9+zaPHj1i2rRpwMdKzqlTpzJ//ny2bdtGixYtiImJQa/XEx4e/rsVxUII8b2TeRNCCCH+lvjkU4kSJVi/fj379u2jUaNG2hLp8RVQb9++xdramvTp0xs5YpGcJZweZWJigo2NDZs2beLBgwe4uLiwbt06AHr16oWpqSnwcTXHq1evkjZtWqPFLb49tra2zJw5k5UrV2pTRoX41Pv373Fzc2Pjxo04Ojpq2zNnzszIkSMBaN++PVFRUbRr1w5AElJCCPEHpFJKCCHEn0pqtamXL19SpkwZ0qdPz7Rp06hVqxYGgwGlFG/fvqVz585a7x/pySKSknBcrVixgvv375M2bVq6detG5syZuXnzJjNnzuTevXu0atWK/v37U7t2bcqWLcvUqVMBmQoq/j3ST0r8nmvXrrFgwQLWrVvHsWPHEvUhe/HiBSNHjmTnzp0EBARgYWEhqzUKIcQfkKSUEEKI3xUQEEDOnDk/2x6fCLhy5Qr169fHzMyMhg0b0qpVKy5cuIC7uzsvXrzg0qVLmJqaypc78ZmEY2LYsGG4ublRpEgRXr58SVRUFCdPniR79uzcvHmThQsXcvDgQfR6Pebm5vj4+GhVU0II8W+KT56/e/eODx8+aH0SAwMDGTx4MMeOHWPPnj1UqVJFO/bly5fExsZibW1t5OiFECL5k6SUEEKIJPXs2ZMXL14wceLEJKeyxN98P3nyhL59++Lj40NgYCB2dnaUKVOGRYsWySp74k+9fPmSUaNG0adPH4oXL861a9cYPHgwt27d4uLFi+TIkYOAgAAePnzIgwcP6NixI3q9XsaVEOJfF/85t3fvXubMmYO/vz9Fixaldu3a9OnTh8DAQMaMGYOHhwe//vorDg4Oxg5ZCCG+OvKztRBCiCTVrVuXy5cvM2/ePHx9fT/br9PpiI2NJUeOHGzfvp3bt29z8+ZNPD09cXV1xWAwEBsbK4kD8bvWrFlDwYIFuXnzJpkyZcLExIRSpUqxZMkSChcuTPny5QkMDCRnzpxUrVqVzp07o9frZVwJIb4InU7HwYMHadu2LY6Ojuzfvx8rKysmTpzIqVOnyJMnD2PGjKFBgwZUqVKF8+fPGztkIYT46sgdnRBCiM8opWjWrBnm5ub07NkTpRSDBw/+rGJKr9ejlCJFihSkSJGCAgUKaL0zlFLS60f8oaxZs1KiRAmuXLlCihQpgI/jpnDhwixdupR+/fphY2NDcHAwGTNm1M6TcSWE+LfFxcURERHBypUrGTp0KMOGDePt27ecOnWKn376idq1awNQsGBBRo0ahZmZmSzsIYQQ/wWZvieEEOIzCRtQu7u706tXL2rXrp1kYkqIvyKpZvkxMTF4enoyYMAAdDodXl5epE6dWtt//fp1li9fzrx58yQRJYT4IuKvVfG9E+vWrcvw4cMpUKAA5cuXp2HDhixbtgyAffv2kS1bNuzs7IiOjpZed0II8V+Q6XtCCCE08b9TJEwe1K9fnyVLlnDkyBHmzp2b5FQ+If5IXFycNqbu3LmDv78/jx49wmAwULlyZebPn0+KFCmoWrUqYWFh2nnFixdn4cKF2pQ9IYT4t+l0OtavX0+HDh14+/YtUVFRrF+/nmrVqtGwYUMWL14MwKtXr9iwYQOXL19GKSUJKSGE+C9JUkoIIQSQOHHw5MkTbt++rW2PvxGXxJT4u5RS2ip7EyZMoEWLFtSsWZP69euzZ88eDAYDVapUYcaMGej1emrUqEFoaOhnzyOVUkKIf1P8jzKvX79mypQplClTBktLS8aOHcvevXtJnTo1y5Yt0/rZzZ07Fx8fH2rVqvVZFagQQoi/TqbvCSGEIC4uTkscjBs3jr1793Lv3j2qVKlC+/btadmyJSlSpGDfvn30799fW3modOnSRo5cfC3GjRuHq6sra9aswcbGhvHjx7Nr1y7WrVvHTz/9pE3lc3JyokaNGri5uRk7ZCHEd+bYsWMcPHiQ0NBQ5s6di4WFBeHh4SxfvpwhQ4bQrFkzMmbMSFhYGPv27eP48ePyOSiEEP8jqZQSQgihJaTGjx/PsmXLGD16NLdu3SIkJIRZs2bh6upKZGQkjRo1YtGiRaxbtw4PDw8jRy2+FhcuXODEiRNs2bIFR0dH/P39OXbsGDVr1qRjx45s3LhRm8q3a9cuVqxYYeyQhRDfmaioKE6ePMn8+fM5e/YsFhYWAKRKlYru3btz6NAhwsPDefr0KenTp+fcuXOSkBJCiH+AVEoJIYQAPiYOevXqxcyZM6lZsyanT5/G0dGRIkWKEBYWxoABA+jSpQtmZmZ4eXlhb28vU6rEX/Lo0SM2bdrEsGHDOHHiBD/99BNjx46lXbt2NGrUCC8vL5YuXUqPHj20c+KbDAshxL8p4SIM/v7+rF+/nnHjxjF//nz69+8P/Kea+NP/CiGE+N9JUkoIIb5T8TfV8TfkT5484ciRI/z000+cOXOG1q1bM336dLp06ULhwoUxNTWldevWDB06lBQpUgCSOBCf+70va2/fvsXS0pIOHTqQLl06bUW9zp07c/HiRdKlS8fp06elN4sQ4ouI/+yLiYnR+kQBPH78mKVLlzJ//nzmzJlDz549gY+rher1enQ6XZKriQohhPjvGP78ECGEEN+i+MTBo0ePyJ07N9bW1vz444/o9XpcXV3p2rUrnTp1AqBw4cJcvnyZ58+fJ1phSBJSIqGECakzZ87w/v17ypYtS/r06bG0tOTt27f4+PjQqlUr9Ho9Hz58IDQ0lNmzZ1O7dm35sieE+CLirzPHjx9n/fr1REVFYWNjw7Rp07CxsaFPnz6YmJgwdOhQTExM6N69e6LElVyjhBDinyNJKSGE+I4dPHiQBg0a4OHhQZ06dbCysiImJobg4GBy586tJZ0sLCxYvny5tsqQJA5EUuITUs7Ozqxbt453795RvHhxevToQfv27bG0tKRevXpMmTKFt2/f4uXlRXR0NDVr1pRxJYT4YnQ6Hbt378bJyYnWrVtjbW3N1q1buXXrFrt27SJHjhz06tULvV5Pz549MTU1xcnJydhhCyHEN0mm7wkhxHcsMDCQsWPHsnHjRvbv30+tWrV4//49Xbp04enTp5QoUYKbN2/y8uVLrl69Kr00RJLik0lKKa5fv063bt1YuHAhGTJkYPTo0Tx69IjWrVvTp08fwsLCmDFjBt7e3tjY2LBixQpMTU1lKqgQ4ovx8fGhTZs2DBo0iN69e+Pv70+lSpUICgrCwcGBEydOYDAYePToERs2bKBFixYULFjQ2GELIcQ3SZJSQgjxnfi0CiX+8bNnzxg7diyrV6/G3d2dOnXq8OTJE4YNG0ZoaCipUqViw4YNmJqaSkJKfCbhmIiOjubx48dMmzYNV1dXTExMCA8Pp1+/fty4cYMOHTrQo0cPUqRIQVhYGKlTpwb4rKeLEEL8k968ecPr169RSpEvXz48PT3ZunUrCxcu5PHjx1SrVo2aNWvSpk0bmjRpQq1atdi2bRumpqZyfRJCiH+ZJKWEEOI7M3/+fCpWrEj58uW1xFRQUBBjx47Fzc0NDw8PatWqRUREBGZmZloiS27MxR9xcXHB3d2dN2/ekC1bNo4dO6bti09M3b59mwYNGiRqli9T9oQQ/yZfX1+tGspgMNCsWTNmz57N/fv3yZUrFy1btsTCwoL169cTHh5OtWrVuHTpErVr1+bQoUPGDl8IIb558nO3EEJ8R0JCQti3bx/169fHx8dHm3KVJUsWRo4cSdGiRWnUqBHu7u6kTJlSSxYopSQhJRKJi4vT/l6zZg2zZs2iadOm5MiRg5s3b+Ls7Ex0dDQAqVKlYtGiRVhbW/Po0aNEzfIlISWE+LdcvXqVihUrUqJECWbMmEHVqlXZtGkT48aNI1++fISGhhIQEECLFi3Q6XQYDAZKlSrF/v37cXV1NXb4QgjxXZBKKSGE+IbduHGDXLlykTp1ambOnEmzZs348OEDLi4unD59Gnd3d0qXLq0d3759e7y8vLCxseHMmTNGjFx8Lfbv38+1a9coWLAgzZs318bXiRMnqFatGi4uLlpCMzIyElNTU0xMTKRCSgjxr7p37x7Fixdn6NChTJw4EYAPHz7QoEEDoqKiOHPmDBERERQrVgw7OztmzJjB4sWL2bt3L6dOnSJLlixGfgdCCPF9kEopIYT4RsU3cnV1daVfv34MGzYMpRTFihVjzJgxODg4UL9+fa5fvw58TBgopVixYgWnT582cvTia3Dx4kV+/vlnpk+fTsqUKQEwNzdn+PDhVK9enZMnTzJu3DhiYmIAMDMz05rlS0JKCPFviYuLw83NjTRp0pAxY0Ztu7m5OdWrVycmJoa3b99ibm7OnDlzOH36ND/88ANbt25ly5YtkpASQogvSCqlhBDiG+bs7My6desICwvj0KFDODg4aPtu3LjBhAkT2LVrF61ateL27dvo9XrOnz+PXq+XpubiM59WN71584a1a9cye/ZsypQpw549e7R97969Y/r06WzevJmhQ4fSq1cvI0QshPhePX36lBkzZnD+/HkaN27MyJEjefnyJXny5GHMmDE4OztrxwYHB3Pv3j3y5s0rCSkhhPjCJCklhBDfmPgqFJ1Ox+bNmxkwYACZMmWic+fOdOvWDSsrK+3YV69esXbtWry9vcmUKROzZ8/G1NSU2NhY9Hq9Ed+FSG4+TVK+f/8eCwsLPnz4wNq1a1mwYAGVKlVi5cqV2jGhoaFs3ryZbt26yXgSQnxxQUFBTJ48mcuXL+Pg4MDmzZtp2rQpCxYsAD6/rgkhhPjyJCklhBDfqLNnz5InTx5iY2NZuHAhR48epWnTpvTr14906dIlOjYqKkpbDU1W2ROfSvjFbfbs2Vy6dInLly/TrVs36tevT8GCBVm6dCkrV66kXLlyrFix4rPnkESnEMIYnj17xpQpU9i5cyfZs2fH29sbkM86IYRILuSnASGE+AZ5enri6OiITqcjR44cTJ8+napVq7Jnzx6WLl1KaGgoAH369OHBgwdaQgqQm3TxmfiE1IgRI5g5cyblypXj559/ZvLkyYwZM4aIiAg6depEt27duHTpEi1atPjsOSQhJYQwhqxZszJ69GhatGiBXq9n+vTpwMfPuoSriAohhDAO+eYhhBDfgE97/eh0OpRSREREaNvmzJnDL7/8wq5du7hw4QLv3r3j6tWr2jQGIf7IxYsX2bVrF3v27MHe3p6LFy/y7t07mjRpgoWFBQBdunQhLCyMW7duybQYIUSyYW1tzciRI5k8eTL79u0jPDycCRMmyDVKCCGSAbkSCyHEN+DTlcwcHBzIly8fN27cANCSU7Nnz6Z169ZkyJCBrFmz8vTpUwwGA7GxsV88ZpG8RUZGJnqslMLS0hJ7e3u2bdtG9erVWbhwIR07diQsLIwjR46QKlUqBgwYwJo1a7RV9oQQIjnIkiULo0aNwtbWlrNnz/Lq1StjhySEEAKplBJCiG/GhAkT8PX1JXPmzNjZ2REYGMitW7do0KABKVOm1I4bMmRIovOkr4b41OHDh7l69SpVq1alfPnywMe+Y0+fPmXlypUMGTKE6dOn07t3bwB+++03li5dio2NDYUKFQI+JrGkCkEIkZxkyZKFadOmAZAhQwYjRyOEEAIkKSWEEN+EDx8+aNVPPj4+PHz4kJcvX+Ls7MzJkyeJjY2lTp06xMTE0LdvX1KlSgV8TBxIQkoktHr1asaMGUPjxo2pVq2att3BwYEffviBHj16MH78ePr06QN8rKiaN28eZmZmFChQQDv+0+o9IYRIDqytrY0dghBCiARk9T0hhPgK/Vm/ntjYWMaPH8+pU6eoXbs2AQEBPHr0iPfv33PmzBmpYBFJ2rJlC127dmX16tXUq1ePtGnTJtp/7tw5Ro0ahb+/P+PHjyckJAR3d3cCAwPx8fHB1NRUekkJIYQQQoi/TJJSQgjxlUn4pX/t2rXcuXOH58+f4+TkhL29PaampgDMmzePdevWcfnyZeBjoip+BTRJHIhPvXjxglatWtGiRQv69u2rbQ8LC+PGjRsYDAbs7Oy4e/cu06ZN4+jRo+TPn5+8efPi6uqKwWCQqaBCCCGEEOJvkTtHIYT4ysQnk5ydndmyZQvVq1cnZcqUVK1alUWLFtG1a1fMzMyoV68eCxYsICAgABsbGy0hJb1+xO8JDg4me/bs2uOlS5dy/Phxdu7cibW1NUWKFOHYsWO4ubnx8uVLMmbMqB0rCSkhhBBCCPF3ybcSIYT4Cu3fv5/NmzezZ88e1q5dS8eOHQFInz49ZmZmAKRLl46AgAAePnyYqL+P9PoRvyc0NJQDBw5w/PhxWrRowdKlS8mUKROHDh1i4cKFPHz4EBcXF+Dj+IonvcmEEEIIIcR/Q+4ghRDiK/DpdLsXL15QqVIlypQpw7Zt2+jatStLliyhTZs2vHnzhtDQUKytrRk0aBCVK1c2YuTia5EpUybWrFlD8+bNOX78OGnSpGHevHmULFmSDBkyEBISgqWlJbGxsQCJklCS6BRCCCGEEP8NSUoJIUQyFx0drfWJCg0NJW3atLx//56nT5+ya9cuunfvzowZM+jVqxcAu3fv5tixY6xevZpZs2YBiftJCfF7atasiZ+fH2FhYeTJk+ez/WnSpCFbtmxGiEwIIYQQQnyLZPqeEEIkY0ePHmXRokUA9OrVi3bt2hEbG0u1atVQStG6dWvGjh1L7969AXj//j179uzBzMwsUSWLJKTEX5UpU6bPElIvXrygQ4cOREVF0bVrVyNFJoQQQgghvjVSKSWEEMlUdHQ0a9eu5fbt2xw4cAAfHx/OnDmDXq+naNGiODo6EhISwoMHD7h69SrBwcHMnTuXp0+fsnPnTnQ6HUopmVol/msvX75k5cqVeHp6EhwcjJeXF3q9XirvhBBCCCHEP0KSUkIIkUyZmpqyfv16ypYty/Hjx3F2dqZIkSLAxx4+I0aMIDY2liNHjmBnZ4ednR2ZMmXC29sbg8EgiQPxP3vy5AleXl7kz5+fPXv2YDAYZJU9IYQQQgjxj9EppZSxgxBCCPG5qKgo3rx5wy+//MK7d+94+fIlLVq0oHfv3toKewCRkZHcvHmTHDlykDFjRnQ6nSQOxD/mzZs3WFpaotPpJNEphBBCCCH+UZKUEkKIZOTTVfbixcbG0qVLF+7cuUObNm0SJaZCQkKwsrL60+cQ4n8hU0GFEEIIIcQ/TZJSQgiRDC1dupSLFy+SK1cuateuTcWKFQkLC6Nfv374+fnRuHFjunbtSqtWrciePTvr1683dshCCCGEEEII8bdIUkoIIZKBhNVNo0ePxtXVlUqVKvH06VMiIiKYNm0aDRs2JCwsjF9++QUvLy9CQ0PJlCkT586dI0WKFEZ+B0IIIYQQQgjx90jDESGESAbiE1K3bt0iPDwcd3d3ypcvz+XLl1m0aBF9+/ZFKUWjRo2YN28e3t7evH79mkaNGqHX66WHlBBCCCGEEOKrI5VSQgiRTOzevZv+/fuTPn16Dh48SPbs2QHw9fVl7ty5HDt2jIULF9KoUaNE50nzaSGEEEIIIcTXSDrhCiFEMmFubk7ZsmW5f/8+jx8/1rYXK1aMwYMHU7t2bVq0aMG5c+cSnScJKSGEEEIIIcTXSOZ6CCGEESS1Ql69evVInTo14eHh9OjRgxUrVlChQgXgY2KqT58+5M2bl/LlyxsjZCGEEEIIIYT4R8n0PSGE+MISJqSOHDlCeHg4ERERtG7dGoCzZ88yY8YMAgICcHV1TTIJJVP2hBBCCCGEEF87SUoJIYSRDBkyhE2bNpE2bVqePn1K0aJFmTlzJpUrV8bT05M5c+bw+PFj5s6dS+XKlY0drhBCCCGEEEL8o6SnlBBCGMHq1atZt24dBw4c4NSpU9y9e5e4uDgGDhzI9evXqVy5Mv369SNVqlS4ubkZO1whhBBCCCGE+MdJpZQQQnxBSil0Oh0jR47kxo0b/Prrr8TExGAwGIiIiMDOzo68efOyb98+AHx8fChZsuRn/aeEEEIIIYQQ4msn33KEEOJfdvXqVX799Ve8vLzQ6XQABAUF8erVKwAMBgMfPnwgZcqUzJo1C29vb+7fvw9A6dKlMTExIS4uzmjxCyGEEEIIIcS/QZJSQgjxL9q4cSNOTk64ublx4MABbXvnzp25evUq8+bNA8Dc3ByA6OhoMmbMSJo0aRI9j1RKCSGEEEIIIb41BmMHIIQQ36p169bRq1cv3NzcqFevHunSpdP2lShRgp9//pkFCxYQERFB7969efPmDStWrCBHjhxkypTJeIELIYQQQgghxBcgPaWEEOJfcOPGDVq3bs2gQYPo1q2btj2+pxRAQEAAW7ZsYdKkSaRKlYo0adJgZWWFl5cXpqamxMXFSYWUEEIIIYQQ4pslSSkhhPgXHD58mF69euHh4YGtra2WiIqXMDkVGBjIxYsXsbS0pEqVKuj1eq35uRBCCCGEEEJ8q+QbjxBC/AsuXbrEu3fvKFCgAJA4CQWg0+m4desWz58/p1q1amTPnl3bFxsbKwkpIYQQQgghxDdP5oUIIcS/IH/+/Lx//57Dhw8DfFYpBR97Tm3atIlPC1b1ev0XiVEIIYQQQgghjEmSUkII8S+ws7MjRYoULF++nICAAG17fAIqNDQUPz8/ihcvnmTCSgghhBBCCCG+dZKUEkKIf0HevHlxdXVl//79jBgxAh8fH+BjxdTTp09p06YNQUFB9O7d28iRCiGEEEIIIYRxSKNzIYT4l8TGxrJ69Wr69OmDtbU1xYoVIy4ujrdv3xIXF6etshcbGytT9oQQQgghhBDfHUlKCSHEv+zKlSu4ublx584dbGxsKF26NL169ZJV9oQQQgghhBDfNUlKCSGEkUiFlBBCCCGEEOJ7JkkpIYT4ApRS0tBcCCGEEEIIIRKQRudCCPEFSEJKCCGEEEIIIRKTpJQQQgghhBBCCCGE+OIkKSWEEEIIIYQQQgghvjhJSgkhhBBCCCGEEEKIL06SUkIIIYQQQgghhBDii5OklBBCCCGEEEIIIYT44iQpJYQQQgghhBBCCCG+OElKCSGEEEIIIYQQQogvTpJSQgghhBB/g06nY8+ePV/0NatVq8agQYO+6GsmZfz48ZQqVeoff96TJ0+i0+l48+bNP/7cQgghhEi+JCklhBBCiK+Wk5MTOp0OnU6HqakpefLkwdnZmYiIiL/8HH83IfLs2TMcHR3/y4j/HWvWrEGn01G4cOHP9m3fvh2dTkfu3Ln/1nMaI/kmhBBCiO+LJKWEEEII8VWrV68ez54948GDB8ydO5dly5Yxbty4f/x1oqKiAMiSJQtmZmb/+PP/rywsLAgODubcuXOJtq9atYqcOXMaKSohhBBCiN8nSSkhhBBCfNXMzMzIkiULNjY2/Pjjj9SqVYsjR45o++Pi4pg6dSp58uTB3NyckiVLsmPHDgD8/f2pXr06AFZWVuh0OpycnICPU+b69evHoEGDyJgxI3Xr1gU+ryB6/PgxrVq1Il26dKRPn54mTZrg7+8PwOHDh0mZMuVnVVgDBw6kRo0aALx69Yq2bduSPXt2UqVKRfHixdm8efPf/ncwGAy0a9cONzc3bduTJ084efIk7dq1++z4X3/9lTJlypAyZUry5s3LhAkTiImJAdCqqpo2bZpkldX69evJnTs3lpaWtGnThnfv3mn7IiMjGTBgAJkzZyZlypRUrlwZb2/vROe7u7tToEABzM3NqV69uvbvJYQQQojviySlhBBCCPHN8PX15ezZs6RIkULbNnXqVNatW4erqys3btxg8ODB/PTTT5w6dQobGxt27twJwJ07d3j27Bnz58/Xzl27di0pUqTAy8sLV1fXz14vOjqaunXrkiZNGs6cOYOXlxepU6emXr16REVFUbNmTdKlS6e9BkBsbCxbt26lffv2AERERGBnZ8eBAwfw9fWlR48edOjQgQsXLvzt99+lSxe2bdtGeHg48HFaX7169bC2tk503JkzZ+jYsSMDBw7k5s2bLFu2jDVr1jB58mQALYm0evVqnj17liipdP/+ffbs2cP+/fvZv38/p06dYtq0adp+Z2dndu7cydq1a7l8+TL58+enbt26vH79GviYxGvWrBmNGjXiypUrdOvWjeHDh//t9yqEEEKIb4ASQgghhPhKderUSen1emVhYaHMzMwUoExMTNSOHTuUUkpFRESoVKlSqbNnzyY6r2vXrqpt27ZKKaVOnDihABUSEpLomKpVq6rSpUt/9pqA2r17t1JKqfXr16uCBQuquLg4bX9kZKQyNzdXhw4dUkopNXDgQFWjRg1t/6FDh5SZmdlnr5dQgwYN1C+//JIoloEDB/7u8atXr1aWlpZKKaVKlSql1q5dq+Li4lS+fPnUr7/+qubOnaty5cqlHV+zZk01ZcqURM+xfv16lTVr1iTfZ7xx48apVKlSqdDQUG3b0KFDVYUKFZRSSoWFhSlTU1O1ceNGbX9UVJTKli2bmjFjhlJKqREjRqgiRYoket5hw4Yl+f9ACCGEEN82g1EzYkIIIYQQ/6Pq1auzdOlS3r9/z9y5czEYDDRv3hyAe/fuER4eTu3atROdExUVRenSpf/0ue3s7P5w/9WrV7l37x5p0qRJtD0iIoL79+8D0L59e+zt7Xn69CnZsmVj48aNNGjQgHTp0gEfK6emTJnCtm3bCAwMJCoqisjISFKlSvVX/wkS6dKlC6tXryZnzpy8f/+e+vXrs2jRos/i9vLy0iqj4uOIiIggPDz8D187d+7cid5v1qxZCQ4OBj5WUUVHR+Pg4KDtNzU1pXz58ty6dQuAW7duUaFChUTPWbFixf/qvQohhBDi6yZJKSGEEEJ81SwsLMifPz8Abm5ulCxZklWrVtG1a1fCwsIAOHDgANmzZ0903l9pVm5hYfGH+8PCwrCzs2Pjxo2f7cuUKRMA5cqVI1++fGzZsoXevXuze/du1qxZox03c+ZM5s+fz7x58yhevDgWFhYMGjRIa6z+d7Vv3x5nZ2fGjx9Phw4dMBg+v90LCwtjwoQJNGvW7LN9KVOm/MPnNzU1TfRYp9MRFxf3X8UqhBBCiO+bJKWEEEII8c0wMTFh5MiR/Pzzz7Rr144iRYpgZmZGQEAAVatWTfKc+P5TsbGxf/v1ypQpw9atW8mcOTNp06b93ePat2/Pxo0byZEjByYmJjRo0EDb5+XlRZMmTfjpp5+Aj43Z7969S5EiRf52PADp06encePGbNu2Lck+WPFx37lzR0vmJcXU1PRv/5vky5dP68GVK1cu4GPfLW9vbwYNGgRA4cKF2bt3b6Lzzp8//7deRwghhBDfBml0LoQQQohvSsuWLdHr9SxevJg0adIwZMgQBg8ezNq1a7l//z6XL19m4cKFrF27FoBcuXKh0+nYv38/L1680Kqr/or27duTMWNGmjRpwpkzZ3j48CEnT55kwIABPHnyJNFxly9fZvLkybRo0SJRlZatrS1Hjhzh7Nmz3Lp1i549e/L8+fP/6d9gzZo1vHz5kkKFCiW5f+zYsaxbt44JEyZw48YNbt26xZYtWxg9erR2TO7cuTl27BhBQUGEhIT8pde1sLCgd+/eDB06FA8PD27evEn37t0JDw+na9euAPTq1Qs/Pz+GDh3KnTt32LRpU6LKMSGEEEJ8PyQpJYQQQohvisFgoF+/fsyYMYP379/j4uLCmDFjmDp1KoULF6ZevXocOHCAPHnyAJA9e3YmTJjA8OHDsba2pl+/fn/5tVKlSsXp06fJmTMnzZo1o3DhwnTt2pWIiIhElVP58+enfPnyXLt2TVt1L97o0aMpU6YMdevWpVq1amTJkoUff/zxf/o3MDc3J0OGDL+7v27duuzfv5/Dhw9Trlw57O3tmTt3rlbdBDB79myOHDmCjY3NX+q/FW/atGk0b96cDh06UKZMGe7du8ehQ4ewsrICIGfOnOzcuZM9e/ZQsmRJXF1dmTJlyn//ZoUQQgjx1dIppZSxgxBCCCGEEEIIIYQQ3xeplBJCCCGEEEIIIYQQX5wkpYQQQgghhBBCCCHEFydJKSGEEEIIIYQQQgjxxUlSSgghhBBCCCGEEEJ8cZKUEkIIIYQQQgghhBBfnCSlhBBCCCGEEEIIIcQXJ0kpIYQQQgghhBBCCPHFSVJKCCGEEEIIIYQQQnxxkpQSQgghhBBCCCGEEF+cJKWEEEIIIYQQQgghxBcnSSkhhBBCCCGEEEII8cVJUkoIIYQQQgghhBBCfHH/B4edOl9BVHODAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with the results\n",
        "data = {\n",
        "    'Retrieval Method': ['Naive', 'BM25', 'Contextual Compression', 'Multi-Query', \n",
        "                         'Parent Document', 'Ensemble', 'Semantic Chunking'],\n",
        "    'Context Recall': [0.8300, 0.3000, 0.8300, 0.8500, 0.4800, 0.9667, 0.8667],\n",
        "    'Context Entity Recall': [0.7217, 0.4983, 0.6517, 0.6229, 0.5150, 0.7333, 0.7125],\n",
        "    'Noise Sensitivity': [0.3825, 0.1823, 0.3025, 0.4537, 0.2849, 0.0, 0.2533],  # Using 0 for N/A\n",
        "    'Context Precision': [0.7558, 0.1417, 0.8083, 0.7140, 0.7000, 0.6974, 0.6427]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate a combined score (average of all metrics)\n",
        "df['Combined Score'] = df[['Context Recall', 'Context Entity Recall', 'Context Precision']].mean(axis=1)\n",
        "\n",
        "# Sort by combined score for the chart\n",
        "df_sorted = df.sort_values('Combined Score', ascending=False)\n",
        "\n",
        "# Create a bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "x = np.arange(len(df_sorted['Retrieval Method']))\n",
        "width = 0.2\n",
        "\n",
        "# Plot each metric\n",
        "ax.bar(x - width*1.5, df_sorted['Context Recall'], width, label='Context Recall')\n",
        "ax.bar(x - width/2, df_sorted['Context Entity Recall'], width, label='Context Entity Recall')\n",
        "ax.bar(x + width/2, df_sorted['Context Precision'], width, label='Context Precision')\n",
        "ax.bar(x + width*1.5, df_sorted['Combined Score'], width, label='Combined Score')\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Retrieval Method')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Comparison of Retrieval Methods Using RAGAS Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(df_sorted['Retrieval Method'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Findings and Conclusions on retriever performance\n",
        "\n",
        "1. Best Overall Performance: The Ensemble Retriever demonstrates the highest overall performance with exceptional context recall (0.9667) and strong entity recall (0.7333). This suggests that combining multiple retrieval strategies provides the most comprehensive results.\n",
        "\n",
        "2. Best Precision: The Contextual Compression Retriever (using Cohere's reranker) achieves the highest context precision (0.8083), indicating it's most effective at retrieving highly relevant documents while minimizing irrelevant information.  \n",
        "\n",
        "3. Worst Performance: The BM25 Retriever significantly underperforms across all metrics, particularly in context precision (0.1417) and recall (0.3000). This suggests that keyword-based retrieval alone is insufficient for complex queries about movie reviews.\n",
        "\n",
        "4. Balanced Performance: The Naive Retriever shows surprisingly strong and balanced performance across all metrics, making it a good baseline option when computational resources are limited.\n",
        "\n",
        "5. Specialized Strengths:\n",
        "    - Multi-Query Retriever excels in noise sensitivity (0.4537), suggesting it's most robust to variations in query formulation.\n",
        "    - Semantic Chunking Retriever performs well on context recall (0.8667) and entity recall (0.7125), indicating it effectively captures semantic relationships.\n",
        "\n",
        "6. Efficiency vs. Effectiveness Tradeoff: While the Ensemble Retriever performs best, it also experienced timeout issues, suggesting higher computational demands. The Contextual Compression Retriever offers an excellent balance of high performance with fewer computational issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Recommendations\n",
        "\n",
        "1. For Maximum Accuracy: Use the Ensemble Retriever when comprehensive information retrieval is critical and computational resources are available.\n",
        "\n",
        "2. For Balanced Performance: The Contextual Compression Retriever offers the best balance of precision and recall with reasonable computational demands.\n",
        "\n",
        "3. For Resource-Constrained Environments: The Naive Retriever provides surprisingly good results with minimal complexity.\n",
        "\n",
        "4. Avoid: The BM25 Retriever alone is not recommended for this type of content unless specifically needed for keyword-exact matching.\n",
        "\n",
        "5. For Production Systems: Consider implementing a tiered approach - start with the Naive or Contextual Compression retriever for most queries, and fall back to the Ensemble retriever for complex or high-importance queries.\n",
        "\n",
        "This analysis demonstrates that the choice of retrieval method significantly impacts RAG system performance, with modern neural approaches substantially outperforming traditional keyword-based methods for this particular dataset and task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - SDG - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"JohnWick LangSmith Dataset!\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"JohnWick LangSmith Dataset!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangSmith Evaluation Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
        "\n",
        "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"helpfulness\": (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \" taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"output\"],\n",
        "        \"reference\": example.outputs[\"answer\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "coherence_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"coherence\": {\n",
        "                \"Is the response coherent with the retrieved data or the query?\"  \n",
        "                \"Is the response easy to read and logically structured and well-written?\"            \n",
        "            }          \n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith Naive Retrieval Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'excellent-house-11' at:\n",
            "https://smith.langchain.com/o/b2bbbf41-ce3b-46fa-8cc0-7de9bb768aa9/datasets/6358e678-d29a-445d-a5e4-feefef89eb02/compare?selectedSessions=4f1b0b8c-fb53-4da8-acd1-19c75f42a577\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6eb5328e47324dbb82d560739ae7ffab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 575ab0d5-c4d0-4446-8087-7ad400a3f5fa: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 575ab0d5-c4d0-4446-8087-7ad400a3f5fa: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 575ab0d5-c4d0-4446-8087-7ad400a3f5fa: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 67d662b9-c5e8-442e-9f98-240276e566df: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 67d662b9-c5e8-442e-9f98-240276e566df: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 67d662b9-c5e8-442e-9f98-240276e566df: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 14c98fab-43de-4d87-bb02-6a9b86ce2c5a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 14c98fab-43de-4d87-bb02-6a9b86ce2c5a: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 14c98fab-43de-4d87-bb02-6a9b86ce2c5a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 218011ff-0454-4b99-b45c-630400a22a14: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 218011ff-0454-4b99-b45c-630400a22a14: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 218011ff-0454-4b99-b45c-630400a22a14: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 79279be4-a8fa-4cc7-ad4e-a5f3472ecaed: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 79279be4-a8fa-4cc7-ad4e-a5f3472ecaed: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 79279be4-a8fa-4cc7-ad4e-a5f3472ecaed: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8d7f18ac-102d-4fe0-81c3-74ad1fdf901e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8d7f18ac-102d-4fe0-81c3-74ad1fdf901e: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8d7f18ac-102d-4fe0-81c3-74ad1fdf901e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d5046214-5771-49f5-84d4-95d5d8db2ef0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d5046214-5771-49f5-84d4-95d5d8db2ef0: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d5046214-5771-49f5-84d4-95d5d8db2ef0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3555f97f-ddca-41ba-9164-f380236a7664: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3555f97f-ddca-41ba-9164-f380236a7664: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3555f97f-ddca-41ba-9164-f380236a7664: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run de1b4062-9d8f-4bbe-aca6-861fa82519af: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run de1b4062-9d8f-4bbe-aca6-861fa82519af: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run de1b4062-9d8f-4bbe-aca6-861fa82519af: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c28978ef-ef46-43ce-bd28-76e063bf7e8d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c28978ef-ef46-43ce-bd28-76e063bf7e8d: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c28978ef-ef46-43ce-bd28-76e063bf7e8d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.response</th>\n",
              "      <th>outputs.context</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.wrapper</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does Ian McShane's character influence Joh...</td>\n",
              "      <td>content=\"Ian McShane's character plays a signi...</td>\n",
              "      <td>[page_content=': 9\\nReview: At first glance, J...</td>\n",
              "      <td>None</td>\n",
              "      <td>Ian McShane plays Winston, the owner of the Co...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.837578</td>\n",
              "      <td>26fe8b70-0d0d-46e4-bdc4-572214c82dcc</td>\n",
              "      <td>575ab0d5-c4d0-4446-8087-7ad400a3f5fa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is 'John Wick: Chapter 4' considered the best ...</td>\n",
              "      <td>content='According to the reviews, \"John Wick:...</td>\n",
              "      <td>[page_content=': 20\\nReview: In a world where ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The reviews for 'John Wick: Chapter 4' are mix...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.184345</td>\n",
              "      <td>0e3f986b-1a80-413e-bce8-8a6c10e3105a</td>\n",
              "      <td>67d662b9-c5e8-442e-9f98-240276e566df</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How did the film 'Parabellum' contribute to th...</td>\n",
              "      <td>content=\"I don't have specific information abo...</td>\n",
              "      <td>[page_content=': 8\\nReview: In this 2nd instal...</td>\n",
              "      <td>None</td>\n",
              "      <td>The film 'Parabellum' contributed to the perce...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.836779</td>\n",
              "      <td>5f10c1bb-450d-4277-a1c1-ed8013c937a0</td>\n",
              "      <td>14c98fab-43de-4d87-bb02-6a9b86ce2c5a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How does Parabellum expand the world of John W...</td>\n",
              "      <td>content='\"John Wick: Chapter 3 - Parabellum\" e...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>Parabellum expands the world of John Wick by i...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.593456</td>\n",
              "      <td>d6bb98ee-f4a8-477d-8590-581710ae4bba</td>\n",
              "      <td>218011ff-0454-4b99-b45c-630400a22a14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What John Wick: Chapter 3 - Parabellum about a...</td>\n",
              "      <td>content='\"John Wick: Chapter 3 - Parabellum\" i...</td>\n",
              "      <td>[page_content=': 24\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum is about the...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.774633</td>\n",
              "      <td>78e03c53-c06e-496c-982a-c594ef6c4c16</td>\n",
              "      <td>79279be4-a8fa-4cc7-ad4e-a5f3472ecaed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What motivates John Wick to come out of retire...</td>\n",
              "      <td>content='John Wick comes out of retirement in ...</td>\n",
              "      <td>[page_content=': 5\\nReview: Ultra-violent firs...</td>\n",
              "      <td>None</td>\n",
              "      <td>In the original John Wick (2014), the ex-hit-m...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.179548</td>\n",
              "      <td>ea1ded1c-f812-4bb0-ba2e-f576198c011d</td>\n",
              "      <td>8d7f18ac-102d-4fe0-81c3-74ad1fdf901e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What them Russian mobsters do in John Wick?</td>\n",
              "      <td>content='The provided context does not specifi...</td>\n",
              "      <td>[page_content=': 22\\nReview: Lets contemplate ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In John Wick, a group of Russian mobsters vict...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.992287</td>\n",
              "      <td>5cfb1393-c05d-48e9-b7b6-3e606ca8a98a</td>\n",
              "      <td>d5046214-5771-49f5-84d4-95d5d8db2ef0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Who is Chad Stahelski and what makes his direc...</td>\n",
              "      <td>content='Chad Stahelski is a director and form...</td>\n",
              "      <td>[page_content=': 3\\nReview: John wick has a ve...</td>\n",
              "      <td>None</td>\n",
              "      <td>Chad Stahelski is a director known for his exp...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.233980</td>\n",
              "      <td>8a824ea6-0077-4f19-9c44-f5f35d45c026</td>\n",
              "      <td>3555f97f-ddca-41ba-9164-f380236a7664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Why is John Wick popular among audiences?</td>\n",
              "      <td>content=\"John Wick is popular among audiences ...</td>\n",
              "      <td>[page_content=': 16\\nReview: John Wick 3 is wi...</td>\n",
              "      <td>None</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>None</td>\n",
              "      <td>2.416881</td>\n",
              "      <td>55ac82ee-4249-4832-afd5-a0db3d2f8acb</td>\n",
              "      <td>de1b4062-9d8f-4bbe-aca6-861fa82519af</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What is the basic plot of John Wick?</td>\n",
              "      <td>content='The basic plot of \"John Wick\" revolve...</td>\n",
              "      <td>[page_content=': 5\\nReview: The first John Wic...</td>\n",
              "      <td>None</td>\n",
              "      <td>The basic plot of John Wick involves Keanu Ree...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.325190</td>\n",
              "      <td>5826844b-91c9-4d0f-a78b-6ae607fd584f</td>\n",
              "      <td>c28978ef-ef46-43ce-bd28-76e063bf7e8d</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults excellent-house-11>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_results = evaluate(\n",
        "    naive_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        coherence_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"naive_retrieval_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith BM25 Retrieval Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'definite-hill-6' at:\n",
            "https://smith.langchain.com/o/b2bbbf41-ce3b-46fa-8cc0-7de9bb768aa9/datasets/6358e678-d29a-445d-a5e4-feefef89eb02/compare?selectedSessions=db167432-97e4-4257-9d6c-74002345cb6f\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c9e363321b6476bbee1a9480504b917",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7d431ec-0836-4228-95b5-950d287cfa98: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7d431ec-0836-4228-95b5-950d287cfa98: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7d431ec-0836-4228-95b5-950d287cfa98: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 24afe417-7eb3-48d5-8581-532c65afa385: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 24afe417-7eb3-48d5-8581-532c65afa385: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 24afe417-7eb3-48d5-8581-532c65afa385: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 43f4da88-c88c-4a09-b066-2434f575afdd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 43f4da88-c88c-4a09-b066-2434f575afdd: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 43f4da88-c88c-4a09-b066-2434f575afdd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fbe9574d-a8bb-4f84-abd3-473f51c22f5e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fbe9574d-a8bb-4f84-abd3-473f51c22f5e: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fbe9574d-a8bb-4f84-abd3-473f51c22f5e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 74ab4e4e-8a43-4594-a04e-659a66684e9b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 74ab4e4e-8a43-4594-a04e-659a66684e9b: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 74ab4e4e-8a43-4594-a04e-659a66684e9b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 989c1273-c182-401a-8833-f567138e6e3e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 989c1273-c182-401a-8833-f567138e6e3e: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 989c1273-c182-401a-8833-f567138e6e3e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dac914d0-8744-4af4-9eb1-3bcfe9408312: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dac914d0-8744-4af4-9eb1-3bcfe9408312: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dac914d0-8744-4af4-9eb1-3bcfe9408312: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 99ffc276-657b-41ae-a1cc-6014d87a02a7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 99ffc276-657b-41ae-a1cc-6014d87a02a7: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 99ffc276-657b-41ae-a1cc-6014d87a02a7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dee0007c-26be-4fd3-b2e6-d5465ff573e2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dee0007c-26be-4fd3-b2e6-d5465ff573e2: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dee0007c-26be-4fd3-b2e6-d5465ff573e2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 54e9b016-00e4-4a39-a101-ddb1a6f88b2f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 54e9b016-00e4-4a39-a101-ddb1a6f88b2f: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 54e9b016-00e4-4a39-a101-ddb1a6f88b2f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bm25_results = evaluate(\n",
        "    bm25_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        coherence_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"bm25_retrieval_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith Contextual Compression Retrieval Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'extraneous-camp-83' at:\n",
            "https://smith.langchain.com/o/b2bbbf41-ce3b-46fa-8cc0-7de9bb768aa9/datasets/6358e678-d29a-445d-a5e4-feefef89eb02/compare?selectedSessions=e8f6f077-336a-4ba5-b16f-eab21d52a2c9\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7786f5ebce9948e7a8492c049b157454",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 29acbbb7-9e26-4af3-bb64-2b665bb803bf: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 29acbbb7-9e26-4af3-bb64-2b665bb803bf: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 29acbbb7-9e26-4af3-bb64-2b665bb803bf: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e6cb8b76-389f-4b7a-a7e5-f5a0b61afa66: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e6cb8b76-389f-4b7a-a7e5-f5a0b61afa66: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e6cb8b76-389f-4b7a-a7e5-f5a0b61afa66: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d9cc6e22-264c-468f-9d85-fb0edf00a5fb: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d9cc6e22-264c-468f-9d85-fb0edf00a5fb: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d9cc6e22-264c-468f-9d85-fb0edf00a5fb: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e532d387-5c02-4ce2-bcf5-3605491a51b7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e532d387-5c02-4ce2-bcf5-3605491a51b7: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e532d387-5c02-4ce2-bcf5-3605491a51b7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7f400ff3-e5cf-4840-b6d7-6b850cec897b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7f400ff3-e5cf-4840-b6d7-6b850cec897b: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7f400ff3-e5cf-4840-b6d7-6b850cec897b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8906cc59-aee8-4114-bd87-fdc941b4949d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8906cc59-aee8-4114-bd87-fdc941b4949d: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8906cc59-aee8-4114-bd87-fdc941b4949d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7376565-de01-4057-a95c-b54b4b340730: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7376565-de01-4057-a95c-b54b4b340730: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7376565-de01-4057-a95c-b54b4b340730: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0fd90bc7-904c-4144-9b5e-2bbc97c8db2c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0fd90bc7-904c-4144-9b5e-2bbc97c8db2c: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0fd90bc7-904c-4144-9b5e-2bbc97c8db2c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run aeba20f2-f40f-451e-bbd5-b4bf492acc44: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run aeba20f2-f40f-451e-bbd5-b4bf492acc44: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run aeba20f2-f40f-451e-bbd5-b4bf492acc44: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1610f190-f33f-4ea0-be7f-e8c3e189e931: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1610f190-f33f-4ea0-be7f-e8c3e189e931: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1610f190-f33f-4ea0-be7f-e8c3e189e931: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "contextual_compression_results = evaluate(\n",
        "    contextual_compression_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        coherence_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"contextual_compression_retrieval_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith Multi-Query Retrieval Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'only-heart-44' at:\n",
            "https://smith.langchain.com/o/b2bbbf41-ce3b-46fa-8cc0-7de9bb768aa9/datasets/6358e678-d29a-445d-a5e4-feefef89eb02/compare?selectedSessions=4e63032e-f9b2-4f13-a857-563291c51b91\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f78825ad0f346dbb652351f9b9f5db9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86ff135a-a849-4881-be54-2537cb5c23fe: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86ff135a-a849-4881-be54-2537cb5c23fe: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86ff135a-a849-4881-be54-2537cb5c23fe: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c6a93d78-cfa2-4793-8b46-18f396c19673: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c6a93d78-cfa2-4793-8b46-18f396c19673: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c6a93d78-cfa2-4793-8b46-18f396c19673: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 844e66bb-8fb6-49d5-bbcf-29f2b90bca01: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 844e66bb-8fb6-49d5-bbcf-29f2b90bca01: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 844e66bb-8fb6-49d5-bbcf-29f2b90bca01: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e46aaa71-5acf-4891-8498-5153e5bd922c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e46aaa71-5acf-4891-8498-5153e5bd922c: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e46aaa71-5acf-4891-8498-5153e5bd922c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ba8b1b93-0038-4630-afda-21a61a82575c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ba8b1b93-0038-4630-afda-21a61a82575c: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ba8b1b93-0038-4630-afda-21a61a82575c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6f036436-25e5-4d1d-ba87-c623cfd41492: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6f036436-25e5-4d1d-ba87-c623cfd41492: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6f036436-25e5-4d1d-ba87-c623cfd41492: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1d607d04-ae21-4085-a187-e6f6d76894e9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1d607d04-ae21-4085-a187-e6f6d76894e9: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1d607d04-ae21-4085-a187-e6f6d76894e9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4447410d-997e-4159-88c7-ff731cc44702: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4447410d-997e-4159-88c7-ff731cc44702: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4447410d-997e-4159-88c7-ff731cc44702: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5cd92415-f681-475d-b7db-1f50e1d3a5df: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5cd92415-f681-475d-b7db-1f50e1d3a5df: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5cd92415-f681-475d-b7db-1f50e1d3a5df: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9ccf0c7d-2763-4d40-9381-a424c2fedcb7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9ccf0c7d-2763-4d40-9381-a424c2fedcb7: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9ccf0c7d-2763-4d40-9381-a424c2fedcb7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "multi_query_results = evaluate(\n",
        "    multi_query_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        coherence_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"multi_query_retrieval_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Langsmith Parent Document Retrieval Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'scholarly-history-55' at:\n",
            "https://smith.langchain.com/o/b2bbbf41-ce3b-46fa-8cc0-7de9bb768aa9/datasets/6358e678-d29a-445d-a5e4-feefef89eb02/compare?selectedSessions=4132138a-2710-4eb3-99bb-4c0b54f146bb\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b11766864fb4e7cb5ea166a309cd3e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8ba25bf4-a07d-4dd7-b4db-e1b7dd07b354: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8ba25bf4-a07d-4dd7-b4db-e1b7dd07b354: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8ba25bf4-a07d-4dd7-b4db-e1b7dd07b354: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86bd5071-0e52-4258-9e20-af9d69dc8253: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86bd5071-0e52-4258-9e20-af9d69dc8253: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86bd5071-0e52-4258-9e20-af9d69dc8253: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4334c639-5905-4d94-9268-01f287b7e7a9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4334c639-5905-4d94-9268-01f287b7e7a9: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4334c639-5905-4d94-9268-01f287b7e7a9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 49bfc5a6-0548-4f3c-ac2f-ca2f80a26f5a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 49bfc5a6-0548-4f3c-ac2f-ca2f80a26f5a: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 49bfc5a6-0548-4f3c-ac2f-ca2f80a26f5a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7fa6aa2d-a07d-4132-ab05-30334245f6a6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7fa6aa2d-a07d-4132-ab05-30334245f6a6: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7fa6aa2d-a07d-4132-ab05-30334245f6a6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2e881296-3948-4227-b36d-61bfa533b546: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2e881296-3948-4227-b36d-61bfa533b546: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2e881296-3948-4227-b36d-61bfa533b546: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 624a21b3-3d74-449a-ac51-ec296085d344: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 624a21b3-3d74-449a-ac51-ec296085d344: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 624a21b3-3d74-449a-ac51-ec296085d344: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 91dae1b8-b71d-4560-b087-af496ba04b7c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 91dae1b8-b71d-4560-b087-af496ba04b7c: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 91dae1b8-b71d-4560-b087-af496ba04b7c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a940a87-fc75-4d8c-8f4d-316658f3f905: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a940a87-fc75-4d8c-8f4d-316658f3f905: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a940a87-fc75-4d8c-8f4d-316658f3f905: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b512f52b-480a-4f24-89ff-b99609389c68: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b512f52b-480a-4f24-89ff-b99609389c68: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b512f52b-480a-4f24-89ff-b99609389c68: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parent_document_results = evaluate(\n",
        "    parent_document_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        coherence_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"parent_document_retrieval_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith Ensemble Retrieval Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'ordinary-pain-15' at:\n",
            "https://smith.langchain.com/o/b2bbbf41-ce3b-46fa-8cc0-7de9bb768aa9/datasets/6358e678-d29a-445d-a5e4-feefef89eb02/compare?selectedSessions=095b42e8-b3a5-4578-8219-5dbbf7fe8dd1\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7518f0c40cae4fb29efd21e4059d09fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 07ab09d0-07a2-4a77-b090-e6f21a00228b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 07ab09d0-07a2-4a77-b090-e6f21a00228b: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 07ab09d0-07a2-4a77-b090-e6f21a00228b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0772161c-d1c4-4036-a92f-3aa890d95437: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0772161c-d1c4-4036-a92f-3aa890d95437: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0772161c-d1c4-4036-a92f-3aa890d95437: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bf4bf6bd-a3ed-41fb-ad04-da8e80bc28f0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bf4bf6bd-a3ed-41fb-ad04-da8e80bc28f0: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bf4bf6bd-a3ed-41fb-ad04-da8e80bc28f0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 70bca352-f23f-47a9-9b6f-79776e51132d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 70bca352-f23f-47a9-9b6f-79776e51132d: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 70bca352-f23f-47a9-9b6f-79776e51132d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 857c820a-7173-4c2c-823d-b8f86e5e2676: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 857c820a-7173-4c2c-823d-b8f86e5e2676: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 857c820a-7173-4c2c-823d-b8f86e5e2676: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 658d073e-44c1-4ff9-8d3c-ca68d0e3a35d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 658d073e-44c1-4ff9-8d3c-ca68d0e3a35d: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 658d073e-44c1-4ff9-8d3c-ca68d0e3a35d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a76885fe-9450-418f-b112-561833c02941: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a76885fe-9450-418f-b112-561833c02941: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a76885fe-9450-418f-b112-561833c02941: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 07ef5c78-81c8-42e0-bdce-6bbfed6d0a3b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 07ef5c78-81c8-42e0-bdce-6bbfed6d0a3b: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 07ef5c78-81c8-42e0-bdce-6bbfed6d0a3b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 12c3201c-e154-445a-9bcb-d443ac923442: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 12c3201c-e154-445a-9bcb-d443ac923442: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 12c3201c-e154-445a-9bcb-d443ac923442: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 95e4a08a-4e14-4f95-a954-94636957ea62: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 95e4a08a-4e14-4f95-a954-94636957ea62: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 95e4a08a-4e14-4f95-a954-94636957ea62: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ensemble_results = evaluate(\n",
        "    ensemble_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        coherence_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"ensemble_retrieval_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith Semantic Retrieval Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'proper-knowledge-23' at:\n",
            "https://smith.langchain.com/o/b2bbbf41-ce3b-46fa-8cc0-7de9bb768aa9/datasets/6358e678-d29a-445d-a5e4-feefef89eb02/compare?selectedSessions=a1a5959d-e70b-468a-92cc-6386d7939746\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e779fa03b11b4e5fa07664c83aef2744",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2d09d8b9-a8ad-4ce7-8730-8f7b1dedccb0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2d09d8b9-a8ad-4ce7-8730-8f7b1dedccb0: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2d09d8b9-a8ad-4ce7-8730-8f7b1dedccb0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 68f1f89e-80ed-4de9-ad91-c12f5865d570: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 68f1f89e-80ed-4de9-ad91-c12f5865d570: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 68f1f89e-80ed-4de9-ad91-c12f5865d570: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f42c63ff-8d88-4899-8fdc-85d9d566d7b1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f42c63ff-8d88-4899-8fdc-85d9d566d7b1: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f42c63ff-8d88-4899-8fdc-85d9d566d7b1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4206f43e-e849-48b4-bb1b-fdb76372a78e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4206f43e-e849-48b4-bb1b-fdb76372a78e: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4206f43e-e849-48b4-bb1b-fdb76372a78e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8e0a596a-a709-42d7-b88c-352c4cb99744: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8e0a596a-a709-42d7-b88c-352c4cb99744: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8e0a596a-a709-42d7-b88c-352c4cb99744: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d6d1351-682f-4dac-ae2e-6444604a3775: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d6d1351-682f-4dac-ae2e-6444604a3775: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d6d1351-682f-4dac-ae2e-6444604a3775: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 50c39fc3-1c3d-4943-ad23-c06bffda762c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 50c39fc3-1c3d-4943-ad23-c06bffda762c: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 50c39fc3-1c3d-4943-ad23-c06bffda762c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 44e36897-4137-49c9-b4b9-f82bf5f82622: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 44e36897-4137-49c9-b4b9-f82bf5f82622: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 44e36897-4137-49c9-b4b9-f82bf5f82622: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 77b4282c-4501-4922-b127-6241f74672f9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 77b4282c-4501-4922-b127-6241f74672f9: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 77b4282c-4501-4922-b127-6241f74672f9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b78413d0-55a5-47b3-a0eb-cc0896c56e86: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b78413d0-55a5-47b3-a0eb-cc0896c56e86: KeyError('output')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "         ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1755566/3019131688.py\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"output\"],\n",
            "                  ~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'output'\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b78413d0-55a5-47b3-a0eb-cc0896c56e86: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'input\\', \\'output\\'], input_types={}, partial_variables={\\'criteria\\': \"coherence: {\\'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?\\'}\"}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name=\\'coherence\\' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/home/pkang/ai/aibootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={'criteria': \"coherence: {'Is the response coherent with the retrieved data or the query?Is the response easy to read and logically structured and well-written?'}\"}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7358caf07360>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7358caf076f0>, root_client=<openai.OpenAI object at 0x7358cada1e50>, root_async_client=<openai.AsyncOpenAI object at 0x7358cad98f50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=CriteriaResultOutputParser() llm_kwargs={} criterion_name='coherence' only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "semantic_results = evaluate(\n",
        "    semantic_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        coherence_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"semantic_retrieval_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "#bm25_resp = client.read_project(project_name=bm25_results.experiment_name, include_stats=True)\n",
        "#bm25_metrics = extract_metrics_from_experiment(bm25_resp.json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith Retrieval Evaluation for Latency and Token Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from langsmith import Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Define the function to extract metrics\n",
        "def extract_metrics_from_experiment(experiment_json):\n",
        "    \"\"\"\n",
        "    Extract token usage and latency metrics from a LangSmith experiment JSON response\n",
        "    \n",
        "    Args:\n",
        "        experiment_json: Either a dictionary or a JSON string\n",
        "    \"\"\"\n",
        "    # Convert to dictionary if it's a string\n",
        "    if isinstance(experiment_json, str):\n",
        "        try:\n",
        "            experiment_json = json.loads(experiment_json)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error: Invalid JSON string\")\n",
        "            return {}\n",
        "    \n",
        "    metrics = {\n",
        "        # Token usage\n",
        "        \"total_tokens\": experiment_json.get(\"total_tokens\", 0),\n",
        "        \"prompt_tokens\": experiment_json.get(\"prompt_tokens\", 0),\n",
        "        \"completion_tokens\": experiment_json.get(\"completion_tokens\", 0),\n",
        "        \n",
        "        # Latency\n",
        "        \"latency_p50\": experiment_json.get(\"latency_p50\", 0),  # Median latency\n",
        "        \"latency_p99\": experiment_json.get(\"latency_p99\", 0),  # 99th percentile latency\n",
        "        \n",
        "        # Cost\n",
        "        \"total_cost\": experiment_json.get(\"total_cost\", 0),\n",
        "        \"prompt_cost\": experiment_json.get(\"prompt_cost\", 0),\n",
        "        \"completion_cost\": experiment_json.get(\"completion_cost\", 0),\n",
        "        \n",
        "        # Run information\n",
        "        \"run_count\": experiment_json.get(\"run_count\", 0),\n",
        "        \"retriever_type\": experiment_json.get(\"extra\", {}).get(\"metadata\", {}).get(\"revision_id\", \"unknown\")\n",
        "    }\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed experiment: proper-pleasure-59 (Naive Retriever)\n",
            "Successfully processed experiment: definite-hill-6 (BM25 Retriever)\n",
            "Successfully processed experiment: extraneous-camp-83 (Contextual Compression Retriever)\n",
            "Successfully processed experiment: only-heart-44 (Multi-Query Retriever)\n",
            "Successfully processed experiment: scholarly-history-55 (Parent Document Retriever)\n",
            "Successfully processed experiment: ordinary-pain-15 (Ensemble Retriever)\n",
            "Successfully processed experiment: proper-knowledge-23 (Semantic Retriever)\n",
            "\n",
            "Comparison of Retrieval Methods:\n",
            "                     retriever_name  total_tokens  latency_p50  total_cost  \\\n",
            "0                   Naive Retriever         37390       3.6495    0.006513   \n",
            "1                    BM25 Retriever         14322       1.7995    0.002869   \n",
            "2  Contextual Compression Retriever         12620       3.3895    0.002637   \n",
            "3             Multi-Query Retriever         51678       4.5595    0.008943   \n",
            "4         Parent Document Retriever          7106       2.6780    0.001648   \n",
            "5                Ensemble Retriever         56330       6.5000    0.009691   \n",
            "6                Semantic Retriever         31992       3.1415    0.005626   \n",
            "\n",
            "   run_count  tokens_per_run  \n",
            "0         10          3739.0  \n",
            "1         10          1432.2  \n",
            "2         10          1262.0  \n",
            "3         10          5167.8  \n",
            "4         10           710.6  \n",
            "5         10          5633.0  \n",
            "6         10          3199.2  \n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Compare experiments using the result objects\n",
        "# This assumes you have already run the experiments and have the result objects\n",
        "\n",
        "# List of experiment result objects\n",
        "experiment_results = [\n",
        "    naive_results,     # Replace with your actual result object names\n",
        "    bm25_results,\n",
        "    contextual_compression_results,\n",
        "    multi_query_results,\n",
        "    parent_document_results,\n",
        "    ensemble_results,\n",
        "    semantic_results\n",
        "]\n",
        "\n",
        "# List of experiment names (for better labeling)\n",
        "retriever_names = [\n",
        "    \"Naive Retriever\",\n",
        "    \"BM25 Retriever\",\n",
        "    \"Contextual Compression Retriever\",\n",
        "    \"Multi-Query Retriever\",\n",
        "    \"Parent Document Retriever\",\n",
        "    \"Ensemble Retriever\",\n",
        "    \"Semantic Retriever\"\n",
        "]\n",
        "\n",
        "# Collect metrics for all experiments\n",
        "all_metrics = []\n",
        "\n",
        "for i, result in enumerate(experiment_results):\n",
        "    try:\n",
        "        # Get the experiment data from LangSmith\n",
        "        exp_resp = client.read_project(project_name=result.experiment_name, include_stats=True)\n",
        "        \n",
        "        # Extract metrics\n",
        "        metrics = extract_metrics_from_experiment(exp_resp.json())\n",
        "        \n",
        "        # Add experiment name and retriever name\n",
        "        metrics[\"experiment_name\"] = result.experiment_name\n",
        "        metrics[\"retriever_name\"] = retriever_names[i]\n",
        "        \n",
        "        all_metrics.append(metrics)\n",
        "        print(f\"Successfully processed experiment: {result.experiment_name} ({retriever_names[i]})\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing experiment {retriever_names[i]}: {e}\")\n",
        "\n",
        "# Create DataFrame from all metrics\n",
        "comparison_df = pd.DataFrame(all_metrics)\n",
        "\n",
        "# Calculate tokens per run\n",
        "comparison_df['tokens_per_run'] = comparison_df['total_tokens'] / comparison_df['run_count'].apply(lambda x: max(x, 1))\n",
        "\n",
        "# Display the comparison\n",
        "print(\"\\nComparison of Retrieval Methods:\")\n",
        "print(comparison_df[['retriever_name', 'total_tokens', 'latency_p50', 'total_cost', 'run_count', 'tokens_per_run']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x1200 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detailed Comparison Table:\n",
            "                     retriever_name  total_tokens  tokens_per_run  \\\n",
            "0                   Naive Retriever         37390          3739.0   \n",
            "1                    BM25 Retriever         14322          1432.2   \n",
            "2  Contextual Compression Retriever         12620          1262.0   \n",
            "3             Multi-Query Retriever         51678          5167.8   \n",
            "4         Parent Document Retriever          7106           710.6   \n",
            "5                Ensemble Retriever         56330          5633.0   \n",
            "6                Semantic Retriever         31992          3199.2   \n",
            "\n",
            "   latency_p50  total_cost  tokens_per_second  run_count  \n",
            "0        3.650    0.006513            1024.38         10  \n",
            "1        1.800    0.002869             795.67         10  \n",
            "2        3.390    0.002637             372.27         10  \n",
            "3        4.560    0.008943            1133.29         10  \n",
            "4        2.678    0.001648             265.35         10  \n",
            "5        6.500    0.009691             866.62         10  \n",
            "6        3.142    0.005626            1018.20         10  \n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Visualize the comparison (Fixed version)\n",
        "if not comparison_df.empty:\n",
        "    # Set the style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    \n",
        "    # Create a figure with multiple subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Plot token usage (total tokens)\n",
        "    sns.barplot(x='retriever_name', y='total_tokens', data=comparison_df, ax=axes[0, 0])\n",
        "    axes[0, 0].set_title('Total Token Usage by Retriever')\n",
        "    axes[0, 0].set_xlabel('Retriever')\n",
        "    axes[0, 0].set_ylabel('Total Tokens')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot tokens per run (make sure the scale is different)\n",
        "    sns.barplot(x='retriever_name', y='tokens_per_run', data=comparison_df, ax=axes[0, 1])\n",
        "    axes[0, 1].set_title('Tokens per Run by Retriever')\n",
        "    axes[0, 1].set_xlabel('Retriever')\n",
        "    axes[0, 1].set_ylabel('Tokens per Run')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    # Force y-axis to start at 0 and have a different scale than the total tokens plot\n",
        "    max_tokens_per_run = comparison_df['tokens_per_run'].max()\n",
        "    axes[0, 1].set_ylim(0, max_tokens_per_run * 1.2)  # Add 20% padding\n",
        "    \n",
        "    # Plot latency\n",
        "    sns.barplot(x='retriever_name', y='latency_p50', data=comparison_df, ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('Median Latency by Retriever')\n",
        "    axes[1, 0].set_xlabel('Retriever')\n",
        "    axes[1, 0].set_ylabel('Latency (seconds)')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot cost\n",
        "    sns.barplot(x='retriever_name', y='total_cost', data=comparison_df, ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('Total Cost by Retriever')\n",
        "    axes[1, 1].set_xlabel('Retriever')\n",
        "    axes[1, 1].set_ylabel('Cost ($)')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add a note about the relationship between total tokens and tokens per run\n",
        "    fig.text(0.5, 0.01, \n",
        "             'Note: Tokens per Run = Total Tokens / Number of Runs', \n",
        "             ha='center', fontsize=12, style='italic')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('retriever_comparison.png')\n",
        "    plt.show()\n",
        "    \n",
        "    # Create a more detailed comparison table\n",
        "    print(\"\\nDetailed Comparison Table:\")\n",
        "    comparison_table = comparison_df.copy()\n",
        "    \n",
        "    # Format the columns\n",
        "    for col in ['total_tokens', 'prompt_tokens', 'completion_tokens', 'tokens_per_run']:\n",
        "        if col in comparison_table.columns:\n",
        "            comparison_table[col] = comparison_table[col].round(2)\n",
        "    \n",
        "    comparison_table['latency_p50'] = comparison_table['latency_p50'].round(3)\n",
        "    comparison_table['total_cost'] = comparison_table['total_cost'].round(6)\n",
        "    \n",
        "    # Add a column for tokens per second (efficiency)\n",
        "    comparison_table['tokens_per_second'] = (comparison_table['tokens_per_run'] / comparison_table['latency_p50']).round(2)\n",
        "    \n",
        "    # Display the table\n",
        "    print(comparison_table[['retriever_name', 'total_tokens', 'tokens_per_run', \n",
        "                           'latency_p50', 'total_cost', 'tokens_per_second', 'run_count']])\n",
        "    \n",
        "    # Save to CSV\n",
        "    comparison_table.to_csv('retriever_comparison.csv', index=False)\n",
        "else:\n",
        "    print(\"No data available for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion on Latency and Cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Token Usage\n",
        "\n",
        "- Ensemble Retriever uses the most tokens (56,330 total)\n",
        "\n",
        "- Multi-Query Retriever is the second highest (51,678 total)\n",
        "\n",
        "- Naive Retriever falls in the middle (37,390 total)\n",
        "\n",
        "- Parent Document Retriever uses moderate tokens (likely around 20,000-30,000)\n",
        "- BM25 Retriever is more efficient (14,322 total)\n",
        "Contextual Compression Retriever is the most token-efficient (12,620 total)\n",
        "\n",
        "2. Latency\n",
        "\n",
        "- BM25 Retriever is the fastest (1.80 seconds median latency)\n",
        "\n",
        "- Contextual Compression Retriever is moderately fast (3.39 seconds)\n",
        "\n",
        "- Naive Retriever is slower (3.65 seconds)\n",
        "\n",
        "- Multi-Query Retriever is quite slow (4.56 seconds)\n",
        "\n",
        "- Parent Document Retriever has high latency (likely around 5-6 seconds based on tokens per second)\n",
        "\n",
        "- Ensemble Retriever has the highest latency (6.50 seconds)\n",
        "\n",
        "3. Cost\n",
        "\n",
        "-  Contextual Compression Retriever is the cheapest ($0.002637)\n",
        "\n",
        "- BM25 Retriever is also cost-effective ($0.002869)\n",
        "\n",
        "- Naive Retriever costs more ($0.006513)\n",
        "\n",
        "- Parent Document Retriever is likely expensive (based on its low efficiency)\n",
        "\n",
        "- Multi-Query Retriever is expensive ($0.008943)\n",
        "\n",
        "- Ensemble Retriever is the most expensive (likely around $0.01, based on token usage)\n",
        "\n",
        "4. Efficiency (Tokens per Second)\n",
        "\n",
        "- Multi-Query Retriever processes the most tokens per second (1,133.29)\n",
        "\n",
        "- Naive Retriever is also quite efficient (1,024.38)\n",
        "\n",
        "- Ensemble Retriever would be around 866.62 tokens per second\n",
        "\n",
        "- BM25 Retriever is moderately efficient (795.67)\n",
        "\n",
        "- Contextual Compression Retriever has lower efficiency (372.27)\n",
        "\n",
        "- Parent Document Retriever processes the fewest tokens per second (265.35)\n",
        "\n",
        "\n",
        "Overall Conclusions:\n",
        "\n",
        "- Parent Document Retriever has the poorest computational efficiency (tokens per second), suggesting it may be doing more complex processing on each document\n",
        "\n",
        "- Ensemble Retriever is the most resource-intensive approach overall, with both the highest token usage and high latency\n",
        "\n",
        "- BM25 Retriever offers the best balance of speed, cost, and token efficiency\n",
        "\n",
        "- Contextual Compression Retriever is the most economical in terms of token usage and cost\n",
        "\n",
        "Practical Recommendations:\n",
        "\n",
        "- For speed and low latency: BM25 Retriever is clearly superior\n",
        "- For minimizing token usage and cost: Contextual Compression Retriever is optimal\n",
        "- For high throughput processing: Multi-Query or Naive Retrievers process tokens fastest\n",
        "- For comprehensive retrieval: Ensemble Retriever provides the most thorough approach but at significantly higher computational cost\n",
        "- Avoid for performance-critical applications: Parent Document Retriever has the poorest computational efficiency\n",
        "\n",
        "The low efficiency of the Parent Document Retriever suggests it's doing more complex processing on each document, possibly analyzing parent-child relationships between chunks, which explains its lower tokens-per-second rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Conclusion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best Overall Retrievers\n",
        "\n",
        "1. Contextual Compression Retriever\n",
        "\n",
        "- Best for quality-to-cost ratio\n",
        "\n",
        "- Highest precision (0.8083) - retrieves the most relevant documents\n",
        "\n",
        "- Very token-efficient (12,620 tokens total)\n",
        "\n",
        "- Lowest cost ($0.002637)\n",
        "\n",
        "- Moderate latency (3.39 seconds)\n",
        "\n",
        "- Recommendation: Ideal for production systems where both quality and cost matter\n",
        "\n",
        "2. Ensemble Retriever\n",
        "\n",
        "- Best for maximum recall and comprehensiveness\n",
        "\n",
        "- Exceptional context recall (0.9667) - finds nearly all relevant information\n",
        "\n",
        "- Strong entity recall (0.7333)\n",
        "\n",
        "- Highest token usage (56,330) and latency (6.50 seconds)\n",
        "\n",
        "- Most expensive option ($0.009691)\n",
        "\n",
        "- Recommendation: Use when recall is critical and cost/latency are secondary concerns\n",
        "\n",
        "3. BM25 Retriever\n",
        "\n",
        "- Best for speed and low latency\n",
        "\n",
        "- Fastest retriever (1.80 seconds)\n",
        "\n",
        "- Cost-effective ($0.002869)\n",
        "\n",
        "- However, significantly underperforms on quality metrics (context precision: 0.1417, recall: 0.3000)\n",
        "\n",
        "- Recommendation: Only use when speed is the absolute priority and quality can be sacrificed\n",
        "\n",
        "Specialized Use Cases\n",
        "\n",
        "For Budget-Constrained Applications\n",
        "\n",
        "- Contextual Compression Retriever: Best balance of quality and cost\n",
        "\n",
        "- Parent Document Retriever: Lowest token usage (7,106) but poor computational efficiency\n",
        "\n",
        "For Time-Critical Applications\n",
        "\n",
        "- BM25 Retriever: Fastest (1.80 seconds) but poor quality\n",
        "\n",
        "- Parent Document Retriever: Good latency (2.678 seconds) with moderate quality\n",
        "\n",
        "For High-Quality Results\n",
        "\n",
        "- Ensemble Retriever: Highest recall but expensive and slow\n",
        "\n",
        "- Contextual Compression Retriever: Highest precision with reasonable cost\n",
        "\n",
        "For Balanced Performance\n",
        "\n",
        "- Semantic Retriever: Good balance of metrics (recall: 0.8667, entity recall: 0.7125)\n",
        "\n",
        "- Naive Retriever: Surprisingly strong baseline with balanced performance\n",
        "\n",
        "Final Recommendation\n",
        "\n",
        "- For most real-world applications, I recommend the Contextual Compression Retriever as the best overall choice. It offers:\n",
        "\n",
        "- The highest precision (0.8083) - meaning less irrelevant information\n",
        "\n",
        "- The lowest cost ($0.002637)\n",
        "Good token efficiency (12,620 tokens)\n",
        "Reasonable latency (3.39 seconds)\n",
        "\n",
        "If your application absolutely requires finding all possible relevant information and can tolerate higher costs and latency, the Ensemble Retriever would be the better choice.\n",
        "\n",
        "For systems where you need to optimize for different priorities at different times, consider implementing both the Contextual Compression and Ensemble retrievers, and switching between them based on the specific query requirements."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
